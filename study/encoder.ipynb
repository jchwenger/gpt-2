{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chilperic/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/chilperic/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/chilperic/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/chilperic/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/chilperic/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/chilperic/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nprint(*args):\n",
    "    print(*args, end='\\n\\n-----------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cprint(things):\n",
    "    print(*things, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.encoder import get_pairs\n",
    "from src.encoder import get_encoder\n",
    "from src.encoder import bytes_to_unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex as re\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### The Encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    \"\"\"\n",
    "    Attributes: \n",
    "    - encoder/decoder (dicts)\n",
    "    - errors: option for bytearray.decode()\n",
    "    - byte_encoder/decoder (dicts)\n",
    "    - bpe_ranks\n",
    "    \"\"\"\n",
    "                                            # errors='replace'\n",
    "                                            # an option for the bytearray() conversion function used below.\n",
    "                                            # cf Python doc: Replace with a suitable replacement marker; \n",
    "                                            # Python will use the official U+FFFD REPLACEMENT CHARACTER \n",
    "                                            # for the built-in codecs on decoding, and ‘?’ on encoding.  \n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}            # simply reversing from {k:v} to {v:k}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()                          # our look-up table function\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()} # reversing again, for bytes\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))  # { x0: 0, x1: 1, ...}\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "                            # Regexes:\n",
    "                            # contractions\n",
    "                            # words: one or more of any letter (\\p{L}), preceded by optional space\n",
    "                            # numbers: (\\p{N}), preceded by optional space\n",
    "                            # no code: NOT a space followed by one letter & one or more of any number (code)\n",
    "                            #          preceded by optional space, all this one or more times\n",
    "                            # no single space: one or more spaces not followed non-whitespace, negative lookahead: (?!\\S) \n",
    "                            # one or more spaces ok\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\", flags=re.IGNORECASE)\n",
    "                                                                                                                # adding ignorecase, as mentioned above\n",
    "\n",
    "    def bpe(self, token):\n",
    "\n",
    "        # don't do the work twice, save words on the go\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "\n",
    "        word = tuple(token)     # turn token to char tuple\n",
    "        pairs = get_pairs(word) # get all char pairs: ('w','o','r','d') > { ('w', 'o'), ('o', 'r'), ('r', 'd') }\n",
    "\n",
    "        # if word was only one symbol?\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "                                                                            \n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "                                                                            # float('inf'): It acts as an unbounded upper value for \n",
    "            if bigram not in self.bpe_ranks:                                # comparison. This is useful for finding lowest \n",
    "                break                                                       # values for something. \n",
    "                                                                            # https://stackoverflow.com/a/34264749\n",
    "            first, second = bigram                                          \n",
    "            new_word = []                                                   \n",
    "\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)   # returns index of searched element (first), starting at i\n",
    "                    new_word.extend(word[i:j]) # append items from iterable to the end of the array\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "\n",
    "        bpe_tokens = []                                 \n",
    "        \n",
    "        # for each token found by our regex (words, numbers, more than one space, punctuation)\n",
    "        for token in re.findall(self.pat, text):     \n",
    "\n",
    "            # encode to utf-8 (char > int), then encode to byte, then join in a string\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of dissection... We can reuse the `117M` encoder we uploaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = json117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_self_encoder = encoder\n",
    "enc_self_decoder = {v:k for k,v in enc_self_encoder.items()}            # simply reversing from {k:v} to {v:k}\n",
    "enc_self_byte_encoder = bytes_to_unicode()                          # our look-up table function\n",
    "enc_self_byte_decoder = {v:k for k, v in enc_self_byte_encoder.items()} # reversing again, for bytes\n",
    "enc_self_bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))  # { x0: 0, x1: 1, ...}\n",
    "enc_self_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contractions\n",
    "# words: one or more of any letter (\\p{L}), preceded by optional space\n",
    "# numbers: (\\p{N}), preceded by optional space\n",
    "# punctuation: not a space, a letter or a number, one or more times, preceded by optional space\n",
    "# no single space: one or more spaces not followed non-whitespace, negative lookahead: (?!\\S) \n",
    "# one or more spaces ok\n",
    "enc_self_pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\", flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Won', \"'t\", '?', ' I', \"'m\", ' I', \"'ll\", ' I', \"'d\", ' numbers', ' 9012', ' also', ' multiple', ' spaces', \" '\", ' ?', '       ', ' no', ' code', ' a', '9877', '  ', ' b', '002', ' x', '0', ' b', '9', 'd', '8']\n"
     ]
    }
   ],
   "source": [
    "reg_test = \"Won't? I'm I'll I'd numbers 9012 also multiple spaces ' ?        no code a9877   b002 x0 b9d8\"\n",
    "print(re.findall(enc_self_pat, reg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also separate the regexes, to see what they do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", '?', \"'\", \"'\", \"'\", \" '\", ' ?']\n",
      "['       ', '  ']\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '        ', ' ', ' ', '   ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "enc_self_reg1 = re.compile(r\"\"\" ?[^\\s\\p{L}\\p{N}]+\"\"\", flags=re.IGNORECASE)\n",
    "enc_self_reg2 = re.compile(r\"\"\"\\s+(?!\\S)\"\"\", flags=re.IGNORECASE)\n",
    "enc_self_reg3 = re.compile(r\"\"\"\\s+\"\"\", flags=re.IGNORECASE) # all spaces\n",
    "print(re.findall(enc_self_reg1, reg_test))\n",
    "print(re.findall(enc_self_reg2, reg_test))\n",
    "print(re.findall(enc_self_reg3, reg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### The bpe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_bpe(token):\n",
    "\n",
    "    # don't do the work twice, save words on the go\n",
    "    if token in enc_self_cache:\n",
    "        return enc_self_cache[token]\n",
    "\n",
    "    word = tuple(token)     # turn token to char tuple\n",
    "    pairs = get_pairs(word) # get all char pairs: ('w','o','r','d') > { ('w', 'o'), ('o', 'r'), ('r', 'd') }\n",
    "\n",
    "    # if word was only one symbol?\n",
    "    if not pairs:\n",
    "        return token\n",
    "\n",
    "    while True:\n",
    "\n",
    "        bigram = min(pairs, key = lambda pair: enc_self_bpe_ranks.get(pair, float('inf')))\n",
    "                                                                        # float('inf'): It acts as an unbounded upper value for \n",
    "        if bigram not in enc_self_bpe_ranks:                            # comparison. This is useful for finding lowest \n",
    "            break                                                       # values for something. \n",
    "                                                                        # https://stackoverflow.com/a/34264749\n",
    "        first, second = bigram                                          \n",
    "        new_word = []                                                   \n",
    "\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)   # returns index of searched element (first), starting at i\n",
    "                new_word.extend(word[i:j]) # append items from iterable to the end of the array\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "\n",
    "            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                new_word.append(first+second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "    \n",
    "    # returns a string\n",
    "    word = ' '.join(word)\n",
    "    enc_self_cache[token] = word\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dissect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antidystopianarianism\n"
     ]
    }
   ],
   "source": [
    "txt = \"antidystopianarianism\"\n",
    "token = re.findall(enc_self_pat, txt)[0]  \n",
    "token = ''.join(enc_self_byte_encoder[b] for b in token.encode('utf-8'))\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i', 'a', 'n', 'a', 'r', 'i', 'a', 'n', 'i', 's', 'm')\n",
      "{('t', 'o'), ('p', 'i'), ('s', 'm'), ('d', 'y'), ('a', 'n'), ('n', 'a'), ('r', 'i'), ('i', 's'), ('n', 'i'), ('i', 'a'), ('i', 'd'), ('n', 't'), ('o', 'p'), ('s', 't'), ('a', 'r'), ('t', 'i'), ('y', 's')}\n"
     ]
    }
   ],
   "source": [
    "tktpl = tuple(token)\n",
    "print(tktpl)\n",
    "tkpairs = get_pairs(tktpl)\n",
    "print(tkpairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the `enc_self_bpe_ranks` is a `dict` containing pairs of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ten elements:\n",
      "[(('Ġ', 't'), 0), (('Ġ', 'a'), 1), (('h', 'e'), 2), (('i', 'n'), 3), (('r', 'e'), 4), (('o', 'n'), 5), (('Ġt', 'he'), 6), (('e', 'r'), 7), (('Ġ', 's'), 8), (('a', 't'), 9)]\n",
      "\n",
      "Random ten elements:\n",
      "[(('Ġtreasure', 'r'), 43172), (('I', 'AS'), 43173), (('Ġcolon', 'ists'), 43174), (('Ġin', 'und'), 43175), (('ĠWW', 'F'), 43176), (('ĠCon', 'verted'), 43177), (('6', '000'), 43178), (('out', 'side'), 43179), (('ĠApp', 'earance'), 43180), (('ĠRel', 'ic'), 43181)]\n"
     ]
    }
   ],
   "source": [
    "print('First ten elements:', list(enc_self_bpe_ranks.items())[:10], sep='\\n')\n",
    "print()\n",
    "index = random.randint(0,len(enc_self_bpe_ranks)-11)\n",
    "print('Random ten elements:', list(enc_self_bpe_ranks.items())[index:index+10], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method:\n",
    "- take the `min()` pair according to its ranking in `enc_self_bpe_ranks`;\n",
    "- for that, use a `lambda` function, that gets the appropriate ranking number, and if the pair is not found, return `float('inf')`, namely, don't select it.\n",
    "\n",
    "Recap:   \n",
    "`dict.get()` [documentation](https://docs.python.org/3/library/stdtypes.html?highlight=dict%20get#dict.get): \n",
    "> Return the value for key if key is in the dictionary, else default. If default is not given, it defaults to None, so that this method never raises a KeyError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair: ('t', 'o') | Rank: 1206\n",
      "Pair: ('p', 'i') | Rank: 14159\n",
      "Pair: ('s', 'm') | Rank: 5540\n",
      "Pair: ('d', 'y') | Rank: 9636\n",
      "Pair: ('a', 'n') | Rank: 16\n",
      "Pair: ('n', 'a') | Rank: 2360\n",
      "Pair: ('r', 'i') | Rank: 124\n",
      "Pair: ('i', 's') | Rank: 15\n",
      "Pair: ('n', 'i') | Rank: 8205\n",
      "Pair: ('i', 'a') | Rank: 288\n",
      "Pair: ('i', 'd') | Rank: 56\n",
      "Pair: ('n', 't') | Rank: 173\n",
      "Pair: ('o', 'p') | Rank: 148\n",
      "Pair: ('s', 't') | Rank: 45\n",
      "Pair: ('a', 'r') | Rank: 27\n",
      "Pair: ('t', 'i') | Rank: 20003\n",
      "Pair: ('y', 's') | Rank: 637\n",
      "\n",
      "Returned pair: ('i', 's') | Rank: 15\n"
     ]
    }
   ],
   "source": [
    "for tkpair in tkpairs:\n",
    "    print('Pair:', tkpair, '| Rank:', enc_self_bpe_ranks.get(tkpair, float('inf')))\n",
    "\n",
    "    tkbigram = min(tkpairs, key = lambda tkpair: enc_self_bpe_ranks.get(tkpair, float('inf')))\n",
    "print()\n",
    "print('Returned pair:', tkbigram, '| Rank:',  enc_self_bpe_ranks.get(tkbigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An error mechanism is used below to trigger the `except:` block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | ('a',)\n",
      "12 | ('a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i', 'a')\n",
      "17 | ('a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i', 'a', 'n', 'a', 'r', 'i', 'a')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "tuple.index(x): x not in tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-e997b50410b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtktpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtktpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtktpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtktpl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# index will throw an error if nothing is found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tuple.index(x): x not in tuple"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < len(tktpl):\n",
    "    i = tktpl.index('n', i)\n",
    "    print(tktpl.index('n', i), '|', tktpl[:i]) # index will throw an error if nothing is found\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token tuple: ('a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i', 'a', 'n', 'a', 'r', 'i', 'a', 'n', 'i', 's', 'm')\n",
      "first, second: i s\n",
      "\n",
      "-------------\n",
      "\n",
      "try:\n",
      "word before: []\n",
      "adding tktpl[i:j]: ('a', 'n', 't')\n",
      "word after: ['a', 'n', 't'] | indices: 0 3\n",
      "\n",
      "-------------\n",
      "\n",
      "else:\n",
      "word before: ['a', 'n', 't'] | adding tktpl[i]: i\n",
      "word after: ['a', 'n', 't', 'i'] | indices: 3 3\n",
      "\n",
      "-------------\n",
      "now incrementing i by 1 to 4 \n",
      "\n",
      "try:\n",
      "word before: ['a', 'n', 't', 'i']\n",
      "adding tktpl[i:j]: ('d', 'y', 's', 't', 'o', 'p')\n",
      "word after: ['a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p'] | indices: 4 10\n",
      "\n",
      "-------------\n",
      "\n",
      "else:\n",
      "word before: ['a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p'] | adding tktpl[i]: i\n",
      "word after: ['a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i'] | indices: 10 10\n",
      "\n",
      "-------------\n",
      "now incrementing i by 1 to 11 \n",
      "\n",
      "try:\n",
      "word before: ['a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i']\n",
      "adding tktpl[i:j]: ('a', 'n', 'a', 'r')\n",
      "word after: ['a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i', 'a', 'n', 'a', 'r'] | indices: 11 15\n",
      "\n",
      "-------------\n",
      "\n",
      "else:\n",
      "word before: ['a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i', 'a', 'n', 'a', 'r'] | adding tktpl[i]: i\n",
      "word after: ['a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i', 'a', 'n', 'a', 'r', 'i'] | indices: 15 15\n",
      "\n",
      "-------------\n",
      "now incrementing i by 1 to 16 \n",
      "\n",
      "try:\n",
      "word before: ['a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i', 'a', 'n', 'a', 'r', 'i']\n",
      "adding tktpl[i:j]: ('a', 'n')\n",
      "word after: ['a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i', 'a', 'n', 'a', 'r', 'i', 'a', 'n'] | indices: 16 18\n",
      "\n",
      "-------------\n",
      "\n",
      "if:\n",
      "word before: ['a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i', 'a', 'n', 'a', 'r', 'i', 'a', 'n']\n",
      "word after: ['a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i', 'a', 'n', 'a', 'r', 'i', 'a', 'n', 'is'] | indices: 18 18\n",
      "\n",
      "-------------\n",
      "now incrementing i by 2 to 20 \n",
      "\n",
      "try:\n",
      "word before: ['a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i', 'a', 'n', 'a', 'r', 'i', 'a', 'n', 'is']\n",
      "except:\n",
      "word before: ['a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i', 'a', 'n', 'a', 'r', 'i', 'a', 'n', 'is']\n",
      "adding the end, tktpl[i:]: ('m',)\n",
      "word after ['a', 'n', 't', 'i', 'd', 'y', 's', 't', 'o', 'p', 'i', 'a', 'n', 'a', 'r', 'i', 'a', 'n', 'is', 'm'] | indices: 20 18\n",
      "\n",
      "-------------\n",
      "\n",
      "breaking: word has now the first pair: ('i', 's') inside it\n"
     ]
    }
   ],
   "source": [
    "tktpl = tuple(token)\n",
    "print('token tuple:', tktpl)\n",
    "frst, scnd = tkbigram     \n",
    "print('first, second:', frst, scnd)\n",
    "nw_wd = []\n",
    "print('\\n-------------\\n')\n",
    "\n",
    "i = 0\n",
    "while i < len(tktpl):\n",
    "    try:\n",
    "        print('try:')\n",
    "        print('word before:', nw_wd)\n",
    "        j = tktpl.index(frst, i)   # returns index of searched element (frst), starting at i\n",
    "        print('adding tktpl[i:j]:', tktpl[i:j])\n",
    "        nw_wd.extend(tktpl[i:j]) # append items from iterable to the end of the array\n",
    "        print('word after:', nw_wd, '| indices:', i, j)\n",
    "        i = j\n",
    "        print('\\n-------------\\n')\n",
    "    except:\n",
    "        print('except:')\n",
    "        print('word before:', nw_wd)\n",
    "        print('adding the end, tktpl[i:]:', tktpl[i:])\n",
    "        nw_wd.extend(tktpl[i:]) # add the end of the word\n",
    "        print('word after', nw_wd, '| indices:', i, j)\n",
    "        print('\\n-------------\\n')        \n",
    "        print('breaking: word has now the first pair:', tkbigram, 'inside it')\n",
    "        break # the end: the word has now the first pair in it, and the rest single tokens!\n",
    "\n",
    "    # if\n",
    "    # token i is first\n",
    "    # i smaller than last index\n",
    "    # token after i is scnd\n",
    "    if tktpl[i] == frst and i < len(tktpl)-1 and tktpl[i+1] == scnd:\n",
    "        print('if:')\n",
    "        print('word before:', nw_wd)\n",
    "        nw_wd.append(frst+scnd)\n",
    "        print('word after:', nw_wd, '| indices:', i, j)\n",
    "        print('\\n-------------\\nnow incrementing i by 2 to', i+2, '\\n')\n",
    "        i += 2\n",
    "    else:\n",
    "        print('else:')\n",
    "        print('word before:', nw_wd, '| adding tktpl[i]:', tktpl[i])\n",
    "        nw_wd.append(tktpl[i])\n",
    "        print('word after:', nw_wd,'| indices:', i, j)\n",
    "        print('\\n-------------\\nnow incrementing i by 1 to', i+1, '\\n')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token tuple: ('d', 'r', 'e', 'a', 'd', 'f', 'u', 'l')\n",
      "Pairs: {('f', 'u'), ('r', 'e'), ('u', 'l'), ('a', 'd'), ('e', 'a'), ('d', 'r'), ('d', 'f')}\n",
      "------------------------------\n",
      "Chosen bigram: ('r', 'e') | Rank: 4\n",
      "Token tuple updated: ('d', 're', 'a', 'd', 'f', 'u', 'l')\n",
      "Repairing, pairs: {('d', 're'), ('u', 'l'), ('a', 'd'), ('re', 'a'), ('f', 'u'), ('d', 'f')}\n",
      "\n",
      "Chosen bigram: ('a', 'd') | Rank: 68\n",
      "Token tuple updated: ('d', 're', 'ad', 'f', 'u', 'l')\n",
      "Repairing, pairs: {('d', 're'), ('u', 'l'), ('ad', 'f'), ('re', 'ad'), ('f', 'u')}\n",
      "\n",
      "Chosen bigram: ('u', 'l') | Rank: 121\n",
      "Token tuple updated: ('d', 're', 'ad', 'f', 'ul')\n",
      "Repairing, pairs: {('d', 're'), ('f', 'ul'), ('ad', 'f'), ('re', 'ad')}\n",
      "\n",
      "Chosen bigram: ('f', 'ul') | Rank: 657\n",
      "Token tuple updated: ('d', 're', 'ad', 'ful')\n",
      "Repairing, pairs: {('d', 're'), ('ad', 'ful'), ('re', 'ad')}\n",
      "\n",
      "Chosen bigram: ('re', 'ad') | Rank: 705\n",
      "Token tuple updated: ('d', 'read', 'ful')\n",
      "Repairing, pairs: {('read', 'ful'), ('d', 'read')}\n",
      "\n",
      "Chosen bigram: ('read', 'ful') | Rank: None\n",
      "---------------\n",
      "Bigram ain't in ranks, bugger off!\n"
     ]
    }
   ],
   "source": [
    "token = 'dreadful'\n",
    "tktpl = tuple(token)\n",
    "tkpairs = get_pairs(tktpl)\n",
    "print('Token tuple:', tktpl)\n",
    "print('Pairs:', tkpairs)\n",
    "print('-'*30)\n",
    "\n",
    "while True:\n",
    "                                        # get pair rank, return 'inf' if not found\n",
    "    bgrm = min(tkpairs, key = lambda pair: enc_self_bpe_ranks.get(pair, float('inf')))\n",
    "    print('Chosen bigram:', bgrm, '| Rank:', enc_self_bpe_ranks.get(bgrm))\n",
    "    \n",
    "    # if not in ranks stop\n",
    "    if bgrm not in enc_self_bpe_ranks: \n",
    "        print('---------------')\n",
    "        print('Bigram ain\\'t in ranks, bugger off!')\n",
    "        break \n",
    "        \n",
    "    # split the pair in two\n",
    "    frst, scnd = bgrm                                          \n",
    "    nw_wd = []                                                   \n",
    "    i = 0\n",
    "    \n",
    "    while i < len(tktpl):\n",
    "        \n",
    "        try:\n",
    "            j = tktpl.index(frst, i) # lowest index of frst, start at i\n",
    "            nw_wd.extend(tktpl[i:j]) \n",
    "            i = j\n",
    "        except:\n",
    "            nw_wd.extend(tktpl[i:])\n",
    "            break\n",
    "            \n",
    "        if tktpl[i] == frst and i < len(tktpl)-1 and tktpl[i+1] == scnd:\n",
    "            nw_wd.append(frst+scnd)\n",
    "            i += 2\n",
    "        else:\n",
    "            nw_wd.append(tktpl[i])\n",
    "            i += 1\n",
    "            \n",
    "    nw_wd = tuple(nw_wd)\n",
    "    tktpl = nw_wd\n",
    "    print('Token tuple updated:', tktpl)\n",
    "    \n",
    "    if len(tktpl) == 1:\n",
    "        print()\n",
    "        print('Length now', len(tktpl), 'hence breaking')\n",
    "        break\n",
    "    else:\n",
    "        print('Repairing, pairs:', get_pairs(tktpl), end='\\n\\n')\n",
    "        tkpairs = get_pairs(tktpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is turned again into a string, and will be unpacked by the `encode` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d read ful\n",
      "d 67\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "' '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-5c8b441b0161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_self_encoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: ' '"
     ]
    }
   ],
   "source": [
    "a = ' '.join(tktpl)\n",
    "print(a)\n",
    "for b in a:\n",
    "    print(b, enc_self_encoder[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### The *encode* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_encode(text):\n",
    "    bpe_tokens = []                                 \n",
    "    # for each token found by our regex (words, numbers, more than one space, punctuation)\n",
    "    for token in re.findall(enc_self_pat, text):     \n",
    "        # encode to utf-8 (char > int), then encode to byte, then join in a string\n",
    "        token = ''.join(enc_self_byte_encoder[b] for b in token.encode('utf-8'))\n",
    "        bpe_tokens.extend(enc_self_encoder[bpe_token] for bpe_token in enc_bpe(token).split(' '))\n",
    "    return bpe_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, our byte encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, '!'),\n",
       " (34, '\"'),\n",
       " (35, '#'),\n",
       " (36, '$'),\n",
       " (37, '%'),\n",
       " (38, '&'),\n",
       " (39, \"'\"),\n",
       " (40, '('),\n",
       " (41, ')'),\n",
       " (42, '*')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enc_self_byte_encoder.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- parse text for regexes, return as list;\n",
    "- encode each word as a list of utf-8 codes;\n",
    "- take these codes and transfer to bytes;\n",
    "- apply the `bpe` function;\n",
    "- encode the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'antidisestablishmentarianism'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original token: antidisestablishmentarianism\n",
      "In UTF-8: [97, 110, 116, 105, 100, 105, 115, 101, 115, 116, 97, 98, 108, 105, 115, 104, 109, 101, 110, 116, 97, 114, 105, 97, 110, 105, 115, 109]\n",
      "Now in bytes: ['a', 'n', 't', 'i', 'd', 'i', 's', 'e', 's', 't', 'a', 'b', 'l', 'i', 's', 'h', 'm', 'e', 'n', 't', 'a', 'r', 'i', 'a', 'n', 'i', 's', 'm']\n",
      "Result of the enc_bpe fn: ant idis establishment arian ism\n",
      "Result of encoder: [415, 29207, 44390, 3699, 1042]\n",
      "\n",
      "\n",
      "\n",
      "-----------------\n",
      "End result [415, 29207, 44390, 3699, 1042]\n"
     ]
    }
   ],
   "source": [
    "bpe_tkns = []\n",
    "for token in re.findall(enc_self_pat, text):\n",
    "    print('Original token:', token)\n",
    "\n",
    "    print('In UTF-8:', [b for b in token.encode('utf-8')])\n",
    "    print('Now in bytes:', [enc_self_byte_encoder[b] for b in token.encode('utf-8')])\n",
    "    token = ''.join(enc_self_byte_encoder[b] for b in token.encode('utf-8'))\n",
    "\n",
    "    print('Result of the enc_bpe fn:', enc_bpe(token))\n",
    "    print('Result of encoder:', [enc_self_encoder[bpe_token] for bpe_token in enc_bpe(token).split(' ')])\n",
    "    bpe_tkns.extend(enc_self_encoder[bpe_token] for bpe_token in enc_bpe(token).split(' '))\n",
    "    print()\n",
    "nprint()\n",
    "print('End result', bpe_tkns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### The *decode* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_decode(tokens):\n",
    "    # first decode from number to char\n",
    "    # then \n",
    "    text = ''.join([enc_self_decoder[token] for token in tokens])\n",
    "    text = bytearray([enc_self_byte_decoder[c] for c in text]).decode('utf-8', errors='replace')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method:\n",
    "- decode numbers to bytes;\n",
    "- decode bytes to utf-8;\n",
    "- decode utf-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hullo hullo\n",
      "[39, 84, 75, 75, 78, 220, 71, 84, 75, 75, 78]\n",
      "('H', 39) ('u', 84) ('l', 75) ('l', 75) ('o', 78) (' ', 220) ('h', 71) ('u', 84) ('l', 75) ('l', 75) ('o', 78)\n"
     ]
    }
   ],
   "source": [
    "txt = \"Hullo hullo\"\n",
    "tkns = enc_encode(txt)\n",
    "print(txt)\n",
    "print(tkns)\n",
    "print(*zip(list(txt), tkns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, our decoder and byte decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '!'), (1, '\"'), (2, '#'), (3, '$'), (4, '%'), (5, '&'), (6, \"'\"), (7, '('), (8, ')'), (9, '*')]\n",
      "[('!', 33), ('\"', 34), ('#', 35), ('$', 36), ('%', 37), ('&', 38), (\"'\", 39), ('(', 40), (')', 41), ('*', 42)]\n"
     ]
    }
   ],
   "source": [
    "print(list(enc_self_decoder.items())[:10])\n",
    "print(list(enc_self_byte_decoder.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'u', 'l', 'l', 'o', 'Ġ', 'h', 'u', 'l', 'l', 'o']\n"
     ]
    }
   ],
   "source": [
    "print([enc_self_decoder[tkn] for tkn in tkns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'Hullo hullo')\n",
      "Hullo hullo\n"
     ]
    }
   ],
   "source": [
    "tkstr = ''.join([enc_self_decoder[tkn] for tkn in tkns])\n",
    "print(bytearray([enc_self_byte_decoder[c] for c in tkstr]))\n",
    "print(bytearray([enc_self_byte_decoder[c] for c in tkstr]).decode('utf-8', errors='replace'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hullo hullo'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_decode(tkns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### The final wrap-up: loading the full encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc117 = get_encoder('117M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 0), ('\"', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9), ('+', 10), (',', 11), ('-', 12), ('.', 13), ('/', 14), ('0', 15), ('1', 16), ('2', 17), ('3', 18), ('4', 19), ('5', 20), ('6', 21), ('7', 22), ('8', 23), ('9', 24), (':', 25), (';', 26), ('<', 27), ('=', 28), ('>', 29), ('?', 30), ('@', 31), ('A', 32), ('B', 33), ('C', 34), ('D', 35), ('E', 36), ('F', 37), ('G', 38), ('H', 39), ('I', 40), ('J', 41), ('K', 42), ('L', 43), ('M', 44), ('N', 45), ('O', 46), ('P', 47), ('Q', 48), ('R', 49)]\n",
      "\n",
      "-----------------\n",
      "[('Ġkernels', 50207), ('ĠFranÃ§ois', 50208), ('ĠDuff', 50209), ('ĠPon', 50210), ('ĠLeica', 50211), ('ĠGarmin', 50212), ('Ġorphans', 50213), ('ĠClaudia', 50214), ('Ġcalendars', 50215), ('ĠLeilan', 50216), ('ento', 50217), ('Rocket', 50218), ('Ġbrunch', 50219), ('ĠHawking', 50220), ('ainers', 50221), ('Ġsensibilities', 50222), ('ĠkW', 50223), ('ĠKand', 50224), ('Ġreclaimed', 50225), ('Ġinterestingly', 50226), ('×©', 50227), ('romy', 50228), ('JM', 50229), ('ĠEnhancement', 50230), ('bush', 50231), ('Skip', 50232), ('Ġrappers', 50233), ('Ġgazing', 50234), ('pedia', 50235), ('athlon', 50236), ('Revolution', 50237), ('Ġsnipers', 50238), ('Ġreverted', 50239), ('Ġconglomerate', 50240), ('Terry', 50241), ('794', 50242), ('Ġharsher', 50243), ('Ġdesolate', 50244), ('ĠHitman', 50245), ('Commission', 50246), ('Ġ(/', 50247), ('âĢ¦.\"', 50248), ('Compar', 50249), ('Ġamplification', 50250), ('ominated', 50251), ('Ġregress', 50252), ('ĠCollider', 50253), ('Ġinformants', 50254), ('Ġgazed', 50255), ('<|endoftext|>', 50256)]\n",
      "\n",
      "-----------------\n",
      "[(0, '!'), (1, '\"'), (2, '#'), (3, '$'), (4, '%'), (5, '&'), (6, \"'\"), (7, '('), (8, ')'), (9, '*'), (10, '+'), (11, ','), (12, '-'), (13, '.'), (14, '/'), (15, '0'), (16, '1'), (17, '2'), (18, '3'), (19, '4'), (20, '5'), (21, '6'), (22, '7'), (23, '8'), (24, '9'), (25, ':'), (26, ';'), (27, '<'), (28, '='), (29, '>'), (30, '?'), (31, '@'), (32, 'A'), (33, 'B'), (34, 'C'), (35, 'D'), (36, 'E'), (37, 'F'), (38, 'G'), (39, 'H'), (40, 'I'), (41, 'J'), (42, 'K'), (43, 'L'), (44, 'M'), (45, 'N'), (46, 'O'), (47, 'P'), (48, 'Q'), (49, 'R')]\n",
      "\n",
      "-----------------\n",
      "[(50207, 'Ġkernels'), (50208, 'ĠFranÃ§ois'), (50209, 'ĠDuff'), (50210, 'ĠPon'), (50211, 'ĠLeica'), (50212, 'ĠGarmin'), (50213, 'Ġorphans'), (50214, 'ĠClaudia'), (50215, 'Ġcalendars'), (50216, 'ĠLeilan'), (50217, 'ento'), (50218, 'Rocket'), (50219, 'Ġbrunch'), (50220, 'ĠHawking'), (50221, 'ainers'), (50222, 'Ġsensibilities'), (50223, 'ĠkW'), (50224, 'ĠKand'), (50225, 'Ġreclaimed'), (50226, 'Ġinterestingly'), (50227, '×©'), (50228, 'romy'), (50229, 'JM'), (50230, 'ĠEnhancement'), (50231, 'bush'), (50232, 'Skip'), (50233, 'Ġrappers'), (50234, 'Ġgazing'), (50235, 'pedia'), (50236, 'athlon'), (50237, 'Revolution'), (50238, 'Ġsnipers'), (50239, 'Ġreverted'), (50240, 'Ġconglomerate'), (50241, 'Terry'), (50242, '794'), (50243, 'Ġharsher'), (50244, 'Ġdesolate'), (50245, 'ĠHitman'), (50246, 'Commission'), (50247, 'Ġ(/'), (50248, 'âĢ¦.\"'), (50249, 'Compar'), (50250, 'Ġamplification'), (50251, 'ominated'), (50252, 'Ġregress'), (50253, 'ĠCollider'), (50254, 'Ġinformants'), (50255, 'Ġgazed'), (50256, '<|endoftext|>')]\n",
      "\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "nprint(list(enc117.encoder.items())[:50])\n",
    "nprint(list(enc117.encoder.items())[-50:])\n",
    "nprint(list(enc117.decoder.items())[:50])\n",
    "nprint(list(enc117.decoder.items())[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(33, '!'), (34, '\"'), (35, '#'), (36, '$'), (37, '%'), (38, '&'), (39, \"'\"), (40, '('), (41, ')'), (42, '*'), (43, '+'), (44, ','), (45, '-'), (46, '.'), (47, '/'), (48, '0'), (49, '1'), (50, '2'), (51, '3'), (52, '4'), (53, '5'), (54, '6'), (55, '7'), (56, '8'), (57, '9'), (58, ':'), (59, ';'), (60, '<'), (61, '='), (62, '>'), (63, '?'), (64, '@'), (65, 'A'), (66, 'B'), (67, 'C'), (68, 'D'), (69, 'E'), (70, 'F'), (71, 'G'), (72, 'H'), (73, 'I'), (74, 'J'), (75, 'K'), (76, 'L'), (77, 'M'), (78, 'N'), (79, 'O'), (80, 'P'), (81, 'Q'), (82, 'R')]\n",
      "\n",
      "-----------------\n",
      "[(18, 'Ē'), (19, 'ē'), (20, 'Ĕ'), (21, 'ĕ'), (22, 'Ė'), (23, 'ė'), (24, 'Ę'), (25, 'ę'), (26, 'Ě'), (27, 'ě'), (28, 'Ĝ'), (29, 'ĝ'), (30, 'Ğ'), (31, 'ğ'), (32, 'Ġ'), (127, 'ġ'), (128, 'Ģ'), (129, 'ģ'), (130, 'Ĥ'), (131, 'ĥ'), (132, 'Ħ'), (133, 'ħ'), (134, 'Ĩ'), (135, 'ĩ'), (136, 'Ī'), (137, 'ī'), (138, 'Ĭ'), (139, 'ĭ'), (140, 'Į'), (141, 'į'), (142, 'İ'), (143, 'ı'), (144, 'Ĳ'), (145, 'ĳ'), (146, 'Ĵ'), (147, 'ĵ'), (148, 'Ķ'), (149, 'ķ'), (150, 'ĸ'), (151, 'Ĺ'), (152, 'ĺ'), (153, 'Ļ'), (154, 'ļ'), (155, 'Ľ'), (156, 'ľ'), (157, 'Ŀ'), (158, 'ŀ'), (159, 'Ł'), (160, 'ł'), (173, 'Ń')]\n",
      "\n",
      "-----------------\n",
      "[('!', 33), ('\"', 34), ('#', 35), ('$', 36), ('%', 37), ('&', 38), (\"'\", 39), ('(', 40), (')', 41), ('*', 42), ('+', 43), (',', 44), ('-', 45), ('.', 46), ('/', 47), ('0', 48), ('1', 49), ('2', 50), ('3', 51), ('4', 52), ('5', 53), ('6', 54), ('7', 55), ('8', 56), ('9', 57), (':', 58), (';', 59), ('<', 60), ('=', 61), ('>', 62), ('?', 63), ('@', 64), ('A', 65), ('B', 66), ('C', 67), ('D', 68), ('E', 69), ('F', 70), ('G', 71), ('H', 72), ('I', 73), ('J', 74), ('K', 75), ('L', 76), ('M', 77), ('N', 78), ('O', 79), ('P', 80), ('Q', 81), ('R', 82)]\n",
      "\n",
      "-----------------\n",
      "[('Ē', 18), ('ē', 19), ('Ĕ', 20), ('ĕ', 21), ('Ė', 22), ('ė', 23), ('Ę', 24), ('ę', 25), ('Ě', 26), ('ě', 27), ('Ĝ', 28), ('ĝ', 29), ('Ğ', 30), ('ğ', 31), ('Ġ', 32), ('ġ', 127), ('Ģ', 128), ('ģ', 129), ('Ĥ', 130), ('ĥ', 131), ('Ħ', 132), ('ħ', 133), ('Ĩ', 134), ('ĩ', 135), ('Ī', 136), ('ī', 137), ('Ĭ', 138), ('ĭ', 139), ('Į', 140), ('į', 141), ('İ', 142), ('ı', 143), ('Ĳ', 144), ('ĳ', 145), ('Ĵ', 146), ('ĵ', 147), ('Ķ', 148), ('ķ', 149), ('ĸ', 150), ('Ĺ', 151), ('ĺ', 152), ('Ļ', 153), ('ļ', 154), ('Ľ', 155), ('ľ', 156), ('Ŀ', 157), ('ŀ', 158), ('Ł', 159), ('ł', 160), ('Ń', 173)]\n",
      "\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "nprint(list(enc117.byte_encoder.items())[:50])\n",
    "nprint(list(enc117.byte_encoder.items())[-50:])\n",
    "nprint(list(enc117.byte_decoder.items())[:50])\n",
    "nprint(list(enc117.byte_decoder.items())[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Ġ', 't'), 0), (('Ġ', 'a'), 1), (('h', 'e'), 2), (('i', 'n'), 3), (('r', 'e'), 4), (('o', 'n'), 5), (('Ġt', 'he'), 6), (('e', 'r'), 7), (('Ġ', 's'), 8), (('a', 't'), 9), (('Ġ', 'w'), 10), (('Ġ', 'o'), 11), (('e', 'n'), 12), (('Ġ', 'c'), 13), (('i', 't'), 14), (('i', 's'), 15), (('a', 'n'), 16), (('o', 'r'), 17), (('e', 's'), 18), (('Ġ', 'b'), 19), (('e', 'd'), 20), (('Ġ', 'f'), 21), (('in', 'g'), 22), (('Ġ', 'p'), 23), (('o', 'u'), 24), (('Ġa', 'n'), 25), (('a', 'l'), 26), (('a', 'r'), 27), (('Ġt', 'o'), 28), (('Ġ', 'm'), 29), (('Ġo', 'f'), 30), (('Ġ', 'in'), 31), (('Ġ', 'd'), 32), (('Ġ', 'h'), 33), (('Ġan', 'd'), 34), (('i', 'c'), 35), (('a', 's'), 36), (('l', 'e'), 37), (('Ġt', 'h'), 38), (('i', 'on'), 39), (('o', 'm'), 40), (('l', 'l'), 41), (('en', 't'), 42), (('Ġ', 'n'), 43), (('Ġ', 'l'), 44), (('s', 't'), 45), (('Ġ', 're'), 46), (('v', 'e'), 47), (('Ġ', 'e'), 48), (('r', 'o'), 49)]\n",
      "\n",
      "-----------------\n",
      "[(('ĠCan', 'ary'), 49950), (('Ġk', 'ernels'), 49951), (('ĠFranÃ§', 'ois'), 49952), (('ĠD', 'uff'), 49953), (('ĠP', 'on'), 49954), (('ĠLe', 'ica'), 49955), (('ĠGar', 'min'), 49956), (('Ġor', 'phans'), 49957), (('ĠClaud', 'ia'), 49958), (('Ġcal', 'endars'), 49959), (('ĠLe', 'ilan'), 49960), (('ent', 'o'), 49961), (('R', 'ocket'), 49962), (('Ġbr', 'unch'), 49963), (('ĠHaw', 'king'), 49964), (('ain', 'ers'), 49965), (('Ġsens', 'ibilities'), 49966), (('Ġk', 'W'), 49967), (('ĠK', 'and'), 49968), (('Ġre', 'claimed'), 49969), (('Ġinteresting', 'ly'), 49970), (('×', '©'), 49971), (('rom', 'y'), 49972), (('J', 'M'), 49973), (('ĠEnhance', 'ment'), 49974), (('b', 'ush'), 49975), (('Sk', 'ip'), 49976), (('Ġrapp', 'ers'), 49977), (('Ġg', 'azing'), 49978), (('p', 'edia'), 49979), (('ath', 'lon'), 49980), (('Rev', 'olution'), 49981), (('Ġsn', 'ipers'), 49982), (('Ġre', 'verted'), 49983), (('Ġconglomer', 'ate'), 49984), (('T', 'erry'), 49985), (('79', '4'), 49986), (('Ġhars', 'her'), 49987), (('Ġdes', 'olate'), 49988), (('ĠHit', 'man'), 49989), (('Comm', 'ission'), 49990), (('Ġ(', '/'), 49991), (('âĢ¦', '.\"'), 49992), (('Com', 'par'), 49993), (('Ġampl', 'ification'), 49994), (('om', 'inated'), 49995), (('Ġreg', 'ress'), 49996), (('ĠColl', 'ider'), 49997), (('Ġinform', 'ants'), 49998), (('Ġg', 'azed'), 49999)]\n",
      "\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "nprint(list(enc117.bpe_ranks.items())[:50])\n",
    "nprint(list(enc117.bpe_ranks.items())[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[818, 12867, 4583, 290, 7869, 11, 262, 32623, 1906, 2484, 8825, 43366, 318, 257, 2446, 286, 15964, 262, 26789, 1022, 734, 12867, 24570, 13, 632, 318, 635, 1900, 355, 1321, 16874, 357, 4663, 324, 38381, 16, 60, 393, 2472, 43366, 284, 262, 2811, 3693, 17, 60, 632, 318, 1912, 319, 262, 509, 724, 1891, 1906, 3123, 571, 1754, 43366, 11, 351, 617, 12411, 357, 392, 4465, 8, 5400, 11, 1390, 326, 340, 318, 23606, 19482, 290, 340, 1464, 468, 257, 27454, 1988, 13, 383, 6616, 6808, 286, 262, 32623, 1906, 2484, 8825, 43366, 318, 257, 18663, 1690, 6412, 284, 355, 32623, 12, 2484, 8825, 5253, 3693, 18, 7131, 19, 7131, 20, 60]\n",
      "\n",
      "In probability theory and statistics, the Jensen–Shannon divergence is a method of measuring the similarity between two probability distributions. It is also known as information radius (IRad)[1] or total divergence to the average.[2] It is based on the Kullback–Leibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value. The square root of the Jensen–Shannon divergence is a metric often referred to as Jensen-Shannon distance.[3][4][5]\n"
     ]
    }
   ],
   "source": [
    "text = \"In probability theory and statistics, the Jensen–Shannon divergence is a method of measuring the similarity between two probability distributions. It is also known as information radius (IRad)[1] or total divergence to the average.[2] It is based on the Kullback–Leibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value. The square root of the Jensen–Shannon divergence is a metric often referred to as Jensen-Shannon distance.[3][4][5]\"\n",
    "tok117 = enc117.encode(text)\n",
    "print(tok117)\n",
    "print()\n",
    "txt117 = enc117.decode(tok117)\n",
    "print(txt117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('Ġ', 't'), 0), (('Ġ', 'a'), 1), (('h', 'e'), 2), (('i', 'n'), 3), (('r', 'e'), 4), (('o', 'n'), 5), (('Ġt', 'he'), 6), (('e', 'r'), 7), (('Ġ', 's'), 8), (('a', 't'), 9)]\n"
     ]
    }
   ],
   "source": [
    "print(list(enc117.bpe_ranks.items())[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
