{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    x = x - tf.reduce_max(x, axis=axis, keepdims=True)\n",
    "    ex = tf.exp(x)\n",
    "    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where does this business with the reduce_max comes from? \n",
    "\n",
    "The regular softmax is computed using the tf built-in functions in [GPT-1](https://github.com/openai/finetune-transformer-lm/blob/master/train.py). \n",
    "\n",
    "[This](https://github.com/purple-worthy/shendu-xuexi-python/blob/f75a43cc98c0cb9c004ff4a6dfdfa72f41445143/_codes/my_tensorflow/src/activations/__init__.py#L49) and [that](https://github.com/yifannieumontreal/artifact/blob/81f78c9ad7b9c10cc13b53ccb2fa3fe30ed07405/lib_tf/tf_utils.py#L15) have this feature. One better reference: the [Keras implementation](https://github.com/keras-team/keras/blob/bd024a1fc1cd6d88e8bc5da148968ff5e079caeb/keras/activations.py#L14) has it in this form.  \n",
    "Or, rather, it is a form of normalization (pushing all values below zero, with the max value becoming zero...)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "----------\n",
      "innermost max\n",
      "[[3.]\n",
      " [6.]\n",
      " [9.]]\n",
      "----------\n",
      "x2: x - max\n",
      "[[-2. -1.  0.]\n",
      " [-2. -1.  0.]\n",
      " [-2. -1.  0.]]\n",
      "----------\n",
      "ex: exp(x2)\n",
      "[[0.13533528 0.36787945 1.        ]\n",
      " [0.13533528 0.36787945 1.        ]\n",
      " [0.13533528 0.36787945 1.        ]]\n",
      "----------\n",
      "ex/sum(ex)\n",
      "[[0.09003057 0.24472848 0.66524094]\n",
      " [0.09003057 0.24472848 0.66524094]\n",
      " [0.09003057 0.24472848 0.66524094]]\n",
      "----------\n",
      "softmax\n",
      "[[0.09003057 0.24472848 0.66524094]\n",
      " [0.09003057 0.24472848 0.66524094]\n",
      " [0.09003057 0.24472848 0.66524094]]\n",
      "----------\n",
      "all equal?\n",
      "[[ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]]\n",
      "----------\n",
      "and instead the regular softmax\n",
      "[[0.09003057 0.24472848 0.66524094]\n",
      " [0.09003057 0.24472848 0.66524094]\n",
      " [0.09003057 0.24472848 0.66524094]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1.,2.,3.],\n",
    "                 [4.,5.,6.],\n",
    "                 [7.,8.,9.]])\n",
    "\n",
    "print('x',x.numpy(), sep='\\n', end='\\n----------\\n')\n",
    "print('innermost max', tf.reduce_max(x, axis=-1, keepdims=True).numpy(), sep='\\n', end='\\n----------\\n')\n",
    "x2 = x - tf.reduce_max(x, axis=-1, keepdims=True)\n",
    "print('x2: x - max', x2.numpy(), sep='\\n', end='\\n----------\\n')\n",
    "ex = tf.exp(x2)\n",
    "print('ex: exp(x2)', ex.numpy(), sep='\\n', end='\\n----------\\n')\n",
    "print('ex/sum(ex)', (ex/tf.reduce_sum(ex, axis=-1, keepdims=True)).numpy(), sep='\\n', end='\\n----------\\n')\n",
    "print('softmax', softmax(x).numpy(), sep='\\n', end='\\n----------\\n')\n",
    "print('all equal?', softmax(x).numpy() == (ex/tf.reduce_sum(ex, axis=-1, keepdims=True)).numpy(), sep='\\n', end='\\n----------\\n')\n",
    "print('and instead the regular softmax', tf.nn.softmax(x).numpy(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Same with another variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "[[1. 1.]\n",
      " [3. 2.]\n",
      " [5. 9.]]\n",
      "----------\n",
      "innermost max\n",
      "[[1.]\n",
      " [3.]\n",
      " [9.]]\n",
      "----------\n",
      "y2: y - max\n",
      "[[ 0.  0.]\n",
      " [ 0. -1.]\n",
      " [-4.  0.]]\n",
      "----------\n",
      "exy: exp(y2)\n",
      "[[1.         1.        ]\n",
      " [1.         0.36787945]\n",
      " [0.01831564 1.        ]]\n",
      "----------\n",
      "exy/sum(exy)\n",
      "[[0.5        0.5       ]\n",
      " [0.7310586  0.26894143]\n",
      " [0.01798621 0.98201376]]\n",
      "----------\n",
      "softmax\n",
      "[[0.5        0.5       ]\n",
      " [0.7310586  0.26894143]\n",
      " [0.01798621 0.98201376]]\n",
      "----------\n",
      "all equal?\n",
      "[[ True  True]\n",
      " [ True  True]\n",
      " [ True  True]]\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant([[1., 1.],\n",
    "                [3.,2.],\n",
    "                [5., 9.]])\n",
    "print('y',y.numpy(), sep='\\n', end='\\n----------\\n')\n",
    "print('innermost max', tf.reduce_max(y, axis=-1, keepdims=True).numpy(), sep='\\n', end='\\n----------\\n')\n",
    "y2 = y - tf.reduce_max(y, axis=-1, keepdims=True)\n",
    "print('y2: y - max', y2.numpy(), sep='\\n', end='\\n----------\\n')\n",
    "exy = tf.exp(y2)\n",
    "print('exy: exp(y2)', exy.numpy(), sep='\\n', end='\\n----------\\n')\n",
    "print('exy/sum(exy)', (exy/tf.reduce_sum(exy, axis=-1, keepdims=True)).numpy(), sep='\\n', end='\\n----------\\n')\n",
    "print('softmax', softmax(y).numpy(), sep='\\n', end='\\n----------\\n')\n",
    "print('all equal?', softmax(y).numpy() == (exy/tf.reduce_sum(exy, axis=-1, keepdims=True)).numpy(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Hunch: maybe it is performance issue? The current function being faster than the [built-in one](https://www.tensorflow.org/api_docs/python/tf/nn/softmax)? Given the tests below, it does look like this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.7 µs ± 2.71 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.2 µs ± 349 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tf.nn.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps different with a larger tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 14.639416    12.471731  ]\n",
      "  [  6.287172   -10.907554  ]\n",
      "  [-29.439354    -0.95316064]]\n",
      "\n",
      " [[-15.58526      6.7059565 ]\n",
      "  [ 48.06688    -12.80599   ]\n",
      "  [ 30.347822    25.95503   ]]\n",
      "\n",
      " [[  3.0517344    5.536769  ]\n",
      "  [  6.2067804   19.347296  ]\n",
      "  [-24.396057    19.345951  ]]\n",
      "\n",
      " [[ 31.135847   -21.138882  ]\n",
      "  [  4.2857533  -42.758995  ]\n",
      "  [-52.921906    16.973293  ]]], shape=(4, 3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t1 = tf.random.normal([4,3,2],\n",
    "                     mean=0.0,\n",
    "                     stddev=20,\n",
    "                     dtype=tf.float32)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.1 µs ± 7.59 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "softmax(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.9 µs ± 1.1 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tf.nn.softmax(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = tf.random.normal([36,10, 4, 3,2], \n",
    "                      mean=0.0, \n",
    "                      stddev=20,\n",
    "                      dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521 µs ± 50.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "softmax(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 µs ± 10.7 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tf.nn.softmax(t2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
