{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Some fluff first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "----------\n",
      "orig -2\n",
      "[[-1.  0.  1.]\n",
      " [ 2.  3.  4.]]\n",
      "----------\n",
      "squared\n",
      "[[ 1.  0.  1.]\n",
      " [ 4.  9. 16.]]\n",
      "----------\n",
      "sqr = x^1/2\n",
      "[[1. 0. 1.]\n",
      " [2. 3. 4.]]\n",
      "----------\n",
      "rsqr: 1/(x^1/2), the inf at [0,1] being the reason why they use epsilon\n",
      "[[1.                inf 1.        ]\n",
      " [0.5        0.33333334 0.25      ]]\n",
      "----------\n",
      "rsqr: 1/(x^1/2), with epsilon\n",
      "[[9.9999499e-01 3.1622778e+02 9.9999499e-01]\n",
      " [4.9999940e-01 3.3333313e-01 2.4999994e-01]]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1.,2.,3.],[4.,5.,6.]])\n",
    "print('orig', x.numpy(), sep='\\n', end='\\n----------\\n')\n",
    "y = x - 2.\n",
    "print('orig -2', y.numpy(), sep='\\n', end='\\n----------\\n')\n",
    "ysq = tf.square(y)\n",
    "print('squared', ysq.numpy(), sep='\\n', end='\\n----------\\n')\n",
    "print('sqr = x^1/2', tf.sqrt(ysq).numpy(), sep='\\n', end='\\n----------\\n')\n",
    "print('rsqr: 1/(x^1/2), the inf at [0,1] being the reason why they use epsilon', tf.rsqrt(ysq).numpy(), sep='\\n', end='\\n----------\\n') \n",
    "print('rsqr: 1/(x^1/2), with epsilon', tf.rsqrt(ysq + 1e-5).numpy(), sep='\\n', end='\\n----------\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function\n",
    "\n",
    "Deets:\n",
    "- axis=-1: reduce_mean applied to innermost dimension.  \n",
    "- shape[-1]: same  \n",
    "- using rsqrt instead of 1/sqrt\n",
    "\n",
    "The batch norm equations, from the [original paper](https://arxiv.org/pdf/1502.03167v3.pdf), found [here](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c):  \n",
    "![norm](batch_norm.png \"Batch Normalization Equations\")\n",
    "\n",
    "See also these videos by Andrew Ng:\n",
    "- [Normalizing inputs](https://www.youtube.com/watch?v=FDCfw-YqWTE&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=9)\n",
    "- [Batch Normalization](https://www.youtube.com/watch?v=tNIpEZLv_eg&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=27).\n",
    "\n",
    "\n",
    "N.B: a * as as a function argument means: 'no positional arguments after this point', cf. [here](https://stackoverflow.com/a/53797072) and [there](https://www.python.org/dev/peps/pep-3102/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
    "    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        # take the innermost dimension\n",
    "        n_state = x.shape[-1].value\n",
    "        \n",
    "        # weight & bias that will be trained\n",
    "        g = tf.get_variable('g', \n",
    "                            [n_state], \n",
    "                            initializer=tf.constant_initializer(1))\n",
    "        b = tf.get_variable('b', \n",
    "                            [n_state], \n",
    "                            initializer=tf.constant_initializer(0))\n",
    "        \n",
    "        # take the absolute mean\n",
    "        u = tf.reduce_mean(x, axis=axis, keepdims=True)\n",
    "        # take the variance\n",
    "        s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n",
    "        # normalization\n",
    "        x = (x - u) * tf.rsqrt(s + epsilon)\n",
    "        # scaling & shifting using weight & bias\n",
    "        x = x*g + b\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
