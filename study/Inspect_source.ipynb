{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect module for source code\n",
    "\n",
    "Fiddling with the softmax function, I was led to search into the TF source code, and encountered issues with regard to `bazel`-generated files, cf. [this discussion](https://stackoverflow.com/a/41149557). A few steps to find the source files on a local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import gen_nn_ops\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not actually useful this time, but possibly for the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   '_BatchNormWithGlobalNormalizationGradOutput': <class 'tensorflow.python.ops.gen_nn_ops.BatchNormWithGlobalNormalizationGrad'>,\n",
      "    '_FractionalAvgPoolOutput': <class 'tensorflow.python.ops.gen_nn_ops.FractionalAvgPool'>,\n",
      "    '_FractionalMaxPoolOutput': <class 'tensorflow.python.ops.gen_nn_ops.FractionalMaxPool'>,\n",
      "    '_FusedBatchNormGradOutput': <class 'tensorflow.python.ops.gen_nn_ops.FusedBatchNormGrad'>,\n",
      "    '_FusedBatchNormGradV2Output': <class 'tensorflow.python.ops.gen_nn_ops.FusedBatchNormGradV2'>,\n",
      "    '_FusedBatchNormOutput': <class 'tensorflow.python.ops.gen_nn_ops.FusedBatchNorm'>,\n",
      "    '_FusedBatchNormV2Output': <class 'tensorflow.python.ops.gen_nn_ops.FusedBatchNormV2'>,\n",
      "    '_InitOpDefLibrary': <function _InitOpDefLibrary at 0x7f4b3a9d21e0>,\n",
      "    '_MaxPoolWithArgmaxOutput': <class 'tensorflow.python.ops.gen_nn_ops.MaxPoolWithArgmax'>,\n",
      "    '_QuantizedAvgPoolOutput': <class 'tensorflow.python.ops.gen_nn_ops.QuantizedAvgPool'>,\n",
      "    '_QuantizedBatchNormWithGlobalNormalizationOutput': <class 'tensorflow.python.ops.gen_nn_ops.QuantizedBatchNormWithGlobalNormalization'>,\n",
      "    '_QuantizedBiasAddOutput': <class 'tensorflow.python.ops.gen_nn_ops.QuantizedBiasAdd'>,\n",
      "    '_QuantizedConv2DOutput': <class 'tensorflow.python.ops.gen_nn_ops.QuantizedConv2D'>,\n",
      "    '_QuantizedMaxPoolOutput': <class 'tensorflow.python.ops.gen_nn_ops.QuantizedMaxPool'>,\n",
      "    '_QuantizedRelu6Output': <class 'tensorflow.python.ops.gen_nn_ops.QuantizedRelu6'>,\n",
      "    '_QuantizedReluOutput': <class 'tensorflow.python.ops.gen_nn_ops.QuantizedRelu'>,\n",
      "    '_QuantizedReluXOutput': <class 'tensorflow.python.ops.gen_nn_ops.QuantizedReluX'>,\n",
      "    '_SoftmaxCrossEntropyWithLogitsOutput': <class 'tensorflow.python.ops.gen_nn_ops.SoftmaxCrossEntropyWithLogits'>,\n",
      "    '_SparseSoftmaxCrossEntropyWithLogitsOutput': <class 'tensorflow.python.ops.gen_nn_ops.SparseSoftmaxCrossEntropyWithLogits'>,\n",
      "    '_TopKOutput': <class 'tensorflow.python.ops.gen_nn_ops.TopK'>,\n",
      "    '_TopKV2Output': <class 'tensorflow.python.ops.gen_nn_ops.TopKV2'>,\n",
      "    '__builtins__': {   'ArithmeticError': <class 'ArithmeticError'>,\n",
      "                        'AssertionError': <class 'AssertionError'>,\n",
      "                        'AttributeError': <class 'AttributeError'>,\n",
      "                        'BaseException': <class 'BaseException'>,\n",
      "                        'BlockingIOError': <class 'BlockingIOError'>,\n",
      "                        'BrokenPipeError': <class 'BrokenPipeError'>,\n",
      "                        'BufferError': <class 'BufferError'>,\n",
      "                        'BytesWarning': <class 'BytesWarning'>,\n",
      "                        'ChildProcessError': <class 'ChildProcessError'>,\n",
      "                        'ConnectionAbortedError': <class 'ConnectionAbortedError'>,\n",
      "                        'ConnectionError': <class 'ConnectionError'>,\n",
      "                        'ConnectionRefusedError': <class 'ConnectionRefusedError'>,\n",
      "                        'ConnectionResetError': <class 'ConnectionResetError'>,\n",
      "                        'DeprecationWarning': <class 'DeprecationWarning'>,\n",
      "                        'EOFError': <class 'EOFError'>,\n",
      "                        'Ellipsis': Ellipsis,\n",
      "                        'EnvironmentError': <class 'OSError'>,\n",
      "                        'Exception': <class 'Exception'>,\n",
      "                        'False': False,\n",
      "                        'FileExistsError': <class 'FileExistsError'>,\n",
      "                        'FileNotFoundError': <class 'FileNotFoundError'>,\n",
      "                        'FloatingPointError': <class 'FloatingPointError'>,\n",
      "                        'FutureWarning': <class 'FutureWarning'>,\n",
      "                        'GeneratorExit': <class 'GeneratorExit'>,\n",
      "                        'IOError': <class 'OSError'>,\n",
      "                        'ImportError': <class 'ImportError'>,\n",
      "                        'ImportWarning': <class 'ImportWarning'>,\n",
      "                        'IndentationError': <class 'IndentationError'>,\n",
      "                        'IndexError': <class 'IndexError'>,\n",
      "                        'InterruptedError': <class 'InterruptedError'>,\n",
      "                        'IsADirectoryError': <class 'IsADirectoryError'>,\n",
      "                        'KeyError': <class 'KeyError'>,\n",
      "                        'KeyboardInterrupt': <class 'KeyboardInterrupt'>,\n",
      "                        'LookupError': <class 'LookupError'>,\n",
      "                        'MemoryError': <class 'MemoryError'>,\n",
      "                        'ModuleNotFoundError': <class 'ModuleNotFoundError'>,\n",
      "                        'NameError': <class 'NameError'>,\n",
      "                        'None': None,\n",
      "                        'NotADirectoryError': <class 'NotADirectoryError'>,\n",
      "                        'NotImplemented': NotImplemented,\n",
      "                        'NotImplementedError': <class 'NotImplementedError'>,\n",
      "                        'OSError': <class 'OSError'>,\n",
      "                        'OverflowError': <class 'OverflowError'>,\n",
      "                        'PendingDeprecationWarning': <class 'PendingDeprecationWarning'>,\n",
      "                        'PermissionError': <class 'PermissionError'>,\n",
      "                        'ProcessLookupError': <class 'ProcessLookupError'>,\n",
      "                        'RecursionError': <class 'RecursionError'>,\n",
      "                        'ReferenceError': <class 'ReferenceError'>,\n",
      "                        'ResourceWarning': <class 'ResourceWarning'>,\n",
      "                        'RuntimeError': <class 'RuntimeError'>,\n",
      "                        'RuntimeWarning': <class 'RuntimeWarning'>,\n",
      "                        'StopAsyncIteration': <class 'StopAsyncIteration'>,\n",
      "                        'StopIteration': <class 'StopIteration'>,\n",
      "                        'SyntaxError': <class 'SyntaxError'>,\n",
      "                        'SyntaxWarning': <class 'SyntaxWarning'>,\n",
      "                        'SystemError': <class 'SystemError'>,\n",
      "                        'SystemExit': <class 'SystemExit'>,\n",
      "                        'TabError': <class 'TabError'>,\n",
      "                        'TimeoutError': <class 'TimeoutError'>,\n",
      "                        'True': True,\n",
      "                        'TypeError': <class 'TypeError'>,\n",
      "                        'UnboundLocalError': <class 'UnboundLocalError'>,\n",
      "                        'UnicodeDecodeError': <class 'UnicodeDecodeError'>,\n",
      "                        'UnicodeEncodeError': <class 'UnicodeEncodeError'>,\n",
      "                        'UnicodeError': <class 'UnicodeError'>,\n",
      "                        'UnicodeTranslateError': <class 'UnicodeTranslateError'>,\n",
      "                        'UnicodeWarning': <class 'UnicodeWarning'>,\n",
      "                        'UserWarning': <class 'UserWarning'>,\n",
      "                        'ValueError': <class 'ValueError'>,\n",
      "                        'Warning': <class 'Warning'>,\n",
      "                        'ZeroDivisionError': <class 'ZeroDivisionError'>,\n",
      "                        '__IPYTHON__': True,\n",
      "                        '__build_class__': <built-in function __build_class__>,\n",
      "                        '__debug__': True,\n",
      "                        '__doc__': 'Built-in functions, exceptions, and other '\n",
      "                                   'objects.\\n'\n",
      "                                   '\\n'\n",
      "                                   \"Noteworthy: None is the `nil' object; \"\n",
      "                                   \"Ellipsis represents `...' in slices.\",\n",
      "                        '__import__': <built-in function __import__>,\n",
      "                        '__loader__': <class '_frozen_importlib.BuiltinImporter'>,\n",
      "                        '__name__': 'builtins',\n",
      "                        '__package__': '',\n",
      "                        '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>),\n",
      "                        'abs': <built-in function abs>,\n",
      "                        'all': <built-in function all>,\n",
      "                        'any': <built-in function any>,\n",
      "                        'ascii': <built-in function ascii>,\n",
      "                        'bin': <built-in function bin>,\n",
      "                        'bool': <class 'bool'>,\n",
      "                        'bytearray': <class 'bytearray'>,\n",
      "                        'bytes': <class 'bytes'>,\n",
      "                        'callable': <built-in function callable>,\n",
      "                        'chr': <built-in function chr>,\n",
      "                        'classmethod': <class 'classmethod'>,\n",
      "                        'compile': <built-in function compile>,\n",
      "                        'complex': <class 'complex'>,\n",
      "                        'copyright': Copyright (c) 2001-2018 Python Software Foundation.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 2000 BeOpen.com.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
      "All Rights Reserved.\n",
      "\n",
      "Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
      "All Rights Reserved.,\n",
      "                        'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n",
      "    for supporting Python development.  See www.python.org for more information.,\n",
      "                        'delattr': <built-in function delattr>,\n",
      "                        'dict': <class 'dict'>,\n",
      "                        'dir': <built-in function dir>,\n",
      "                        'display': <function display at 0x7f4b73fb4b70>,\n",
      "                        'divmod': <built-in function divmod>,\n",
      "                        'enumerate': <class 'enumerate'>,\n",
      "                        'eval': <built-in function eval>,\n",
      "                        'exec': <built-in function exec>,\n",
      "                        'filter': <class 'filter'>,\n",
      "                        'float': <class 'float'>,\n",
      "                        'format': <built-in function format>,\n",
      "                        'frozenset': <class 'frozenset'>,\n",
      "                        'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f4b70b4f0b8>>,\n",
      "                        'getattr': <built-in function getattr>,\n",
      "                        'globals': <built-in function globals>,\n",
      "                        'hasattr': <built-in function hasattr>,\n",
      "                        'hash': <built-in function hash>,\n",
      "                        'help': Type help() for interactive help, or help(object) for help about object.,\n",
      "                        'hex': <built-in function hex>,\n",
      "                        'id': <built-in function id>,\n",
      "                        'input': <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7f4b70b53ef0>>,\n",
      "                        'int': <class 'int'>,\n",
      "                        'isinstance': <built-in function isinstance>,\n",
      "                        'issubclass': <built-in function issubclass>,\n",
      "                        'iter': <built-in function iter>,\n",
      "                        'len': <built-in function len>,\n",
      "                        'license': Type license() to see the full license text,\n",
      "                        'list': <class 'list'>,\n",
      "                        'locals': <built-in function locals>,\n",
      "                        'map': <class 'map'>,\n",
      "                        'max': <built-in function max>,\n",
      "                        'memoryview': <class 'memoryview'>,\n",
      "                        'min': <built-in function min>,\n",
      "                        'next': <built-in function next>,\n",
      "                        'object': <class 'object'>,\n",
      "                        'oct': <built-in function oct>,\n",
      "                        'open': <built-in function open>,\n",
      "                        'ord': <built-in function ord>,\n",
      "                        'pow': <built-in function pow>,\n",
      "                        'print': <built-in function print>,\n",
      "                        'property': <class 'property'>,\n",
      "                        'range': <class 'range'>,\n",
      "                        'repr': <built-in function repr>,\n",
      "                        'reversed': <class 'reversed'>,\n",
      "                        'round': <built-in function round>,\n",
      "                        'set': <class 'set'>,\n",
      "                        'setattr': <built-in function setattr>,\n",
      "                        'slice': <class 'slice'>,\n",
      "                        'sorted': <built-in function sorted>,\n",
      "                        'staticmethod': <class 'staticmethod'>,\n",
      "                        'str': <class 'str'>,\n",
      "                        'sum': <built-in function sum>,\n",
      "                        'super': <class 'super'>,\n",
      "                        'tuple': <class 'tuple'>,\n",
      "                        'type': <class 'type'>,\n",
      "                        'vars': <built-in function vars>,\n",
      "                        'zip': <class 'zip'>},\n",
      "    '__cached__': '/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/__pycache__/gen_nn_ops.cpython-36.pyc',\n",
      "    '__doc__': 'Python wrappers around TensorFlow ops.\\n'\n",
      "               '\\n'\n",
      "               'This file is MACHINE GENERATED! Do not edit.\\n'\n",
      "               'Original C++ source file: nn_ops.cc\\n',\n",
      "    '__file__': '/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py',\n",
      "    '__fused_batch_norm_outputs': [   'y',\n",
      "                                      'batch_mean',\n",
      "                                      'batch_variance',\n",
      "                                      'reserve_space_1',\n",
      "                                      'reserve_space_2'],\n",
      "    '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7f4b3aa44c18>,\n",
      "    '__name__': 'tensorflow.python.ops.gen_nn_ops',\n",
      "    '__package__': 'tensorflow.python.ops',\n",
      "    '__spec__': ModuleSpec(name='tensorflow.python.ops.gen_nn_ops', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f4b3aa44c18>, origin='/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py'),\n",
      "    '_batch_norm_with_global_normalization': <function _batch_norm_with_global_normalization at 0x7f4b3a9a8488>,\n",
      "    '_batch_norm_with_global_normalization_eager_fallback': <function _batch_norm_with_global_normalization_eager_fallback at 0x7f4b3a9a8510>,\n",
      "    '_batch_norm_with_global_normalization_grad_outputs': [   'dx',\n",
      "                                                              'dm',\n",
      "                                                              'dv',\n",
      "                                                              'db',\n",
      "                                                              'dg'],\n",
      "    '_collections': <module 'collections' from '/home/default/miniconda3/lib/python3.6/collections/__init__.py'>,\n",
      "    '_common_shapes': <module 'tensorflow.python.framework.common_shapes' from '/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py'>,\n",
      "    '_context': <module 'tensorflow.python.eager.context' from '/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/eager/context.py'>,\n",
      "    '_core': <module 'tensorflow.python.eager.core' from '/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/eager/core.py'>,\n",
      "    '_dtypes': <module 'tensorflow.python.framework.dtypes' from '/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py'>,\n",
      "    '_errors': <module 'tensorflow.python.framework.errors' from '/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors.py'>,\n",
      "    '_execute': <module 'tensorflow.python.eager.execute' from '/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py'>,\n",
      "    '_fractional_avg_pool_outputs': [   'output',\n",
      "                                        'row_pooling_sequence',\n",
      "                                        'col_pooling_sequence'],\n",
      "    '_fractional_max_pool_outputs': [   'output',\n",
      "                                        'row_pooling_sequence',\n",
      "                                        'col_pooling_sequence'],\n",
      "    '_fused_batch_norm': <function _fused_batch_norm at 0x7f4b3a9adea0>,\n",
      "    '_fused_batch_norm_eager_fallback': <function _fused_batch_norm_eager_fallback at 0x7f4b3a9bb2f0>,\n",
      "    '_fused_batch_norm_grad_outputs': [   'x_backprop',\n",
      "                                          'scale_backprop',\n",
      "                                          'offset_backprop',\n",
      "                                          'reserve_space_3',\n",
      "                                          'reserve_space_4'],\n",
      "    '_fused_batch_norm_grad_v2_outputs': [   'x_backprop',\n",
      "                                             'scale_backprop',\n",
      "                                             'offset_backprop',\n",
      "                                             'reserve_space_3',\n",
      "                                             'reserve_space_4'],\n",
      "    '_fused_batch_norm_v2_outputs': [   'y',\n",
      "                                        'batch_mean',\n",
      "                                        'batch_variance',\n",
      "                                        'reserve_space_1',\n",
      "                                        'reserve_space_2'],\n",
      "    '_max_pool_with_argmax_outputs': ['output', 'argmax'],\n",
      "    '_op_def_lib': <tensorflow.python.framework.op_def_library.OpDefLibrary object at 0x7f4b3a9b9f98>,\n",
      "    '_op_def_library': <module 'tensorflow.python.framework.op_def_library' from '/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py'>,\n",
      "    '_op_def_pb2': <module 'tensorflow.core.framework.op_def_pb2' from '/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/core/framework/op_def_pb2.py'>,\n",
      "    '_op_def_registry': <module 'tensorflow.python.framework.op_def_registry' from '/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_registry.py'>,\n",
      "    '_ops': <module 'tensorflow.python.framework.ops' from '/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py'>,\n",
      "    '_pywrap_tensorflow': <module 'tensorflow.python.pywrap_tensorflow' from '/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py'>,\n",
      "    '_quantized_avg_pool_outputs': ['output', 'min_output', 'max_output'],\n",
      "    '_quantized_batch_norm_with_global_normalization_outputs': [   'result',\n",
      "                                                                   'result_min',\n",
      "                                                                   'result_max'],\n",
      "    '_quantized_bias_add_outputs': ['output', 'min_out', 'max_out'],\n",
      "    '_quantized_conv2d_outputs': ['output', 'min_output', 'max_output'],\n",
      "    '_quantized_max_pool_outputs': ['output', 'min_output', 'max_output'],\n",
      "    '_quantized_relu6_outputs': [   'activations',\n",
      "                                    'min_activations',\n",
      "                                    'max_activations'],\n",
      "    '_quantized_relu_outputs': [   'activations',\n",
      "                                   'min_activations',\n",
      "                                   'max_activations'],\n",
      "    '_quantized_relu_x_outputs': [   'activations',\n",
      "                                     'min_activations',\n",
      "                                     'max_activations'],\n",
      "    '_six': <module 'six' from '/home/default/miniconda3/lib/python3.6/site-packages/six.py'>,\n",
      "    '_softmax_cross_entropy_with_logits_outputs': ['loss', 'backprop'],\n",
      "    '_sparse_softmax_cross_entropy_with_logits_outputs': ['loss', 'backprop'],\n",
      "    '_tensor_shape': <module 'tensorflow.python.framework.tensor_shape' from '/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py'>,\n",
      "    '_top_k_outputs': ['values', 'indices'],\n",
      "    '_top_kv2_outputs': ['values', 'indices'],\n",
      "    'avg_pool': <function avg_pool at 0x7f4b3a9a8048>,\n",
      "    'avg_pool3d': <function avg_pool3d at 0x7f4b3a9a8158>,\n",
      "    'avg_pool3d_eager_fallback': <function avg_pool3d_eager_fallback at 0x7f4b3a9a81e0>,\n",
      "    'avg_pool3d_grad': <function avg_pool3d_grad at 0x7f4b3a9a8268>,\n",
      "    'avg_pool3d_grad_eager_fallback': <function avg_pool3d_grad_eager_fallback at 0x7f4b3a9a82f0>,\n",
      "    'avg_pool_eager_fallback': <function avg_pool_eager_fallback at 0x7f4b3a9a80d0>,\n",
      "    'avg_pool_grad': <function avg_pool_grad at 0x7f4b3a9a8378>,\n",
      "    'avg_pool_grad_eager_fallback': <function avg_pool_grad_eager_fallback at 0x7f4b3a9a8400>,\n",
      "    'batch_norm_with_global_normalization_grad': <function batch_norm_with_global_normalization_grad at 0x7f4b3a9a86a8>,\n",
      "    'batch_norm_with_global_normalization_grad_eager_fallback': <function batch_norm_with_global_normalization_grad_eager_fallback at 0x7f4b3a9a8a60>,\n",
      "    'bias_add': <function bias_add at 0x7f4b3a9a8ae8>,\n",
      "    'bias_add_eager_fallback': <function bias_add_eager_fallback at 0x7f4b3a9a8b70>,\n",
      "    'bias_add_grad': <function bias_add_grad at 0x7f4b3a9a8bf8>,\n",
      "    'bias_add_grad_eager_fallback': <function bias_add_grad_eager_fallback at 0x7f4b3a9a8c80>,\n",
      "    'bias_add_v1': <function bias_add_v1 at 0x7f4b3a9a8d08>,\n",
      "    'bias_add_v1_eager_fallback': <function bias_add_v1_eager_fallback at 0x7f4b3a9a8d90>,\n",
      "    'conv2d': <function conv2d at 0x7f4b3a9a8e18>,\n",
      "    'conv2d_backprop_filter': <function conv2d_backprop_filter at 0x7f4b3a9a8f28>,\n",
      "    'conv2d_backprop_filter_eager_fallback': <function conv2d_backprop_filter_eager_fallback at 0x7f4b3a9a9048>,\n",
      "    'conv2d_backprop_input': <function conv2d_backprop_input at 0x7f4b3a9a90d0>,\n",
      "    'conv2d_backprop_input_eager_fallback': <function conv2d_backprop_input_eager_fallback at 0x7f4b3a9a9158>,\n",
      "    'conv2d_eager_fallback': <function conv2d_eager_fallback at 0x7f4b3a9a8ea0>,\n",
      "    'conv3d': <function conv3d at 0x7f4b3a9a91e0>,\n",
      "    'conv3d_backprop_filter': <function conv3d_backprop_filter at 0x7f4b3a9a92f0>,\n",
      "    'conv3d_backprop_filter_eager_fallback': <function conv3d_backprop_filter_eager_fallback at 0x7f4b3a9a9378>,\n",
      "    'conv3d_backprop_filter_v2': <function conv3d_backprop_filter_v2 at 0x7f4b3a9a9400>,\n",
      "    'conv3d_backprop_filter_v2_eager_fallback': <function conv3d_backprop_filter_v2_eager_fallback at 0x7f4b3a9a9488>,\n",
      "    'conv3d_backprop_input': <function conv3d_backprop_input at 0x7f4b3a9a9510>,\n",
      "    'conv3d_backprop_input_eager_fallback': <function conv3d_backprop_input_eager_fallback at 0x7f4b3a9a9598>,\n",
      "    'conv3d_backprop_input_v2': <function conv3d_backprop_input_v2 at 0x7f4b3a9a9620>,\n",
      "    'conv3d_backprop_input_v2_eager_fallback': <function conv3d_backprop_input_v2_eager_fallback at 0x7f4b3a9a96a8>,\n",
      "    'conv3d_eager_fallback': <function conv3d_eager_fallback at 0x7f4b3a9a9268>,\n",
      "    'data_format_dim_map': <function data_format_dim_map at 0x7f4b3a9a9730>,\n",
      "    'data_format_dim_map_eager_fallback': <function data_format_dim_map_eager_fallback at 0x7f4b3a9a97b8>,\n",
      "    'data_format_vec_permute': <function data_format_vec_permute at 0x7f4b3a9a9840>,\n",
      "    'data_format_vec_permute_eager_fallback': <function data_format_vec_permute_eager_fallback at 0x7f4b3a9a98c8>,\n",
      "    'deprecated_endpoints': <function deprecated_endpoints at 0x7f4b489ab1e0>,\n",
      "    'depthwise_conv2d_native': <function depthwise_conv2d_native at 0x7f4b3a9a9950>,\n",
      "    'depthwise_conv2d_native_backprop_filter': <function depthwise_conv2d_native_backprop_filter at 0x7f4b3a9a9a60>,\n",
      "    'depthwise_conv2d_native_backprop_filter_eager_fallback': <function depthwise_conv2d_native_backprop_filter_eager_fallback at 0x7f4b3a9a9ae8>,\n",
      "    'depthwise_conv2d_native_backprop_input': <function depthwise_conv2d_native_backprop_input at 0x7f4b3a9a9b70>,\n",
      "    'depthwise_conv2d_native_backprop_input_eager_fallback': <function depthwise_conv2d_native_backprop_input_eager_fallback at 0x7f4b3a9a9bf8>,\n",
      "    'depthwise_conv2d_native_eager_fallback': <function depthwise_conv2d_native_eager_fallback at 0x7f4b3a9a99d8>,\n",
      "    'dilation2d': <function dilation2d at 0x7f4b3a9a9c80>,\n",
      "    'dilation2d_backprop_filter': <function dilation2d_backprop_filter at 0x7f4b3a9a9d90>,\n",
      "    'dilation2d_backprop_filter_eager_fallback': <function dilation2d_backprop_filter_eager_fallback at 0x7f4b3a9a9e18>,\n",
      "    'dilation2d_backprop_input': <function dilation2d_backprop_input at 0x7f4b3a9a9ea0>,\n",
      "    'dilation2d_backprop_input_eager_fallback': <function dilation2d_backprop_input_eager_fallback at 0x7f4b3a9a9f28>,\n",
      "    'dilation2d_eager_fallback': <function dilation2d_eager_fallback at 0x7f4b3a9a9d08>,\n",
      "    'elu': <function elu at 0x7f4b3a9ad048>,\n",
      "    'elu_eager_fallback': <function elu_eager_fallback at 0x7f4b3a9ad0d0>,\n",
      "    'elu_grad': <function elu_grad at 0x7f4b3a9ad158>,\n",
      "    'elu_grad_eager_fallback': <function elu_grad_eager_fallback at 0x7f4b3a9ad1e0>,\n",
      "    'fractional_avg_pool': <function fractional_avg_pool at 0x7f4b3a9ad2f0>,\n",
      "    'fractional_avg_pool_eager_fallback': <function fractional_avg_pool_eager_fallback at 0x7f4b3a9ad6a8>,\n",
      "    'fractional_avg_pool_grad': <function fractional_avg_pool_grad at 0x7f4b3a9ad730>,\n",
      "    'fractional_avg_pool_grad_eager_fallback': <function fractional_avg_pool_grad_eager_fallback at 0x7f4b3a9ad7b8>,\n",
      "    'fractional_max_pool': <function fractional_max_pool at 0x7f4b3a9ad8c8>,\n",
      "    'fractional_max_pool_eager_fallback': <function fractional_max_pool_eager_fallback at 0x7f4b3a9adc80>,\n",
      "    'fractional_max_pool_grad': <function fractional_max_pool_grad at 0x7f4b3a9add08>,\n",
      "    'fractional_max_pool_grad_eager_fallback': <function fractional_max_pool_grad_eager_fallback at 0x7f4b3a9add90>,\n",
      "    'fused_batch_norm_grad': <function fused_batch_norm_grad at 0x7f4b3a9bb400>,\n",
      "    'fused_batch_norm_grad_eager_fallback': <function fused_batch_norm_grad_eager_fallback at 0x7f4b3a9bb7b8>,\n",
      "    'fused_batch_norm_grad_v2': <function fused_batch_norm_grad_v2 at 0x7f4b3a9bb8c8>,\n",
      "    'fused_batch_norm_grad_v2_eager_fallback': <function fused_batch_norm_grad_v2_eager_fallback at 0x7f4b3a9bbc80>,\n",
      "    'fused_batch_norm_v2': <function fused_batch_norm_v2 at 0x7f4b3a9bbe18>,\n",
      "    'fused_batch_norm_v2_eager_fallback': <function fused_batch_norm_v2_eager_fallback at 0x7f4b3a9c8268>,\n",
      "    'fused_pad_conv2d': <function fused_pad_conv2d at 0x7f4b3a9c82f0>,\n",
      "    'fused_pad_conv2d_eager_fallback': <function fused_pad_conv2d_eager_fallback at 0x7f4b3a9c8378>,\n",
      "    'fused_resize_and_pad_conv2d': <function fused_resize_and_pad_conv2d at 0x7f4b3a9c8400>,\n",
      "    'fused_resize_and_pad_conv2d_eager_fallback': <function fused_resize_and_pad_conv2d_eager_fallback at 0x7f4b3a9c8488>,\n",
      "    'in_top_k': <function in_top_k at 0x7f4b3a9c8510>,\n",
      "    'in_top_k_eager_fallback': <function in_top_k_eager_fallback at 0x7f4b3a9c8598>,\n",
      "    'in_top_kv2': <function in_top_kv2 at 0x7f4b3a9c8620>,\n",
      "    'in_top_kv2_eager_fallback': <function in_top_kv2_eager_fallback at 0x7f4b3a9c86a8>,\n",
      "    'l2_loss': <function l2_loss at 0x7f4b3a9c8730>,\n",
      "    'l2_loss_eager_fallback': <function l2_loss_eager_fallback at 0x7f4b3a9c87b8>,\n",
      "    'log_softmax': <function log_softmax at 0x7f4b3a9c8a60>,\n",
      "    'log_softmax_eager_fallback': <function log_softmax_eager_fallback at 0x7f4b3a9c8ae8>,\n",
      "    'lrn': <function lrn at 0x7f4b3a9c8840>,\n",
      "    'lrn_eager_fallback': <function lrn_eager_fallback at 0x7f4b3a9c88c8>,\n",
      "    'lrn_grad': <function lrn_grad at 0x7f4b3a9c8950>,\n",
      "    'lrn_grad_eager_fallback': <function lrn_grad_eager_fallback at 0x7f4b3a9c89d8>,\n",
      "    'max_pool': <function max_pool at 0x7f4b3a9c8b70>,\n",
      "    'max_pool3d': <function max_pool3d at 0x7f4b3a9c8c80>,\n",
      "    'max_pool3d_eager_fallback': <function max_pool3d_eager_fallback at 0x7f4b3a9c8d08>,\n",
      "    'max_pool3d_grad': <function max_pool3d_grad at 0x7f4b3a9c8d90>,\n",
      "    'max_pool3d_grad_eager_fallback': <function max_pool3d_grad_eager_fallback at 0x7f4b3a9c8e18>,\n",
      "    'max_pool3d_grad_grad': <function max_pool3d_grad_grad at 0x7f4b3a9c8ea0>,\n",
      "    'max_pool3d_grad_grad_eager_fallback': <function max_pool3d_grad_grad_eager_fallback at 0x7f4b3a9c8f28>,\n",
      "    'max_pool_eager_fallback': <function max_pool_eager_fallback at 0x7f4b3a9c8bf8>,\n",
      "    'max_pool_grad': <function max_pool_grad at 0x7f4b3a9b0048>,\n",
      "    'max_pool_grad_eager_fallback': <function max_pool_grad_eager_fallback at 0x7f4b3a9b00d0>,\n",
      "    'max_pool_grad_grad': <function max_pool_grad_grad at 0x7f4b3a9b0158>,\n",
      "    'max_pool_grad_grad_eager_fallback': <function max_pool_grad_grad_eager_fallback at 0x7f4b3a9b01e0>,\n",
      "    'max_pool_grad_grad_v2': <function max_pool_grad_grad_v2 at 0x7f4b3a9b0268>,\n",
      "    'max_pool_grad_grad_v2_eager_fallback': <function max_pool_grad_grad_v2_eager_fallback at 0x7f4b3a9b02f0>,\n",
      "    'max_pool_grad_grad_with_argmax': <function max_pool_grad_grad_with_argmax at 0x7f4b3a9b0378>,\n",
      "    'max_pool_grad_grad_with_argmax_eager_fallback': <function max_pool_grad_grad_with_argmax_eager_fallback at 0x7f4b3a9b0400>,\n",
      "    'max_pool_grad_v2': <function max_pool_grad_v2 at 0x7f4b3a9b0488>,\n",
      "    'max_pool_grad_v2_eager_fallback': <function max_pool_grad_v2_eager_fallback at 0x7f4b3a9b0510>,\n",
      "    'max_pool_grad_with_argmax': <function max_pool_grad_with_argmax at 0x7f4b3a9b0598>,\n",
      "    'max_pool_grad_with_argmax_eager_fallback': <function max_pool_grad_with_argmax_eager_fallback at 0x7f4b3a9b0620>,\n",
      "    'max_pool_v2': <function max_pool_v2 at 0x7f4b3a9b06a8>,\n",
      "    'max_pool_v2_eager_fallback': <function max_pool_v2_eager_fallback at 0x7f4b3a9b0730>,\n",
      "    'max_pool_with_argmax': <function max_pool_with_argmax at 0x7f4b3a9b0840>,\n",
      "    'max_pool_with_argmax_eager_fallback': <function max_pool_with_argmax_eager_fallback at 0x7f4b3a9b0bf8>,\n",
      "    'nth_element': <function nth_element at 0x7f4b3a9b0c80>,\n",
      "    'nth_element_eager_fallback': <function nth_element_eager_fallback at 0x7f4b3a9b0d08>,\n",
      "    'quantized_avg_pool': <function quantized_avg_pool at 0x7f4b3a9b0d90>,\n",
      "    'quantized_avg_pool_eager_fallback': <function quantized_avg_pool_eager_fallback at 0x7f4b3a9ae1e0>,\n",
      "    'quantized_batch_norm_with_global_normalization': <function quantized_batch_norm_with_global_normalization at 0x7f4b3a9ae2f0>,\n",
      "    'quantized_batch_norm_with_global_normalization_eager_fallback': <function quantized_batch_norm_with_global_normalization_eager_fallback at 0x7f4b3a9ae6a8>,\n",
      "    'quantized_bias_add': <function quantized_bias_add at 0x7f4b3a9ae730>,\n",
      "    'quantized_bias_add_eager_fallback': <function quantized_bias_add_eager_fallback at 0x7f4b3a9aeae8>,\n",
      "    'quantized_conv2d': <function quantized_conv2d at 0x7f4b3a9aeb70>,\n",
      "    'quantized_conv2d_eager_fallback': <function quantized_conv2d_eager_fallback at 0x7f4b3a9aef28>,\n",
      "    'quantized_max_pool': <function quantized_max_pool at 0x7f4b3a9d1048>,\n",
      "    'quantized_max_pool_eager_fallback': <function quantized_max_pool_eager_fallback at 0x7f4b3a9d1400>,\n",
      "    'quantized_relu': <function quantized_relu at 0x7f4b3a9d1510>,\n",
      "    'quantized_relu6': <function quantized_relu6 at 0x7f4b3a9d19d8>,\n",
      "    'quantized_relu6_eager_fallback': <function quantized_relu6_eager_fallback at 0x7f4b3a9d1d90>,\n",
      "    'quantized_relu_eager_fallback': <function quantized_relu_eager_fallback at 0x7f4b3a9d18c8>,\n",
      "    'quantized_relu_x': <function quantized_relu_x at 0x7f4b3a9d1ea0>,\n",
      "    'quantized_relu_x_eager_fallback': <function quantized_relu_x_eager_fallback at 0x7f4b3a9d32f0>,\n",
      "    'relu': <function relu at 0x7f4b3a9d3378>,\n",
      "    'relu6': <function relu6 at 0x7f4b3a9d3488>,\n",
      "    'relu6_eager_fallback': <function relu6_eager_fallback at 0x7f4b3a9d3510>,\n",
      "    'relu6_grad': <function relu6_grad at 0x7f4b3a9d3598>,\n",
      "    'relu6_grad_eager_fallback': <function relu6_grad_eager_fallback at 0x7f4b3a9d3620>,\n",
      "    'relu_eager_fallback': <function relu_eager_fallback at 0x7f4b3a9d3400>,\n",
      "    'relu_grad': <function relu_grad at 0x7f4b3a9d36a8>,\n",
      "    'relu_grad_eager_fallback': <function relu_grad_eager_fallback at 0x7f4b3a9d3730>,\n",
      "    'selu': <function selu at 0x7f4b3a9d37b8>,\n",
      "    'selu_eager_fallback': <function selu_eager_fallback at 0x7f4b3a9d3840>,\n",
      "    'selu_grad': <function selu_grad at 0x7f4b3a9d38c8>,\n",
      "    'selu_grad_eager_fallback': <function selu_grad_eager_fallback at 0x7f4b3a9d3950>,\n",
      "    'softmax': <function softmax at 0x7f4b3a9d39d8>,\n",
      "    'softmax_cross_entropy_with_logits': <function softmax_cross_entropy_with_logits at 0x7f4b3a9d3ae8>,\n",
      "    'softmax_cross_entropy_with_logits_eager_fallback': <function softmax_cross_entropy_with_logits_eager_fallback at 0x7f4b3a9d3ea0>,\n",
      "    'softmax_eager_fallback': <function softmax_eager_fallback at 0x7f4b3a9d3a60>,\n",
      "    'softplus': <function softplus at 0x7f4b3a9d3f28>,\n",
      "    'softplus_eager_fallback': <function softplus_eager_fallback at 0x7f4b3a9b7048>,\n",
      "    'softplus_grad': <function softplus_grad at 0x7f4b3a9b70d0>,\n",
      "    'softplus_grad_eager_fallback': <function softplus_grad_eager_fallback at 0x7f4b3a9b7158>,\n",
      "    'softsign': <function softsign at 0x7f4b3a9b71e0>,\n",
      "    'softsign_eager_fallback': <function softsign_eager_fallback at 0x7f4b3a9b7268>,\n",
      "    'softsign_grad': <function softsign_grad at 0x7f4b3a9b72f0>,\n",
      "    'softsign_grad_eager_fallback': <function softsign_grad_eager_fallback at 0x7f4b3a9b7378>,\n",
      "    'sparse_softmax_cross_entropy_with_logits': <function sparse_softmax_cross_entropy_with_logits at 0x7f4b3a9b7488>,\n",
      "    'sparse_softmax_cross_entropy_with_logits_eager_fallback': <function sparse_softmax_cross_entropy_with_logits_eager_fallback at 0x7f4b3a9b7840>,\n",
      "    'tf_export': functools.partial(<class 'tensorflow.python.util.tf_export.api_export'>, api_name='tensorflow'),\n",
      "    'top_k': <function top_k at 0x7f4b3a9b78c8>,\n",
      "    'top_k_eager_fallback': <function top_k_eager_fallback at 0x7f4b3a9b7c80>,\n",
      "    'top_kv2': <function top_kv2 at 0x7f4b3a9b7d08>,\n",
      "    'top_kv2_eager_fallback': <function top_kv2_eager_fallback at 0x7f4b3a9d2158>}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(gen_nn_ops.softmax.__globals__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.gen_nn_ops.softmax(logits, name=None)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_nn_ops.softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcefile(gen_nn_ops.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Python wrappers around TensorFlow ops.\n",
      "\n",
      "This file is MACHINE GENERATED! Do not edit.\n",
      "Original C++ source file: nn_ops.cc\n",
      "\"\"\"\n",
      "\n",
      "import collections as _collections\n",
      "import six as _six\n",
      "\n",
      "from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "from tensorflow.python.eager import context as _context\n",
      "from tensorflow.python.eager import core as _core\n",
      "from tensorflow.python.eager import execute as _execute\n",
      "from tensorflow.python.framework import dtypes as _dtypes\n",
      "from tensorflow.python.framework import errors as _errors\n",
      "from tensorflow.python.framework import tensor_shape as _tensor_shape\n",
      "\n",
      "from tensorflow.core.framework import op_def_pb2 as _op_def_pb2\n",
      "# Needed to trigger the call to _set_call_cpp_shape_fn.\n",
      "from tensorflow.python.framework import common_shapes as _common_shapes\n",
      "from tensorflow.python.framework import op_def_registry as _op_def_registry\n",
      "from tensorflow.python.framework import ops as _ops\n",
      "from tensorflow.python.framework import op_def_library as _op_def_library\n",
      "from tensorflow.python.util.deprecation import deprecated_endpoints\n",
      "from tensorflow.python.util.tf_export import tf_export\n",
      "\n",
      "\n",
      "def avg_pool(value, ksize, strides, padding, data_format=\"NHWC\", name=None):\n",
      "  r\"\"\"Performs average pooling on the input.\n",
      "\n",
      "  Each entry in `output` is the mean of the corresponding size `ksize`\n",
      "  window in `value`.\n",
      "\n",
      "  Args:\n",
      "    value: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      4-D with shape `[batch, height, width, channels]`.\n",
      "    ksize: A list of `ints` that has length `>= 4`.\n",
      "      The size of the sliding window for each dimension of `value`.\n",
      "    strides: A list of `ints` that has length `>= 4`.\n",
      "      The stride of the sliding window for each dimension of `value`.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the data is stored in the order of:\n",
      "          [batch, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, in_channels, in_height, in_width].\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `value`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(ksize, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'ksize' argument to \"\n",
      "          \"'avg_pool' Op, not %r.\" % ksize)\n",
      "    ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'avg_pool' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"AvgPool\", value=value, ksize=ksize, strides=strides, padding=padding,\n",
      "        data_format=data_format, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"ksize\", _op.get_attr(\"ksize\"), \"strides\",\n",
      "              _op.get_attr(\"strides\"), \"padding\", _op.get_attr(\"padding\"),\n",
      "              \"data_format\", _op.get_attr(\"data_format\"), \"T\",\n",
      "              _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"AvgPool\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"AvgPool\",\n",
      "        name, _ctx._post_execution_callbacks, value, \"ksize\", ksize,\n",
      "        \"strides\", strides, \"padding\", padding, \"data_format\", data_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return avg_pool_eager_fallback(\n",
      "          value, ksize=ksize, strides=strides, padding=padding,\n",
      "          data_format=data_format, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def avg_pool_eager_fallback(value, ksize, strides, padding, data_format=\"NHWC\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function avg_pool\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(ksize, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'ksize' argument to \"\n",
      "        \"'avg_pool' Op, not %r.\" % ksize)\n",
      "  ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'avg_pool' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  _attr_T, (value,) = _execute.args_to_matching_eager([value], _ctx)\n",
      "  _inputs_flat = [value]\n",
      "  _attrs = (\"ksize\", ksize, \"strides\", strides, \"padding\", padding,\n",
      "  \"data_format\", data_format, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"AvgPool\", 1, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"AvgPool\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.avg_pool3d')\n",
      "def avg_pool3d(input, ksize, strides, padding, data_format=\"NDHWC\", name=None):\n",
      "  r\"\"\"Performs 3D average pooling on the input.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      Shape `[batch, depth, rows, cols, channels]` tensor to pool over.\n",
      "    ksize: A list of `ints` that has length `>= 5`.\n",
      "      1-D tensor of length 5. The size of the window for each dimension of\n",
      "      the input tensor. Must have `ksize[0] = ksize[4] = 1`.\n",
      "    strides: A list of `ints` that has length `>= 5`.\n",
      "      1-D tensor of length 5. The stride of the sliding window for each\n",
      "      dimension of `input`. Must have `strides[0] = strides[4] = 1`.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NDHWC\", \"NCDHW\"`. Defaults to `\"NDHWC\"`.\n",
      "      The data format of the input and output data. With the\n",
      "      default format \"NDHWC\", the data is stored in the order of:\n",
      "          [batch, in_depth, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCDHW\", the data storage order is:\n",
      "          [batch, in_channels, in_depth, in_height, in_width].\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(ksize, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'ksize' argument to \"\n",
      "          \"'avg_pool3d' Op, not %r.\" % ksize)\n",
      "    ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'avg_pool3d' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NDHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"AvgPool3D\", input=input, ksize=ksize, strides=strides,\n",
      "        padding=padding, data_format=data_format, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"ksize\", _op.get_attr(\"ksize\"), \"strides\",\n",
      "              _op.get_attr(\"strides\"), \"padding\", _op.get_attr(\"padding\"),\n",
      "              \"data_format\", _op.get_attr(\"data_format\"), \"T\",\n",
      "              _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"AvgPool3D\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"AvgPool3D\",\n",
      "        name, _ctx._post_execution_callbacks, input, \"ksize\", ksize,\n",
      "        \"strides\", strides, \"padding\", padding, \"data_format\", data_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return avg_pool3d_eager_fallback(\n",
      "          input, ksize=ksize, strides=strides, padding=padding,\n",
      "          data_format=data_format, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def avg_pool3d_eager_fallback(input, ksize, strides, padding, data_format=\"NDHWC\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function avg_pool3d\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(ksize, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'ksize' argument to \"\n",
      "        \"'avg_pool3d' Op, not %r.\" % ksize)\n",
      "  ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'avg_pool3d' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NDHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  _attr_T, (input,) = _execute.args_to_matching_eager([input], _ctx)\n",
      "  _inputs_flat = [input]\n",
      "  _attrs = (\"ksize\", ksize, \"strides\", strides, \"padding\", padding,\n",
      "  \"data_format\", data_format, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"AvgPool3D\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"AvgPool3D\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def avg_pool3d_grad(orig_input_shape, grad, ksize, strides, padding, data_format=\"NDHWC\", name=None):\n",
      "  r\"\"\"Computes gradients of average pooling function.\n",
      "\n",
      "  Args:\n",
      "    orig_input_shape: A `Tensor` of type `int32`.\n",
      "      The original input dimensions.\n",
      "    grad: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      Output backprop of shape `[batch, depth, rows, cols, channels]`.\n",
      "    ksize: A list of `ints` that has length `>= 5`.\n",
      "      1-D tensor of length 5. The size of the window for each dimension of\n",
      "      the input tensor. Must have `ksize[0] = ksize[4] = 1`.\n",
      "    strides: A list of `ints` that has length `>= 5`.\n",
      "      1-D tensor of length 5. The stride of the sliding window for each\n",
      "      dimension of `input`. Must have `strides[0] = strides[4] = 1`.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NDHWC\", \"NCDHW\"`. Defaults to `\"NDHWC\"`.\n",
      "      The data format of the input and output data. With the\n",
      "      default format \"NDHWC\", the data is stored in the order of:\n",
      "          [batch, in_depth, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCDHW\", the data storage order is:\n",
      "          [batch, in_channels, in_depth, in_height, in_width].\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `grad`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(ksize, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'ksize' argument to \"\n",
      "          \"'avg_pool3d_grad' Op, not %r.\" % ksize)\n",
      "    ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'avg_pool3d_grad' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NDHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"AvgPool3DGrad\", orig_input_shape=orig_input_shape, grad=grad,\n",
      "        ksize=ksize, strides=strides, padding=padding,\n",
      "        data_format=data_format, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"ksize\", _op.get_attr(\"ksize\"), \"strides\",\n",
      "              _op.get_attr(\"strides\"), \"padding\", _op.get_attr(\"padding\"),\n",
      "              \"data_format\", _op.get_attr(\"data_format\"), \"T\",\n",
      "              _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"AvgPool3DGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"AvgPool3DGrad\", name, _ctx._post_execution_callbacks,\n",
      "        orig_input_shape, grad, \"ksize\", ksize, \"strides\", strides, \"padding\",\n",
      "        padding, \"data_format\", data_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return avg_pool3d_grad_eager_fallback(\n",
      "          orig_input_shape, grad, ksize=ksize, strides=strides,\n",
      "          padding=padding, data_format=data_format, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def avg_pool3d_grad_eager_fallback(orig_input_shape, grad, ksize, strides, padding, data_format=\"NDHWC\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function avg_pool3d_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(ksize, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'ksize' argument to \"\n",
      "        \"'avg_pool3d_grad' Op, not %r.\" % ksize)\n",
      "  ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'avg_pool3d_grad' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NDHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  _attr_T, (grad,) = _execute.args_to_matching_eager([grad], _ctx)\n",
      "  orig_input_shape = _ops.convert_to_tensor(orig_input_shape, _dtypes.int32)\n",
      "  _inputs_flat = [orig_input_shape, grad]\n",
      "  _attrs = (\"ksize\", ksize, \"strides\", strides, \"padding\", padding,\n",
      "  \"data_format\", data_format, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"AvgPool3DGrad\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"AvgPool3DGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def avg_pool_grad(orig_input_shape, grad, ksize, strides, padding, data_format=\"NHWC\", name=None):\n",
      "  r\"\"\"Computes gradients of the average pooling function.\n",
      "\n",
      "  Args:\n",
      "    orig_input_shape: A `Tensor` of type `int32`.\n",
      "      1-D.  Shape of the original input to `avg_pool`.\n",
      "    grad: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      4-D with shape `[batch, height, width, channels]`.  Gradients w.r.t.\n",
      "      the output of `avg_pool`.\n",
      "    ksize: A list of `ints` that has length `>= 4`.\n",
      "      The size of the sliding window for each dimension of the input.\n",
      "    strides: A list of `ints` that has length `>= 4`.\n",
      "      The stride of the sliding window for each dimension of the input.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the data is stored in the order of:\n",
      "          [batch, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, in_channels, in_height, in_width].\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `grad`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(ksize, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'ksize' argument to \"\n",
      "          \"'avg_pool_grad' Op, not %r.\" % ksize)\n",
      "    ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'avg_pool_grad' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"AvgPoolGrad\", orig_input_shape=orig_input_shape, grad=grad,\n",
      "        ksize=ksize, strides=strides, padding=padding,\n",
      "        data_format=data_format, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"ksize\", _op.get_attr(\"ksize\"), \"strides\",\n",
      "              _op.get_attr(\"strides\"), \"padding\", _op.get_attr(\"padding\"),\n",
      "              \"data_format\", _op.get_attr(\"data_format\"), \"T\",\n",
      "              _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"AvgPoolGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"AvgPoolGrad\",\n",
      "        name, _ctx._post_execution_callbacks, orig_input_shape, grad, \"ksize\",\n",
      "        ksize, \"strides\", strides, \"padding\", padding, \"data_format\",\n",
      "        data_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return avg_pool_grad_eager_fallback(\n",
      "          orig_input_shape, grad, ksize=ksize, strides=strides,\n",
      "          padding=padding, data_format=data_format, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def avg_pool_grad_eager_fallback(orig_input_shape, grad, ksize, strides, padding, data_format=\"NHWC\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function avg_pool_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(ksize, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'ksize' argument to \"\n",
      "        \"'avg_pool_grad' Op, not %r.\" % ksize)\n",
      "  ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'avg_pool_grad' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  _attr_T, (grad,) = _execute.args_to_matching_eager([grad], _ctx)\n",
      "  orig_input_shape = _ops.convert_to_tensor(orig_input_shape, _dtypes.int32)\n",
      "  _inputs_flat = [orig_input_shape, grad]\n",
      "  _attrs = (\"ksize\", ksize, \"strides\", strides, \"padding\", padding,\n",
      "  \"data_format\", data_format, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"AvgPoolGrad\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"AvgPoolGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def _batch_norm_with_global_normalization(t, m, v, beta, gamma, variance_epsilon, scale_after_normalization, name=None):\n",
      "  r\"\"\"Batch normalization.\n",
      "\n",
      "  This op is deprecated. Prefer `tf.nn.batch_normalization`.\n",
      "\n",
      "  Args:\n",
      "    t: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n",
      "      A 4D input Tensor.\n",
      "    m: A `Tensor`. Must have the same type as `t`.\n",
      "      A 1D mean Tensor with size matching the last dimension of t.\n",
      "      This is the first output from tf.nn.moments,\n",
      "      or a saved moving average thereof.\n",
      "    v: A `Tensor`. Must have the same type as `t`.\n",
      "      A 1D variance Tensor with size matching the last dimension of t.\n",
      "      This is the second output from tf.nn.moments,\n",
      "      or a saved moving average thereof.\n",
      "    beta: A `Tensor`. Must have the same type as `t`.\n",
      "      A 1D beta Tensor with size matching the last dimension of t.\n",
      "      An offset to be added to the normalized tensor.\n",
      "    gamma: A `Tensor`. Must have the same type as `t`.\n",
      "      A 1D gamma Tensor with size matching the last dimension of t.\n",
      "      If \"scale_after_normalization\" is true, this tensor will be multiplied\n",
      "      with the normalized tensor.\n",
      "    variance_epsilon: A `float`. A small float number to avoid dividing by 0.\n",
      "    scale_after_normalization: A `bool`.\n",
      "      A bool indicating whether the resulted tensor\n",
      "      needs to be multiplied with gamma.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `t`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    variance_epsilon = _execute.make_float(variance_epsilon, \"variance_epsilon\")\n",
      "    scale_after_normalization = _execute.make_bool(scale_after_normalization, \"scale_after_normalization\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"BatchNormWithGlobalNormalization\", t=t, m=m, v=v, beta=beta,\n",
      "        gamma=gamma, variance_epsilon=variance_epsilon,\n",
      "        scale_after_normalization=scale_after_normalization, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"variance_epsilon\",\n",
      "              _op.get_attr(\"variance_epsilon\"), \"scale_after_normalization\",\n",
      "              _op.get_attr(\"scale_after_normalization\"))\n",
      "    _execute.record_gradient(\n",
      "      \"BatchNormWithGlobalNormalization\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"BatchNormWithGlobalNormalization\", name,\n",
      "        _ctx._post_execution_callbacks, t, m, v, beta, gamma,\n",
      "        \"variance_epsilon\", variance_epsilon, \"scale_after_normalization\",\n",
      "        scale_after_normalization)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return _batch_norm_with_global_normalization_eager_fallback(\n",
      "          t, m, v, beta, gamma, variance_epsilon=variance_epsilon,\n",
      "          scale_after_normalization=scale_after_normalization, name=name,\n",
      "          ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def _batch_norm_with_global_normalization_eager_fallback(t, m, v, beta, gamma, variance_epsilon, scale_after_normalization, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function _batch_norm_with_global_normalization\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  variance_epsilon = _execute.make_float(variance_epsilon, \"variance_epsilon\")\n",
      "  scale_after_normalization = _execute.make_bool(scale_after_normalization, \"scale_after_normalization\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([t, m, v, beta, gamma], _ctx)\n",
      "  (t, m, v, beta, gamma) = _inputs_T\n",
      "  _inputs_flat = [t, m, v, beta, gamma]\n",
      "  _attrs = (\"T\", _attr_T, \"variance_epsilon\", variance_epsilon,\n",
      "  \"scale_after_normalization\", scale_after_normalization)\n",
      "  _result = _execute.execute(b\"BatchNormWithGlobalNormalization\", 1,\n",
      "                             inputs=_inputs_flat, attrs=_attrs, ctx=_ctx,\n",
      "                             name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"BatchNormWithGlobalNormalization\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "_batch_norm_with_global_normalization_grad_outputs = [\"dx\", \"dm\", \"dv\", \"db\",\n",
      "                                                     \"dg\"]\n",
      "_BatchNormWithGlobalNormalizationGradOutput = _collections.namedtuple(\n",
      "    \"BatchNormWithGlobalNormalizationGrad\",\n",
      "    _batch_norm_with_global_normalization_grad_outputs)\n",
      "\n",
      "\n",
      "def batch_norm_with_global_normalization_grad(t, m, v, gamma, backprop, variance_epsilon, scale_after_normalization, name=None):\n",
      "  r\"\"\"Gradients for batch normalization.\n",
      "\n",
      "  This op is deprecated. See `tf.nn.batch_normalization`.\n",
      "\n",
      "  Args:\n",
      "    t: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n",
      "      A 4D input Tensor.\n",
      "    m: A `Tensor`. Must have the same type as `t`.\n",
      "      A 1D mean Tensor with size matching the last dimension of t.\n",
      "      This is the first output from tf.nn.moments,\n",
      "      or a saved moving average thereof.\n",
      "    v: A `Tensor`. Must have the same type as `t`.\n",
      "      A 1D variance Tensor with size matching the last dimension of t.\n",
      "      This is the second output from tf.nn.moments,\n",
      "      or a saved moving average thereof.\n",
      "    gamma: A `Tensor`. Must have the same type as `t`.\n",
      "      A 1D gamma Tensor with size matching the last dimension of t.\n",
      "      If \"scale_after_normalization\" is true, this Tensor will be multiplied\n",
      "      with the normalized Tensor.\n",
      "    backprop: A `Tensor`. Must have the same type as `t`. 4D backprop Tensor.\n",
      "    variance_epsilon: A `float`. A small float number to avoid dividing by 0.\n",
      "    scale_after_normalization: A `bool`.\n",
      "      A bool indicating whether the resulted tensor\n",
      "      needs to be multiplied with gamma.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (dx, dm, dv, db, dg).\n",
      "\n",
      "    dx: A `Tensor`. Has the same type as `t`.\n",
      "    dm: A `Tensor`. Has the same type as `t`.\n",
      "    dv: A `Tensor`. Has the same type as `t`.\n",
      "    db: A `Tensor`. Has the same type as `t`.\n",
      "    dg: A `Tensor`. Has the same type as `t`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    variance_epsilon = _execute.make_float(variance_epsilon, \"variance_epsilon\")\n",
      "    scale_after_normalization = _execute.make_bool(scale_after_normalization, \"scale_after_normalization\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"BatchNormWithGlobalNormalizationGrad\", t=t, m=m, v=v, gamma=gamma,\n",
      "        backprop=backprop, variance_epsilon=variance_epsilon,\n",
      "        scale_after_normalization=scale_after_normalization, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"variance_epsilon\",\n",
      "              _op.get_attr(\"variance_epsilon\"), \"scale_after_normalization\",\n",
      "              _op.get_attr(\"scale_after_normalization\"))\n",
      "    _execute.record_gradient(\n",
      "      \"BatchNormWithGlobalNormalizationGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _BatchNormWithGlobalNormalizationGradOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"BatchNormWithGlobalNormalizationGrad\", name,\n",
      "        _ctx._post_execution_callbacks, t, m, v, gamma, backprop,\n",
      "        \"variance_epsilon\", variance_epsilon, \"scale_after_normalization\",\n",
      "        scale_after_normalization)\n",
      "      _result = _BatchNormWithGlobalNormalizationGradOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return batch_norm_with_global_normalization_grad_eager_fallback(\n",
      "          t, m, v, gamma, backprop, variance_epsilon=variance_epsilon,\n",
      "          scale_after_normalization=scale_after_normalization, name=name,\n",
      "          ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def batch_norm_with_global_normalization_grad_eager_fallback(t, m, v, gamma, backprop, variance_epsilon, scale_after_normalization, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function batch_norm_with_global_normalization_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  variance_epsilon = _execute.make_float(variance_epsilon, \"variance_epsilon\")\n",
      "  scale_after_normalization = _execute.make_bool(scale_after_normalization, \"scale_after_normalization\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([t, m, v, gamma, backprop], _ctx)\n",
      "  (t, m, v, gamma, backprop) = _inputs_T\n",
      "  _inputs_flat = [t, m, v, gamma, backprop]\n",
      "  _attrs = (\"T\", _attr_T, \"variance_epsilon\", variance_epsilon,\n",
      "  \"scale_after_normalization\", scale_after_normalization)\n",
      "  _result = _execute.execute(b\"BatchNormWithGlobalNormalizationGrad\", 5,\n",
      "                             inputs=_inputs_flat, attrs=_attrs, ctx=_ctx,\n",
      "                             name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"BatchNormWithGlobalNormalizationGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _BatchNormWithGlobalNormalizationGradOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "def bias_add(value, bias, data_format=\"NHWC\", name=None):\n",
      "  r\"\"\"Adds `bias` to `value`.\n",
      "\n",
      "  This is a special case of `tf.add` where `bias` is restricted to be 1-D.\n",
      "  Broadcasting is supported, so `value` may have any number of dimensions.\n",
      "\n",
      "  Args:\n",
      "    value: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n",
      "      Any number of dimensions.\n",
      "    bias: A `Tensor`. Must have the same type as `value`.\n",
      "      1-D with size the last dimension of `value`.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the bias tensor will be added to the last dimension\n",
      "      of the value tensor.\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, in_channels, in_height, in_width].\n",
      "      The tensor will be added to \"in_channels\", the third-to-the-last\n",
      "          dimension.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `value`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"BiasAdd\", value=value, bias=bias, data_format=data_format, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"))\n",
      "    _execute.record_gradient(\n",
      "      \"BiasAdd\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"BiasAdd\",\n",
      "        name, _ctx._post_execution_callbacks, value, bias, \"data_format\",\n",
      "        data_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return bias_add_eager_fallback(\n",
      "          value, bias, data_format=data_format, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def bias_add_eager_fallback(value, bias, data_format=\"NHWC\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function bias_add\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([value, bias], _ctx)\n",
      "  (value, bias) = _inputs_T\n",
      "  _inputs_flat = [value, bias]\n",
      "  _attrs = (\"T\", _attr_T, \"data_format\", data_format)\n",
      "  _result = _execute.execute(b\"BiasAdd\", 1, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"BiasAdd\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def bias_add_grad(out_backprop, data_format=\"NHWC\", name=None):\n",
      "  r\"\"\"The backward operation for \"BiasAdd\" on the \"bias\" tensor.\n",
      "\n",
      "  It accumulates all the values from out_backprop into the feature dimension.\n",
      "  For NHWC data format, the feature dimension is the last. For NCHW data format,\n",
      "  the feature dimension is the third-to-last.\n",
      "\n",
      "  Args:\n",
      "    out_backprop: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n",
      "      Any number of dimensions.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the bias tensor will be added to the last dimension\n",
      "      of the value tensor.\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, in_channels, in_height, in_width].\n",
      "      The tensor will be added to \"in_channels\", the third-to-the-last\n",
      "          dimension.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `out_backprop`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"BiasAddGrad\", out_backprop=out_backprop, data_format=data_format,\n",
      "        name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"))\n",
      "    _execute.record_gradient(\n",
      "      \"BiasAddGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"BiasAddGrad\",\n",
      "        name, _ctx._post_execution_callbacks, out_backprop, \"data_format\",\n",
      "        data_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return bias_add_grad_eager_fallback(\n",
      "          out_backprop, data_format=data_format, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def bias_add_grad_eager_fallback(out_backprop, data_format=\"NHWC\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function bias_add_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  _attr_T, (out_backprop,) = _execute.args_to_matching_eager([out_backprop], _ctx)\n",
      "  _inputs_flat = [out_backprop]\n",
      "  _attrs = (\"T\", _attr_T, \"data_format\", data_format)\n",
      "  _result = _execute.execute(b\"BiasAddGrad\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"BiasAddGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def bias_add_v1(value, bias, name=None):\n",
      "  r\"\"\"Adds `bias` to `value`.\n",
      "\n",
      "  This is a deprecated version of BiasAdd and will be soon removed.\n",
      "\n",
      "  This is a special case of `tf.add` where `bias` is restricted to be 1-D.\n",
      "  Broadcasting is supported, so `value` may have any number of dimensions.\n",
      "\n",
      "  Args:\n",
      "    value: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`.\n",
      "      Any number of dimensions.\n",
      "    bias: A `Tensor`. Must have the same type as `value`.\n",
      "      1-D with size the last dimension of `value`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `value`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"BiasAddV1\", value=value, bias=bias, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"BiasAddV1\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"BiasAddV1\",\n",
      "        name, _ctx._post_execution_callbacks, value, bias)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return bias_add_v1_eager_fallback(\n",
      "          value, bias, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def bias_add_v1_eager_fallback(value, bias, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function bias_add_v1\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([value, bias], _ctx)\n",
      "  (value, bias) = _inputs_T\n",
      "  _inputs_flat = [value, bias]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"BiasAddV1\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"BiasAddV1\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.conv2d')\n",
      "def conv2d(input, filter, strides, padding, use_cudnn_on_gpu=True, data_format=\"NHWC\", dilations=[1, 1, 1, 1], name=None):\n",
      "  r\"\"\"Computes a 2-D convolution given 4-D `input` and `filter` tensors.\n",
      "\n",
      "  Given an input tensor of shape `[batch, in_height, in_width, in_channels]`\n",
      "  and a filter / kernel tensor of shape\n",
      "  `[filter_height, filter_width, in_channels, out_channels]`, this op\n",
      "  performs the following:\n",
      "\n",
      "  1. Flattens the filter to a 2-D matrix with shape\n",
      "     `[filter_height * filter_width * in_channels, output_channels]`.\n",
      "  2. Extracts image patches from the input tensor to form a *virtual*\n",
      "     tensor of shape `[batch, out_height, out_width,\n",
      "     filter_height * filter_width * in_channels]`.\n",
      "  3. For each patch, right-multiplies the filter matrix and the image patch\n",
      "     vector.\n",
      "\n",
      "  In detail, with the default NHWC format,\n",
      "\n",
      "      output[b, i, j, k] =\n",
      "          sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] *\n",
      "                          filter[di, dj, q, k]\n",
      "\n",
      "  Must have `strides[0] = strides[3] = 1`.  For the most common case of the same\n",
      "  horizontal and vertices strides, `strides = [1, stride, stride, 1]`.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      A 4-D tensor. The dimension order is interpreted according to the value\n",
      "      of `data_format`, see below for details.\n",
      "    filter: A `Tensor`. Must have the same type as `input`.\n",
      "      A 4-D tensor of shape\n",
      "      `[filter_height, filter_width, in_channels, out_channels]`\n",
      "    strides: A list of `ints`.\n",
      "      1-D tensor of length 4.  The stride of the sliding window for each\n",
      "      dimension of `input`. The dimension order is determined by the value of\n",
      "      `data_format`, see below for details.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    use_cudnn_on_gpu: An optional `bool`. Defaults to `True`.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the data is stored in the order of:\n",
      "          [batch, height, width, channels].\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, channels, height, width].\n",
      "    dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.\n",
      "      1-D tensor of length 4.  The dilation factor for each dimension of\n",
      "      `input`. If set to k > 1, there will be k-1 skipped cells between each\n",
      "      filter element on that dimension. The dimension order is determined by the\n",
      "      value of `data_format`, see above for details. Dilations in the batch and\n",
      "      depth dimensions must be 1.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'conv2d' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if use_cudnn_on_gpu is None:\n",
      "      use_cudnn_on_gpu = True\n",
      "    use_cudnn_on_gpu = _execute.make_bool(use_cudnn_on_gpu, \"use_cudnn_on_gpu\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    if dilations is None:\n",
      "      dilations = [1, 1, 1, 1]\n",
      "    if not isinstance(dilations, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'dilations' argument to \"\n",
      "          \"'conv2d' Op, not %r.\" % dilations)\n",
      "    dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Conv2D\", input=input, filter=filter, strides=strides,\n",
      "        padding=padding, use_cudnn_on_gpu=use_cudnn_on_gpu,\n",
      "        data_format=data_format, dilations=dilations, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"strides\", _op.get_attr(\"strides\"),\n",
      "              \"use_cudnn_on_gpu\", _op.get_attr(\"use_cudnn_on_gpu\"), \"padding\",\n",
      "              _op.get_attr(\"padding\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"), \"dilations\",\n",
      "              _op.get_attr(\"dilations\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Conv2D\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"Conv2D\", name,\n",
      "        _ctx._post_execution_callbacks, input, filter, \"strides\", strides,\n",
      "        \"use_cudnn_on_gpu\", use_cudnn_on_gpu, \"padding\", padding,\n",
      "        \"data_format\", data_format, \"dilations\", dilations)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return conv2d_eager_fallback(\n",
      "          input, filter, strides=strides, use_cudnn_on_gpu=use_cudnn_on_gpu,\n",
      "          padding=padding, data_format=data_format, dilations=dilations,\n",
      "          name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def conv2d_eager_fallback(input, filter, strides, padding, use_cudnn_on_gpu=True, data_format=\"NHWC\", dilations=[1, 1, 1, 1], name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function conv2d\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'conv2d' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if use_cudnn_on_gpu is None:\n",
      "    use_cudnn_on_gpu = True\n",
      "  use_cudnn_on_gpu = _execute.make_bool(use_cudnn_on_gpu, \"use_cudnn_on_gpu\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  if dilations is None:\n",
      "    dilations = [1, 1, 1, 1]\n",
      "  if not isinstance(dilations, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'dilations' argument to \"\n",
      "        \"'conv2d' Op, not %r.\" % dilations)\n",
      "  dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter], _ctx)\n",
      "  (input, filter) = _inputs_T\n",
      "  _inputs_flat = [input, filter]\n",
      "  _attrs = (\"T\", _attr_T, \"strides\", strides, \"use_cudnn_on_gpu\",\n",
      "  use_cudnn_on_gpu, \"padding\", padding, \"data_format\", data_format,\n",
      "  \"dilations\", dilations)\n",
      "  _result = _execute.execute(b\"Conv2D\", 1, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Conv2D\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.conv2d_backprop_filter')\n",
      "def conv2d_backprop_filter(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu=True, data_format=\"NHWC\", dilations=[1, 1, 1, 1], name=None):\n",
      "  r\"\"\"Computes the gradients of convolution with respect to the filter.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      4-D with shape `[batch, in_height, in_width, in_channels]`.\n",
      "    filter_sizes: A `Tensor` of type `int32`.\n",
      "      An integer vector representing the tensor shape of `filter`,\n",
      "      where `filter` is a 4-D\n",
      "      `[filter_height, filter_width, in_channels, out_channels]` tensor.\n",
      "    out_backprop: A `Tensor`. Must have the same type as `input`.\n",
      "      4-D with shape `[batch, out_height, out_width, out_channels]`.\n",
      "      Gradients w.r.t. the output of the convolution.\n",
      "    strides: A list of `ints`.\n",
      "      The stride of the sliding window for each dimension of the input\n",
      "      of the convolution. Must be in the same order as the dimension specified with\n",
      "      format.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    use_cudnn_on_gpu: An optional `bool`. Defaults to `True`.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the data is stored in the order of:\n",
      "          [batch, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, in_channels, in_height, in_width].\n",
      "    dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.\n",
      "      1-D tensor of length 4.  The dilation factor for each dimension of\n",
      "      `input`. If set to k > 1, there will be k-1 skipped cells between each filter\n",
      "      element on that dimension. The dimension order is determined by the value of\n",
      "      `data_format`, see above for details. Dilations in the batch and depth\n",
      "      dimensions must be 1.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'conv2d_backprop_filter' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if use_cudnn_on_gpu is None:\n",
      "      use_cudnn_on_gpu = True\n",
      "    use_cudnn_on_gpu = _execute.make_bool(use_cudnn_on_gpu, \"use_cudnn_on_gpu\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    if dilations is None:\n",
      "      dilations = [1, 1, 1, 1]\n",
      "    if not isinstance(dilations, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'dilations' argument to \"\n",
      "          \"'conv2d_backprop_filter' Op, not %r.\" % dilations)\n",
      "    dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Conv2DBackpropFilter\", input=input, filter_sizes=filter_sizes,\n",
      "        out_backprop=out_backprop, strides=strides, padding=padding,\n",
      "        use_cudnn_on_gpu=use_cudnn_on_gpu, data_format=data_format,\n",
      "        dilations=dilations, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"strides\", _op.get_attr(\"strides\"),\n",
      "              \"use_cudnn_on_gpu\", _op.get_attr(\"use_cudnn_on_gpu\"), \"padding\",\n",
      "              _op.get_attr(\"padding\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"), \"dilations\",\n",
      "              _op.get_attr(\"dilations\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Conv2DBackpropFilter\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"Conv2DBackpropFilter\", name, _ctx._post_execution_callbacks, input,\n",
      "        filter_sizes, out_backprop, \"strides\", strides, \"use_cudnn_on_gpu\",\n",
      "        use_cudnn_on_gpu, \"padding\", padding, \"data_format\", data_format,\n",
      "        \"dilations\", dilations)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return conv2d_backprop_filter_eager_fallback(\n",
      "          input, filter_sizes, out_backprop, strides=strides,\n",
      "          use_cudnn_on_gpu=use_cudnn_on_gpu, padding=padding,\n",
      "          data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def conv2d_backprop_filter_eager_fallback(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu=True, data_format=\"NHWC\", dilations=[1, 1, 1, 1], name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function conv2d_backprop_filter\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'conv2d_backprop_filter' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if use_cudnn_on_gpu is None:\n",
      "    use_cudnn_on_gpu = True\n",
      "  use_cudnn_on_gpu = _execute.make_bool(use_cudnn_on_gpu, \"use_cudnn_on_gpu\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  if dilations is None:\n",
      "    dilations = [1, 1, 1, 1]\n",
      "  if not isinstance(dilations, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'dilations' argument to \"\n",
      "        \"'conv2d_backprop_filter' Op, not %r.\" % dilations)\n",
      "  dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, out_backprop], _ctx)\n",
      "  (input, out_backprop) = _inputs_T\n",
      "  filter_sizes = _ops.convert_to_tensor(filter_sizes, _dtypes.int32)\n",
      "  _inputs_flat = [input, filter_sizes, out_backprop]\n",
      "  _attrs = (\"T\", _attr_T, \"strides\", strides, \"use_cudnn_on_gpu\",\n",
      "  use_cudnn_on_gpu, \"padding\", padding, \"data_format\", data_format,\n",
      "  \"dilations\", dilations)\n",
      "  _result = _execute.execute(b\"Conv2DBackpropFilter\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Conv2DBackpropFilter\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.conv2d_backprop_input')\n",
      "def conv2d_backprop_input(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu=True, data_format=\"NHWC\", dilations=[1, 1, 1, 1], name=None):\n",
      "  r\"\"\"Computes the gradients of convolution with respect to the input.\n",
      "\n",
      "  Args:\n",
      "    input_sizes: A `Tensor` of type `int32`.\n",
      "      An integer vector representing the shape of `input`,\n",
      "      where `input` is a 4-D `[batch, height, width, channels]` tensor.\n",
      "    filter: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      4-D with shape\n",
      "      `[filter_height, filter_width, in_channels, out_channels]`.\n",
      "    out_backprop: A `Tensor`. Must have the same type as `filter`.\n",
      "      4-D with shape `[batch, out_height, out_width, out_channels]`.\n",
      "      Gradients w.r.t. the output of the convolution.\n",
      "    strides: A list of `ints`.\n",
      "      The stride of the sliding window for each dimension of the input\n",
      "      of the convolution. Must be in the same order as the dimension specified with\n",
      "      format.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    use_cudnn_on_gpu: An optional `bool`. Defaults to `True`.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the data is stored in the order of:\n",
      "          [batch, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, in_channels, in_height, in_width].\n",
      "    dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.\n",
      "      1-D tensor of length 4.  The dilation factor for each dimension of\n",
      "      `input`. If set to k > 1, there will be k-1 skipped cells between each filter\n",
      "      element on that dimension. The dimension order is determined by the value of\n",
      "      `data_format`, see above for details. Dilations in the batch and depth\n",
      "      dimensions must be 1.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `filter`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'conv2d_backprop_input' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if use_cudnn_on_gpu is None:\n",
      "      use_cudnn_on_gpu = True\n",
      "    use_cudnn_on_gpu = _execute.make_bool(use_cudnn_on_gpu, \"use_cudnn_on_gpu\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    if dilations is None:\n",
      "      dilations = [1, 1, 1, 1]\n",
      "    if not isinstance(dilations, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'dilations' argument to \"\n",
      "          \"'conv2d_backprop_input' Op, not %r.\" % dilations)\n",
      "    dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Conv2DBackpropInput\", input_sizes=input_sizes, filter=filter,\n",
      "        out_backprop=out_backprop, strides=strides, padding=padding,\n",
      "        use_cudnn_on_gpu=use_cudnn_on_gpu, data_format=data_format,\n",
      "        dilations=dilations, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"strides\", _op.get_attr(\"strides\"),\n",
      "              \"use_cudnn_on_gpu\", _op.get_attr(\"use_cudnn_on_gpu\"), \"padding\",\n",
      "              _op.get_attr(\"padding\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"), \"dilations\",\n",
      "              _op.get_attr(\"dilations\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Conv2DBackpropInput\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"Conv2DBackpropInput\", name, _ctx._post_execution_callbacks,\n",
      "        input_sizes, filter, out_backprop, \"strides\", strides,\n",
      "        \"use_cudnn_on_gpu\", use_cudnn_on_gpu, \"padding\", padding,\n",
      "        \"data_format\", data_format, \"dilations\", dilations)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return conv2d_backprop_input_eager_fallback(\n",
      "          input_sizes, filter, out_backprop, strides=strides,\n",
      "          use_cudnn_on_gpu=use_cudnn_on_gpu, padding=padding,\n",
      "          data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def conv2d_backprop_input_eager_fallback(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu=True, data_format=\"NHWC\", dilations=[1, 1, 1, 1], name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function conv2d_backprop_input\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'conv2d_backprop_input' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if use_cudnn_on_gpu is None:\n",
      "    use_cudnn_on_gpu = True\n",
      "  use_cudnn_on_gpu = _execute.make_bool(use_cudnn_on_gpu, \"use_cudnn_on_gpu\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  if dilations is None:\n",
      "    dilations = [1, 1, 1, 1]\n",
      "  if not isinstance(dilations, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'dilations' argument to \"\n",
      "        \"'conv2d_backprop_input' Op, not %r.\" % dilations)\n",
      "  dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([filter, out_backprop], _ctx)\n",
      "  (filter, out_backprop) = _inputs_T\n",
      "  input_sizes = _ops.convert_to_tensor(input_sizes, _dtypes.int32)\n",
      "  _inputs_flat = [input_sizes, filter, out_backprop]\n",
      "  _attrs = (\"T\", _attr_T, \"strides\", strides, \"use_cudnn_on_gpu\",\n",
      "  use_cudnn_on_gpu, \"padding\", padding, \"data_format\", data_format,\n",
      "  \"dilations\", dilations)\n",
      "  _result = _execute.execute(b\"Conv2DBackpropInput\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Conv2DBackpropInput\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.conv3d')\n",
      "def conv3d(input, filter, strides, padding, data_format=\"NDHWC\", dilations=[1, 1, 1, 1, 1], name=None):\n",
      "  r\"\"\"Computes a 3-D convolution given 5-D `input` and `filter` tensors.\n",
      "\n",
      "  In signal processing, cross-correlation is a measure of similarity of\n",
      "  two waveforms as a function of a time-lag applied to one of them. This\n",
      "  is also known as a sliding dot product or sliding inner-product.\n",
      "\n",
      "  Our Conv3D implements a form of cross-correlation.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      Shape `[batch, in_depth, in_height, in_width, in_channels]`.\n",
      "    filter: A `Tensor`. Must have the same type as `input`.\n",
      "      Shape `[filter_depth, filter_height, filter_width, in_channels,\n",
      "      out_channels]`. `in_channels` must match between `input` and `filter`.\n",
      "    strides: A list of `ints` that has length `>= 5`.\n",
      "      1-D tensor of length 5. The stride of the sliding window for each\n",
      "      dimension of `input`. Must have `strides[0] = strides[4] = 1`.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NDHWC\", \"NCDHW\"`. Defaults to `\"NDHWC\"`.\n",
      "      The data format of the input and output data. With the\n",
      "      default format \"NDHWC\", the data is stored in the order of:\n",
      "          [batch, in_depth, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCDHW\", the data storage order is:\n",
      "          [batch, in_channels, in_depth, in_height, in_width].\n",
      "    dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.\n",
      "      1-D tensor of length 5.  The dilation factor for each dimension of\n",
      "      `input`. If set to k > 1, there will be k-1 skipped cells between each\n",
      "      filter element on that dimension. The dimension order is determined by the\n",
      "      value of `data_format`, see above for details. Dilations in the batch and\n",
      "      depth dimensions must be 1.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'conv3d' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NDHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    if dilations is None:\n",
      "      dilations = [1, 1, 1, 1, 1]\n",
      "    if not isinstance(dilations, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'dilations' argument to \"\n",
      "          \"'conv3d' Op, not %r.\" % dilations)\n",
      "    dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Conv3D\", input=input, filter=filter, strides=strides,\n",
      "        padding=padding, data_format=data_format, dilations=dilations,\n",
      "        name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"strides\", _op.get_attr(\"strides\"),\n",
      "              \"padding\", _op.get_attr(\"padding\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"), \"dilations\",\n",
      "              _op.get_attr(\"dilations\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Conv3D\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"Conv3D\", name,\n",
      "        _ctx._post_execution_callbacks, input, filter, \"strides\", strides,\n",
      "        \"padding\", padding, \"data_format\", data_format, \"dilations\",\n",
      "        dilations)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return conv3d_eager_fallback(\n",
      "          input, filter, strides=strides, padding=padding,\n",
      "          data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def conv3d_eager_fallback(input, filter, strides, padding, data_format=\"NDHWC\", dilations=[1, 1, 1, 1, 1], name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function conv3d\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'conv3d' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NDHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  if dilations is None:\n",
      "    dilations = [1, 1, 1, 1, 1]\n",
      "  if not isinstance(dilations, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'dilations' argument to \"\n",
      "        \"'conv3d' Op, not %r.\" % dilations)\n",
      "  dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter], _ctx)\n",
      "  (input, filter) = _inputs_T\n",
      "  _inputs_flat = [input, filter]\n",
      "  _attrs = (\"T\", _attr_T, \"strides\", strides, \"padding\", padding,\n",
      "  \"data_format\", data_format, \"dilations\", dilations)\n",
      "  _result = _execute.execute(b\"Conv3D\", 1, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Conv3D\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def conv3d_backprop_filter(input, filter, out_backprop, strides, padding, dilations=[1, 1, 1, 1, 1], name=None):\n",
      "  r\"\"\"Computes the gradients of 3-D convolution with respect to the filter.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.\n",
      "      Shape `[batch, depth, rows, cols, in_channels]`.\n",
      "    filter: A `Tensor`. Must have the same type as `input`.\n",
      "      Shape `[depth, rows, cols, in_channels, out_channels]`.\n",
      "      `in_channels` must match between `input` and `filter`.\n",
      "    out_backprop: A `Tensor`. Must have the same type as `input`.\n",
      "      Backprop signal of shape `[batch, out_depth, out_rows, out_cols,\n",
      "      out_channels]`.\n",
      "    strides: A list of `ints` that has length `>= 5`.\n",
      "      1-D tensor of length 5. The stride of the sliding window for each\n",
      "      dimension of `input`. Must have `strides[0] = strides[4] = 1`.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'conv3d_backprop_filter' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if dilations is None:\n",
      "      dilations = [1, 1, 1, 1, 1]\n",
      "    if not isinstance(dilations, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'dilations' argument to \"\n",
      "          \"'conv3d_backprop_filter' Op, not %r.\" % dilations)\n",
      "    dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Conv3DBackpropFilter\", input=input, filter=filter,\n",
      "        out_backprop=out_backprop, strides=strides, padding=padding,\n",
      "        dilations=dilations, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"strides\", _op.get_attr(\"strides\"),\n",
      "              \"padding\", _op.get_attr(\"padding\"), \"dilations\",\n",
      "              _op.get_attr(\"dilations\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Conv3DBackpropFilter\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"Conv3DBackpropFilter\", name, _ctx._post_execution_callbacks, input,\n",
      "        filter, out_backprop, \"strides\", strides, \"padding\", padding,\n",
      "        \"dilations\", dilations)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return conv3d_backprop_filter_eager_fallback(\n",
      "          input, filter, out_backprop, strides=strides, padding=padding,\n",
      "          dilations=dilations, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def conv3d_backprop_filter_eager_fallback(input, filter, out_backprop, strides, padding, dilations=[1, 1, 1, 1, 1], name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function conv3d_backprop_filter\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'conv3d_backprop_filter' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if dilations is None:\n",
      "    dilations = [1, 1, 1, 1, 1]\n",
      "  if not isinstance(dilations, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'dilations' argument to \"\n",
      "        \"'conv3d_backprop_filter' Op, not %r.\" % dilations)\n",
      "  dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter, out_backprop], _ctx)\n",
      "  (input, filter, out_backprop) = _inputs_T\n",
      "  _inputs_flat = [input, filter, out_backprop]\n",
      "  _attrs = (\"T\", _attr_T, \"strides\", strides, \"padding\", padding, \"dilations\",\n",
      "  dilations)\n",
      "  _result = _execute.execute(b\"Conv3DBackpropFilter\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Conv3DBackpropFilter\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.conv3d_backprop_filter_v2')\n",
      "def conv3d_backprop_filter_v2(input, filter_sizes, out_backprop, strides, padding, data_format=\"NDHWC\", dilations=[1, 1, 1, 1, 1], name=None):\n",
      "  r\"\"\"Computes the gradients of 3-D convolution with respect to the filter.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      Shape `[batch, depth, rows, cols, in_channels]`.\n",
      "    filter_sizes: A `Tensor` of type `int32`.\n",
      "      An integer vector representing the tensor shape of `filter`,\n",
      "      where `filter` is a 5-D\n",
      "      `[filter_depth, filter_height, filter_width, in_channels, out_channels]`\n",
      "      tensor.\n",
      "    out_backprop: A `Tensor`. Must have the same type as `input`.\n",
      "      Backprop signal of shape `[batch, out_depth, out_rows, out_cols,\n",
      "      out_channels]`.\n",
      "    strides: A list of `ints` that has length `>= 5`.\n",
      "      1-D tensor of length 5. The stride of the sliding window for each\n",
      "      dimension of `input`. Must have `strides[0] = strides[4] = 1`.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NDHWC\", \"NCDHW\"`. Defaults to `\"NDHWC\"`.\n",
      "      The data format of the input and output data. With the\n",
      "      default format \"NDHWC\", the data is stored in the order of:\n",
      "          [batch, in_depth, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCDHW\", the data storage order is:\n",
      "          [batch, in_channels, in_depth, in_height, in_width].\n",
      "    dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.\n",
      "      1-D tensor of length 5.  The dilation factor for each dimension of\n",
      "      `input`. If set to k > 1, there will be k-1 skipped cells between each\n",
      "      filter element on that dimension. The dimension order is determined by the\n",
      "      value of `data_format`, see above for details. Dilations in the batch and\n",
      "      depth dimensions must be 1.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'conv3d_backprop_filter_v2' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NDHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    if dilations is None:\n",
      "      dilations = [1, 1, 1, 1, 1]\n",
      "    if not isinstance(dilations, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'dilations' argument to \"\n",
      "          \"'conv3d_backprop_filter_v2' Op, not %r.\" % dilations)\n",
      "    dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Conv3DBackpropFilterV2\", input=input, filter_sizes=filter_sizes,\n",
      "        out_backprop=out_backprop, strides=strides, padding=padding,\n",
      "        data_format=data_format, dilations=dilations, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"strides\", _op.get_attr(\"strides\"),\n",
      "              \"padding\", _op.get_attr(\"padding\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"), \"dilations\",\n",
      "              _op.get_attr(\"dilations\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Conv3DBackpropFilterV2\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"Conv3DBackpropFilterV2\", name, _ctx._post_execution_callbacks, input,\n",
      "        filter_sizes, out_backprop, \"strides\", strides, \"padding\", padding,\n",
      "        \"data_format\", data_format, \"dilations\", dilations)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return conv3d_backprop_filter_v2_eager_fallback(\n",
      "          input, filter_sizes, out_backprop, strides=strides, padding=padding,\n",
      "          data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def conv3d_backprop_filter_v2_eager_fallback(input, filter_sizes, out_backprop, strides, padding, data_format=\"NDHWC\", dilations=[1, 1, 1, 1, 1], name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function conv3d_backprop_filter_v2\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'conv3d_backprop_filter_v2' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NDHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  if dilations is None:\n",
      "    dilations = [1, 1, 1, 1, 1]\n",
      "  if not isinstance(dilations, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'dilations' argument to \"\n",
      "        \"'conv3d_backprop_filter_v2' Op, not %r.\" % dilations)\n",
      "  dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, out_backprop], _ctx)\n",
      "  (input, out_backprop) = _inputs_T\n",
      "  filter_sizes = _ops.convert_to_tensor(filter_sizes, _dtypes.int32)\n",
      "  _inputs_flat = [input, filter_sizes, out_backprop]\n",
      "  _attrs = (\"T\", _attr_T, \"strides\", strides, \"padding\", padding,\n",
      "  \"data_format\", data_format, \"dilations\", dilations)\n",
      "  _result = _execute.execute(b\"Conv3DBackpropFilterV2\", 1,\n",
      "                             inputs=_inputs_flat, attrs=_attrs, ctx=_ctx,\n",
      "                             name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Conv3DBackpropFilterV2\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def conv3d_backprop_input(input, filter, out_backprop, strides, padding, dilations=[1, 1, 1, 1, 1], name=None):\n",
      "  r\"\"\"Computes the gradients of 3-D convolution with respect to the input.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.\n",
      "      Shape `[batch, depth, rows, cols, in_channels]`.\n",
      "    filter: A `Tensor`. Must have the same type as `input`.\n",
      "      Shape `[depth, rows, cols, in_channels, out_channels]`.\n",
      "      `in_channels` must match between `input` and `filter`.\n",
      "    out_backprop: A `Tensor`. Must have the same type as `input`.\n",
      "      Backprop signal of shape `[batch, out_depth, out_rows, out_cols,\n",
      "      out_channels]`.\n",
      "    strides: A list of `ints` that has length `>= 5`.\n",
      "      1-D tensor of length 5. The stride of the sliding window for each\n",
      "      dimension of `input`. Must have `strides[0] = strides[4] = 1`.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'conv3d_backprop_input' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if dilations is None:\n",
      "      dilations = [1, 1, 1, 1, 1]\n",
      "    if not isinstance(dilations, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'dilations' argument to \"\n",
      "          \"'conv3d_backprop_input' Op, not %r.\" % dilations)\n",
      "    dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Conv3DBackpropInput\", input=input, filter=filter,\n",
      "        out_backprop=out_backprop, strides=strides, padding=padding,\n",
      "        dilations=dilations, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"strides\", _op.get_attr(\"strides\"),\n",
      "              \"padding\", _op.get_attr(\"padding\"), \"dilations\",\n",
      "              _op.get_attr(\"dilations\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Conv3DBackpropInput\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"Conv3DBackpropInput\", name, _ctx._post_execution_callbacks, input,\n",
      "        filter, out_backprop, \"strides\", strides, \"padding\", padding,\n",
      "        \"dilations\", dilations)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return conv3d_backprop_input_eager_fallback(\n",
      "          input, filter, out_backprop, strides=strides, padding=padding,\n",
      "          dilations=dilations, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def conv3d_backprop_input_eager_fallback(input, filter, out_backprop, strides, padding, dilations=[1, 1, 1, 1, 1], name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function conv3d_backprop_input\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'conv3d_backprop_input' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if dilations is None:\n",
      "    dilations = [1, 1, 1, 1, 1]\n",
      "  if not isinstance(dilations, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'dilations' argument to \"\n",
      "        \"'conv3d_backprop_input' Op, not %r.\" % dilations)\n",
      "  dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter, out_backprop], _ctx)\n",
      "  (input, filter, out_backprop) = _inputs_T\n",
      "  _inputs_flat = [input, filter, out_backprop]\n",
      "  _attrs = (\"T\", _attr_T, \"strides\", strides, \"padding\", padding, \"dilations\",\n",
      "  dilations)\n",
      "  _result = _execute.execute(b\"Conv3DBackpropInput\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Conv3DBackpropInput\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def conv3d_backprop_input_v2(input_sizes, filter, out_backprop, strides, padding, data_format=\"NDHWC\", dilations=[1, 1, 1, 1, 1], name=None):\n",
      "  r\"\"\"Computes the gradients of 3-D convolution with respect to the input.\n",
      "\n",
      "  Args:\n",
      "    input_sizes: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "      An integer vector representing the tensor shape of `input`,\n",
      "      where `input` is a 5-D\n",
      "      `[batch, depth, rows, cols, in_channels]` tensor.\n",
      "    filter: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      Shape `[depth, rows, cols, in_channels, out_channels]`.\n",
      "      `in_channels` must match between `input` and `filter`.\n",
      "    out_backprop: A `Tensor`. Must have the same type as `filter`.\n",
      "      Backprop signal of shape `[batch, out_depth, out_rows, out_cols,\n",
      "      out_channels]`.\n",
      "    strides: A list of `ints` that has length `>= 5`.\n",
      "      1-D tensor of length 5. The stride of the sliding window for each\n",
      "      dimension of `input`. Must have `strides[0] = strides[4] = 1`.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NDHWC\", \"NCDHW\"`. Defaults to `\"NDHWC\"`.\n",
      "      The data format of the input and output data. With the\n",
      "      default format \"NDHWC\", the data is stored in the order of:\n",
      "          [batch, in_depth, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCDHW\", the data storage order is:\n",
      "          [batch, in_channels, in_depth, in_height, in_width].\n",
      "    dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1, 1]`.\n",
      "      1-D tensor of length 5.  The dilation factor for each dimension of\n",
      "      `input`. If set to k > 1, there will be k-1 skipped cells between each\n",
      "      filter element on that dimension. The dimension order is determined by the\n",
      "      value of `data_format`, see above for details. Dilations in the batch and\n",
      "      depth dimensions must be 1.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `filter`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'conv3d_backprop_input_v2' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NDHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    if dilations is None:\n",
      "      dilations = [1, 1, 1, 1, 1]\n",
      "    if not isinstance(dilations, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'dilations' argument to \"\n",
      "          \"'conv3d_backprop_input_v2' Op, not %r.\" % dilations)\n",
      "    dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Conv3DBackpropInputV2\", input_sizes=input_sizes, filter=filter,\n",
      "        out_backprop=out_backprop, strides=strides, padding=padding,\n",
      "        data_format=data_format, dilations=dilations, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"strides\", _op.get_attr(\"strides\"),\n",
      "              \"padding\", _op.get_attr(\"padding\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"), \"dilations\",\n",
      "              _op.get_attr(\"dilations\"), \"Tshape\", _op.get_attr(\"Tshape\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Conv3DBackpropInputV2\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"Conv3DBackpropInputV2\", name, _ctx._post_execution_callbacks,\n",
      "        input_sizes, filter, out_backprop, \"strides\", strides, \"padding\",\n",
      "        padding, \"data_format\", data_format, \"dilations\", dilations)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return conv3d_backprop_input_v2_eager_fallback(\n",
      "          input_sizes, filter, out_backprop, strides=strides, padding=padding,\n",
      "          data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def conv3d_backprop_input_v2_eager_fallback(input_sizes, filter, out_backprop, strides, padding, data_format=\"NDHWC\", dilations=[1, 1, 1, 1, 1], name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function conv3d_backprop_input_v2\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'conv3d_backprop_input_v2' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NDHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  if dilations is None:\n",
      "    dilations = [1, 1, 1, 1, 1]\n",
      "  if not isinstance(dilations, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'dilations' argument to \"\n",
      "        \"'conv3d_backprop_input_v2' Op, not %r.\" % dilations)\n",
      "  dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([filter, out_backprop], _ctx)\n",
      "  (filter, out_backprop) = _inputs_T\n",
      "  _attr_Tshape, (input_sizes,) = _execute.args_to_matching_eager([input_sizes], _ctx, _dtypes.int32)\n",
      "  _inputs_flat = [input_sizes, filter, out_backprop]\n",
      "  _attrs = (\"T\", _attr_T, \"strides\", strides, \"padding\", padding,\n",
      "  \"data_format\", data_format, \"dilations\", dilations, \"Tshape\", _attr_Tshape)\n",
      "  _result = _execute.execute(b\"Conv3DBackpropInputV2\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Conv3DBackpropInputV2\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def data_format_dim_map(x, src_format=\"NHWC\", dst_format=\"NCHW\", name=None):\n",
      "  r\"\"\"Returns the dimension index in the destination data format given the one in\n",
      "\n",
      "  the source data format.\n",
      "\n",
      "  Args:\n",
      "    x: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "      A Tensor with each element as a dimension index in source data format.\n",
      "      Must be in the range [-4, 4).\n",
      "    src_format: An optional `string`. Defaults to `\"NHWC\"`.\n",
      "      source data format.\n",
      "    dst_format: An optional `string`. Defaults to `\"NCHW\"`.\n",
      "      destination data format.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `x`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if src_format is None:\n",
      "      src_format = \"NHWC\"\n",
      "    src_format = _execute.make_str(src_format, \"src_format\")\n",
      "    if dst_format is None:\n",
      "      dst_format = \"NCHW\"\n",
      "    dst_format = _execute.make_str(dst_format, \"dst_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"DataFormatDimMap\", x=x, src_format=src_format, dst_format=dst_format,\n",
      "        name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"src_format\",\n",
      "              _op.get_attr(\"src_format\"), \"dst_format\",\n",
      "              _op.get_attr(\"dst_format\"))\n",
      "    _execute.record_gradient(\n",
      "      \"DataFormatDimMap\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"DataFormatDimMap\", name, _ctx._post_execution_callbacks, x,\n",
      "        \"src_format\", src_format, \"dst_format\", dst_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return data_format_dim_map_eager_fallback(\n",
      "          x, src_format=src_format, dst_format=dst_format, name=name,\n",
      "          ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def data_format_dim_map_eager_fallback(x, src_format=\"NHWC\", dst_format=\"NCHW\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function data_format_dim_map\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if src_format is None:\n",
      "    src_format = \"NHWC\"\n",
      "  src_format = _execute.make_str(src_format, \"src_format\")\n",
      "  if dst_format is None:\n",
      "    dst_format = \"NCHW\"\n",
      "  dst_format = _execute.make_str(dst_format, \"dst_format\")\n",
      "  _attr_T, (x,) = _execute.args_to_matching_eager([x], _ctx, _dtypes.int32)\n",
      "  _inputs_flat = [x]\n",
      "  _attrs = (\"T\", _attr_T, \"src_format\", src_format, \"dst_format\", dst_format)\n",
      "  _result = _execute.execute(b\"DataFormatDimMap\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"DataFormatDimMap\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def data_format_vec_permute(x, src_format=\"NHWC\", dst_format=\"NCHW\", name=None):\n",
      "  r\"\"\"Returns the permuted vector/tensor in the destination data format given the\n",
      "\n",
      "  one in the source data format.\n",
      "\n",
      "  Args:\n",
      "    x: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "      Vector of size 4 or Tensor of shape (4, 2) in source data format.\n",
      "    src_format: An optional `string`. Defaults to `\"NHWC\"`.\n",
      "      source data format.\n",
      "    dst_format: An optional `string`. Defaults to `\"NCHW\"`.\n",
      "      destination data format.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `x`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if src_format is None:\n",
      "      src_format = \"NHWC\"\n",
      "    src_format = _execute.make_str(src_format, \"src_format\")\n",
      "    if dst_format is None:\n",
      "      dst_format = \"NCHW\"\n",
      "    dst_format = _execute.make_str(dst_format, \"dst_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"DataFormatVecPermute\", x=x, src_format=src_format,\n",
      "        dst_format=dst_format, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"src_format\",\n",
      "              _op.get_attr(\"src_format\"), \"dst_format\",\n",
      "              _op.get_attr(\"dst_format\"))\n",
      "    _execute.record_gradient(\n",
      "      \"DataFormatVecPermute\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"DataFormatVecPermute\", name, _ctx._post_execution_callbacks, x,\n",
      "        \"src_format\", src_format, \"dst_format\", dst_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return data_format_vec_permute_eager_fallback(\n",
      "          x, src_format=src_format, dst_format=dst_format, name=name,\n",
      "          ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def data_format_vec_permute_eager_fallback(x, src_format=\"NHWC\", dst_format=\"NCHW\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function data_format_vec_permute\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if src_format is None:\n",
      "    src_format = \"NHWC\"\n",
      "  src_format = _execute.make_str(src_format, \"src_format\")\n",
      "  if dst_format is None:\n",
      "    dst_format = \"NCHW\"\n",
      "  dst_format = _execute.make_str(dst_format, \"dst_format\")\n",
      "  _attr_T, (x,) = _execute.args_to_matching_eager([x], _ctx, _dtypes.int32)\n",
      "  _inputs_flat = [x]\n",
      "  _attrs = (\"T\", _attr_T, \"src_format\", src_format, \"dst_format\", dst_format)\n",
      "  _result = _execute.execute(b\"DataFormatVecPermute\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"DataFormatVecPermute\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.depthwise_conv2d_native')\n",
      "def depthwise_conv2d_native(input, filter, strides, padding, data_format=\"NHWC\", dilations=[1, 1, 1, 1], name=None):\n",
      "  r\"\"\"Computes a 2-D depthwise convolution given 4-D `input` and `filter` tensors.\n",
      "\n",
      "  Given an input tensor of shape `[batch, in_height, in_width, in_channels]`\n",
      "  and a filter / kernel tensor of shape\n",
      "  `[filter_height, filter_width, in_channels, channel_multiplier]`, containing\n",
      "  `in_channels` convolutional filters of depth 1, `depthwise_conv2d` applies\n",
      "  a different filter to each input channel (expanding from 1 channel to\n",
      "  `channel_multiplier` channels for each), then concatenates the results\n",
      "  together. Thus, the output has `in_channels * channel_multiplier` channels.\n",
      "\n",
      "  ```\n",
      "  for k in 0..in_channels-1\n",
      "    for q in 0..channel_multiplier-1\n",
      "      output[b, i, j, k * channel_multiplier + q] =\n",
      "        sum_{di, dj} input[b, strides[1] * i + di, strides[2] * j + dj, k] *\n",
      "                          filter[di, dj, k, q]\n",
      "  ```\n",
      "\n",
      "  Must have `strides[0] = strides[3] = 1`.  For the most common case of the same\n",
      "  horizontal and vertices strides, `strides = [1, stride, stride, 1]`.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "    filter: A `Tensor`. Must have the same type as `input`.\n",
      "    strides: A list of `ints`.\n",
      "      1-D of length 4.  The stride of the sliding window for each dimension\n",
      "      of `input`.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the data is stored in the order of:\n",
      "          [batch, height, width, channels].\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, channels, height, width].\n",
      "    dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.\n",
      "      1-D tensor of length 4.  The dilation factor for each dimension of\n",
      "      `input`. If set to k > 1, there will be k-1 skipped cells between each filter\n",
      "      element on that dimension. The dimension order is determined by the value of\n",
      "      `data_format`, see above for details. Dilations in the batch and depth\n",
      "      dimensions must be 1.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'depthwise_conv2d_native' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    if dilations is None:\n",
      "      dilations = [1, 1, 1, 1]\n",
      "    if not isinstance(dilations, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'dilations' argument to \"\n",
      "          \"'depthwise_conv2d_native' Op, not %r.\" % dilations)\n",
      "    dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"DepthwiseConv2dNative\", input=input, filter=filter, strides=strides,\n",
      "        padding=padding, data_format=data_format, dilations=dilations,\n",
      "        name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"strides\", _op.get_attr(\"strides\"),\n",
      "              \"padding\", _op.get_attr(\"padding\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"), \"dilations\",\n",
      "              _op.get_attr(\"dilations\"))\n",
      "    _execute.record_gradient(\n",
      "      \"DepthwiseConv2dNative\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"DepthwiseConv2dNative\", name, _ctx._post_execution_callbacks, input,\n",
      "        filter, \"strides\", strides, \"padding\", padding, \"data_format\",\n",
      "        data_format, \"dilations\", dilations)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return depthwise_conv2d_native_eager_fallback(\n",
      "          input, filter, strides=strides, padding=padding,\n",
      "          data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def depthwise_conv2d_native_eager_fallback(input, filter, strides, padding, data_format=\"NHWC\", dilations=[1, 1, 1, 1], name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function depthwise_conv2d_native\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'depthwise_conv2d_native' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  if dilations is None:\n",
      "    dilations = [1, 1, 1, 1]\n",
      "  if not isinstance(dilations, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'dilations' argument to \"\n",
      "        \"'depthwise_conv2d_native' Op, not %r.\" % dilations)\n",
      "  dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter], _ctx)\n",
      "  (input, filter) = _inputs_T\n",
      "  _inputs_flat = [input, filter]\n",
      "  _attrs = (\"T\", _attr_T, \"strides\", strides, \"padding\", padding,\n",
      "  \"data_format\", data_format, \"dilations\", dilations)\n",
      "  _result = _execute.execute(b\"DepthwiseConv2dNative\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"DepthwiseConv2dNative\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.depthwise_conv2d_native_backprop_filter')\n",
      "def depthwise_conv2d_native_backprop_filter(input, filter_sizes, out_backprop, strides, padding, data_format=\"NHWC\", dilations=[1, 1, 1, 1], name=None):\n",
      "  r\"\"\"Computes the gradients of depthwise convolution with respect to the filter.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      4-D with shape based on `data_format`.  For example, if\n",
      "      `data_format` is 'NHWC' then `input` is a 4-D `[batch, in_height,\n",
      "      in_width, in_channels]` tensor.\n",
      "    filter_sizes: A `Tensor` of type `int32`.\n",
      "      An integer vector representing the tensor shape of `filter`,\n",
      "      where `filter` is a 4-D\n",
      "      `[filter_height, filter_width, in_channels, depthwise_multiplier]` tensor.\n",
      "    out_backprop: A `Tensor`. Must have the same type as `input`.\n",
      "      4-D with shape  based on `data_format`.\n",
      "      For example, if `data_format` is 'NHWC' then\n",
      "      out_backprop shape is `[batch, out_height, out_width, out_channels]`.\n",
      "      Gradients w.r.t. the output of the convolution.\n",
      "    strides: A list of `ints`.\n",
      "      The stride of the sliding window for each dimension of the input\n",
      "      of the convolution.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the data is stored in the order of:\n",
      "          [batch, height, width, channels].\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, channels, height, width].\n",
      "    dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.\n",
      "      1-D tensor of length 4.  The dilation factor for each dimension of\n",
      "      `input`. If set to k > 1, there will be k-1 skipped cells between each filter\n",
      "      element on that dimension. The dimension order is determined by the value of\n",
      "      `data_format`, see above for details. Dilations in the batch and depth\n",
      "      dimensions must be 1.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'depthwise_conv2d_native_backprop_filter' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    if dilations is None:\n",
      "      dilations = [1, 1, 1, 1]\n",
      "    if not isinstance(dilations, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'dilations' argument to \"\n",
      "          \"'depthwise_conv2d_native_backprop_filter' Op, not %r.\" % dilations)\n",
      "    dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"DepthwiseConv2dNativeBackpropFilter\", input=input,\n",
      "        filter_sizes=filter_sizes, out_backprop=out_backprop, strides=strides,\n",
      "        padding=padding, data_format=data_format, dilations=dilations,\n",
      "        name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"strides\", _op.get_attr(\"strides\"),\n",
      "              \"padding\", _op.get_attr(\"padding\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"), \"dilations\",\n",
      "              _op.get_attr(\"dilations\"))\n",
      "    _execute.record_gradient(\n",
      "      \"DepthwiseConv2dNativeBackpropFilter\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"DepthwiseConv2dNativeBackpropFilter\", name,\n",
      "        _ctx._post_execution_callbacks, input, filter_sizes, out_backprop,\n",
      "        \"strides\", strides, \"padding\", padding, \"data_format\", data_format,\n",
      "        \"dilations\", dilations)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return depthwise_conv2d_native_backprop_filter_eager_fallback(\n",
      "          input, filter_sizes, out_backprop, strides=strides, padding=padding,\n",
      "          data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def depthwise_conv2d_native_backprop_filter_eager_fallback(input, filter_sizes, out_backprop, strides, padding, data_format=\"NHWC\", dilations=[1, 1, 1, 1], name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function depthwise_conv2d_native_backprop_filter\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'depthwise_conv2d_native_backprop_filter' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  if dilations is None:\n",
      "    dilations = [1, 1, 1, 1]\n",
      "  if not isinstance(dilations, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'dilations' argument to \"\n",
      "        \"'depthwise_conv2d_native_backprop_filter' Op, not %r.\" % dilations)\n",
      "  dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, out_backprop], _ctx)\n",
      "  (input, out_backprop) = _inputs_T\n",
      "  filter_sizes = _ops.convert_to_tensor(filter_sizes, _dtypes.int32)\n",
      "  _inputs_flat = [input, filter_sizes, out_backprop]\n",
      "  _attrs = (\"T\", _attr_T, \"strides\", strides, \"padding\", padding,\n",
      "  \"data_format\", data_format, \"dilations\", dilations)\n",
      "  _result = _execute.execute(b\"DepthwiseConv2dNativeBackpropFilter\", 1,\n",
      "                             inputs=_inputs_flat, attrs=_attrs, ctx=_ctx,\n",
      "                             name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"DepthwiseConv2dNativeBackpropFilter\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.depthwise_conv2d_native_backprop_input')\n",
      "def depthwise_conv2d_native_backprop_input(input_sizes, filter, out_backprop, strides, padding, data_format=\"NHWC\", dilations=[1, 1, 1, 1], name=None):\n",
      "  r\"\"\"Computes the gradients of depthwise convolution with respect to the input.\n",
      "\n",
      "  Args:\n",
      "    input_sizes: A `Tensor` of type `int32`.\n",
      "      An integer vector representing the shape of `input`, based\n",
      "      on `data_format`.  For example, if `data_format` is 'NHWC' then\n",
      "       `input` is a 4-D `[batch, height, width, channels]` tensor.\n",
      "    filter: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      4-D with shape\n",
      "      `[filter_height, filter_width, in_channels, depthwise_multiplier]`.\n",
      "    out_backprop: A `Tensor`. Must have the same type as `filter`.\n",
      "      4-D with shape  based on `data_format`.\n",
      "      For example, if `data_format` is 'NHWC' then\n",
      "      out_backprop shape is `[batch, out_height, out_width, out_channels]`.\n",
      "      Gradients w.r.t. the output of the convolution.\n",
      "    strides: A list of `ints`.\n",
      "      The stride of the sliding window for each dimension of the input\n",
      "      of the convolution.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the data is stored in the order of:\n",
      "          [batch, height, width, channels].\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, channels, height, width].\n",
      "    dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.\n",
      "      1-D tensor of length 4.  The dilation factor for each dimension of\n",
      "      `input`. If set to k > 1, there will be k-1 skipped cells between each filter\n",
      "      element on that dimension. The dimension order is determined by the value of\n",
      "      `data_format`, see above for details. Dilations in the batch and depth\n",
      "      dimensions must be 1.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `filter`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'depthwise_conv2d_native_backprop_input' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    if dilations is None:\n",
      "      dilations = [1, 1, 1, 1]\n",
      "    if not isinstance(dilations, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'dilations' argument to \"\n",
      "          \"'depthwise_conv2d_native_backprop_input' Op, not %r.\" % dilations)\n",
      "    dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"DepthwiseConv2dNativeBackpropInput\", input_sizes=input_sizes,\n",
      "        filter=filter, out_backprop=out_backprop, strides=strides,\n",
      "        padding=padding, data_format=data_format, dilations=dilations,\n",
      "        name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"strides\", _op.get_attr(\"strides\"),\n",
      "              \"padding\", _op.get_attr(\"padding\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"), \"dilations\",\n",
      "              _op.get_attr(\"dilations\"))\n",
      "    _execute.record_gradient(\n",
      "      \"DepthwiseConv2dNativeBackpropInput\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"DepthwiseConv2dNativeBackpropInput\", name,\n",
      "        _ctx._post_execution_callbacks, input_sizes, filter, out_backprop,\n",
      "        \"strides\", strides, \"padding\", padding, \"data_format\", data_format,\n",
      "        \"dilations\", dilations)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return depthwise_conv2d_native_backprop_input_eager_fallback(\n",
      "          input_sizes, filter, out_backprop, strides=strides, padding=padding,\n",
      "          data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def depthwise_conv2d_native_backprop_input_eager_fallback(input_sizes, filter, out_backprop, strides, padding, data_format=\"NHWC\", dilations=[1, 1, 1, 1], name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function depthwise_conv2d_native_backprop_input\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'depthwise_conv2d_native_backprop_input' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  if dilations is None:\n",
      "    dilations = [1, 1, 1, 1]\n",
      "  if not isinstance(dilations, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'dilations' argument to \"\n",
      "        \"'depthwise_conv2d_native_backprop_input' Op, not %r.\" % dilations)\n",
      "  dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([filter, out_backprop], _ctx)\n",
      "  (filter, out_backprop) = _inputs_T\n",
      "  input_sizes = _ops.convert_to_tensor(input_sizes, _dtypes.int32)\n",
      "  _inputs_flat = [input_sizes, filter, out_backprop]\n",
      "  _attrs = (\"T\", _attr_T, \"strides\", strides, \"padding\", padding,\n",
      "  \"data_format\", data_format, \"dilations\", dilations)\n",
      "  _result = _execute.execute(b\"DepthwiseConv2dNativeBackpropInput\", 1,\n",
      "                             inputs=_inputs_flat, attrs=_attrs, ctx=_ctx,\n",
      "                             name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"DepthwiseConv2dNativeBackpropInput\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.dilation2d')\n",
      "def dilation2d(input, filter, strides, rates, padding, name=None):\n",
      "  r\"\"\"Computes the grayscale dilation of 4-D `input` and 3-D `filter` tensors.\n",
      "\n",
      "  The `input` tensor has shape `[batch, in_height, in_width, depth]` and the\n",
      "  `filter` tensor has shape `[filter_height, filter_width, depth]`, i.e., each\n",
      "  input channel is processed independently of the others with its own structuring\n",
      "  function. The `output` tensor has shape\n",
      "  `[batch, out_height, out_width, depth]`. The spatial dimensions of the output\n",
      "  tensor depend on the `padding` algorithm. We currently only support the default\n",
      "  \"NHWC\" `data_format`.\n",
      "\n",
      "  In detail, the grayscale morphological 2-D dilation is the max-sum correlation\n",
      "  (for consistency with `conv2d`, we use unmirrored filters):\n",
      "\n",
      "      output[b, y, x, c] =\n",
      "         max_{dy, dx} input[b,\n",
      "                            strides[1] * y + rates[1] * dy,\n",
      "                            strides[2] * x + rates[2] * dx,\n",
      "                            c] +\n",
      "                      filter[dy, dx, c]\n",
      "\n",
      "  Max-pooling is a special case when the filter has size equal to the pooling\n",
      "  kernel size and contains all zeros.\n",
      "\n",
      "  Note on duality: The dilation of `input` by the `filter` is equal to the\n",
      "  negation of the erosion of `-input` by the reflected `filter`.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      4-D with shape `[batch, in_height, in_width, depth]`.\n",
      "    filter: A `Tensor`. Must have the same type as `input`.\n",
      "      3-D with shape `[filter_height, filter_width, depth]`.\n",
      "    strides: A list of `ints` that has length `>= 4`.\n",
      "      The stride of the sliding window for each dimension of the input\n",
      "      tensor. Must be: `[1, stride_height, stride_width, 1]`.\n",
      "    rates: A list of `ints` that has length `>= 4`.\n",
      "      The input stride for atrous morphological dilation. Must be:\n",
      "      `[1, rate_height, rate_width, 1]`.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'dilation2d' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    if not isinstance(rates, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'rates' argument to \"\n",
      "          \"'dilation2d' Op, not %r.\" % rates)\n",
      "    rates = [_execute.make_int(_i, \"rates\") for _i in rates]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Dilation2D\", input=input, filter=filter, strides=strides,\n",
      "        rates=rates, padding=padding, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"strides\", _op.get_attr(\"strides\"),\n",
      "              \"rates\", _op.get_attr(\"rates\"), \"padding\",\n",
      "              _op.get_attr(\"padding\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Dilation2D\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"Dilation2D\",\n",
      "        name, _ctx._post_execution_callbacks, input, filter, \"strides\",\n",
      "        strides, \"rates\", rates, \"padding\", padding)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return dilation2d_eager_fallback(\n",
      "          input, filter, strides=strides, rates=rates, padding=padding,\n",
      "          name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def dilation2d_eager_fallback(input, filter, strides, rates, padding, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function dilation2d\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'dilation2d' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  if not isinstance(rates, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'rates' argument to \"\n",
      "        \"'dilation2d' Op, not %r.\" % rates)\n",
      "  rates = [_execute.make_int(_i, \"rates\") for _i in rates]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter], _ctx)\n",
      "  (input, filter) = _inputs_T\n",
      "  _inputs_flat = [input, filter]\n",
      "  _attrs = (\"T\", _attr_T, \"strides\", strides, \"rates\", rates, \"padding\",\n",
      "  padding)\n",
      "  _result = _execute.execute(b\"Dilation2D\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Dilation2D\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def dilation2d_backprop_filter(input, filter, out_backprop, strides, rates, padding, name=None):\n",
      "  r\"\"\"Computes the gradient of morphological 2-D dilation with respect to the filter.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      4-D with shape `[batch, in_height, in_width, depth]`.\n",
      "    filter: A `Tensor`. Must have the same type as `input`.\n",
      "      3-D with shape `[filter_height, filter_width, depth]`.\n",
      "    out_backprop: A `Tensor`. Must have the same type as `input`.\n",
      "      4-D with shape `[batch, out_height, out_width, depth]`.\n",
      "    strides: A list of `ints` that has length `>= 4`.\n",
      "      1-D of length 4. The stride of the sliding window for each dimension of\n",
      "      the input tensor. Must be: `[1, stride_height, stride_width, 1]`.\n",
      "    rates: A list of `ints` that has length `>= 4`.\n",
      "      1-D of length 4. The input stride for atrous morphological dilation.\n",
      "      Must be: `[1, rate_height, rate_width, 1]`.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'dilation2d_backprop_filter' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    if not isinstance(rates, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'rates' argument to \"\n",
      "          \"'dilation2d_backprop_filter' Op, not %r.\" % rates)\n",
      "    rates = [_execute.make_int(_i, \"rates\") for _i in rates]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Dilation2DBackpropFilter\", input=input, filter=filter,\n",
      "        out_backprop=out_backprop, strides=strides, rates=rates,\n",
      "        padding=padding, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"strides\", _op.get_attr(\"strides\"),\n",
      "              \"rates\", _op.get_attr(\"rates\"), \"padding\",\n",
      "              _op.get_attr(\"padding\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Dilation2DBackpropFilter\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"Dilation2DBackpropFilter\", name, _ctx._post_execution_callbacks,\n",
      "        input, filter, out_backprop, \"strides\", strides, \"rates\", rates,\n",
      "        \"padding\", padding)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return dilation2d_backprop_filter_eager_fallback(\n",
      "          input, filter, out_backprop, strides=strides, rates=rates,\n",
      "          padding=padding, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def dilation2d_backprop_filter_eager_fallback(input, filter, out_backprop, strides, rates, padding, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function dilation2d_backprop_filter\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'dilation2d_backprop_filter' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  if not isinstance(rates, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'rates' argument to \"\n",
      "        \"'dilation2d_backprop_filter' Op, not %r.\" % rates)\n",
      "  rates = [_execute.make_int(_i, \"rates\") for _i in rates]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter, out_backprop], _ctx)\n",
      "  (input, filter, out_backprop) = _inputs_T\n",
      "  _inputs_flat = [input, filter, out_backprop]\n",
      "  _attrs = (\"T\", _attr_T, \"strides\", strides, \"rates\", rates, \"padding\",\n",
      "  padding)\n",
      "  _result = _execute.execute(b\"Dilation2DBackpropFilter\", 1,\n",
      "                             inputs=_inputs_flat, attrs=_attrs, ctx=_ctx,\n",
      "                             name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Dilation2DBackpropFilter\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def dilation2d_backprop_input(input, filter, out_backprop, strides, rates, padding, name=None):\n",
      "  r\"\"\"Computes the gradient of morphological 2-D dilation with respect to the input.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      4-D with shape `[batch, in_height, in_width, depth]`.\n",
      "    filter: A `Tensor`. Must have the same type as `input`.\n",
      "      3-D with shape `[filter_height, filter_width, depth]`.\n",
      "    out_backprop: A `Tensor`. Must have the same type as `input`.\n",
      "      4-D with shape `[batch, out_height, out_width, depth]`.\n",
      "    strides: A list of `ints` that has length `>= 4`.\n",
      "      1-D of length 4. The stride of the sliding window for each dimension of\n",
      "      the input tensor. Must be: `[1, stride_height, stride_width, 1]`.\n",
      "    rates: A list of `ints` that has length `>= 4`.\n",
      "      1-D of length 4. The input stride for atrous morphological dilation.\n",
      "      Must be: `[1, rate_height, rate_width, 1]`.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'dilation2d_backprop_input' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    if not isinstance(rates, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'rates' argument to \"\n",
      "          \"'dilation2d_backprop_input' Op, not %r.\" % rates)\n",
      "    rates = [_execute.make_int(_i, \"rates\") for _i in rates]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Dilation2DBackpropInput\", input=input, filter=filter,\n",
      "        out_backprop=out_backprop, strides=strides, rates=rates,\n",
      "        padding=padding, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"strides\", _op.get_attr(\"strides\"),\n",
      "              \"rates\", _op.get_attr(\"rates\"), \"padding\",\n",
      "              _op.get_attr(\"padding\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Dilation2DBackpropInput\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"Dilation2DBackpropInput\", name, _ctx._post_execution_callbacks,\n",
      "        input, filter, out_backprop, \"strides\", strides, \"rates\", rates,\n",
      "        \"padding\", padding)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return dilation2d_backprop_input_eager_fallback(\n",
      "          input, filter, out_backprop, strides=strides, rates=rates,\n",
      "          padding=padding, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def dilation2d_backprop_input_eager_fallback(input, filter, out_backprop, strides, rates, padding, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function dilation2d_backprop_input\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'dilation2d_backprop_input' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  if not isinstance(rates, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'rates' argument to \"\n",
      "        \"'dilation2d_backprop_input' Op, not %r.\" % rates)\n",
      "  rates = [_execute.make_int(_i, \"rates\") for _i in rates]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter, out_backprop], _ctx)\n",
      "  (input, filter, out_backprop) = _inputs_T\n",
      "  _inputs_flat = [input, filter, out_backprop]\n",
      "  _attrs = (\"T\", _attr_T, \"strides\", strides, \"rates\", rates, \"padding\",\n",
      "  padding)\n",
      "  _result = _execute.execute(b\"Dilation2DBackpropInput\", 1,\n",
      "                             inputs=_inputs_flat, attrs=_attrs, ctx=_ctx,\n",
      "                             name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Dilation2DBackpropInput\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.elu')\n",
      "def elu(features, name=None):\n",
      "  r\"\"\"Computes exponential linear: `exp(features) - 1` if < 0, `features` otherwise.\n",
      "\n",
      "  See [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\n",
      "  ](http://arxiv.org/abs/1511.07289)\n",
      "\n",
      "  Args:\n",
      "    features: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `features`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Elu\", features=features, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Elu\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"Elu\", name,\n",
      "        _ctx._post_execution_callbacks, features)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return elu_eager_fallback(\n",
      "          features, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def elu_eager_fallback(features, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function elu\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, (features,) = _execute.args_to_matching_eager([features], _ctx)\n",
      "  _inputs_flat = [features]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"Elu\", 1, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Elu\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def elu_grad(gradients, outputs, name=None):\n",
      "  r\"\"\"Computes gradients for the exponential linear (Elu) operation.\n",
      "\n",
      "  Args:\n",
      "    gradients: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      The backpropagated gradients to the corresponding Elu operation.\n",
      "    outputs: A `Tensor`. Must have the same type as `gradients`.\n",
      "      The outputs of the corresponding Elu operation.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `gradients`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"EluGrad\", gradients=gradients, outputs=outputs, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"EluGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"EluGrad\",\n",
      "        name, _ctx._post_execution_callbacks, gradients, outputs)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return elu_grad_eager_fallback(\n",
      "          gradients, outputs, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def elu_grad_eager_fallback(gradients, outputs, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function elu_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([gradients, outputs], _ctx)\n",
      "  (gradients, outputs) = _inputs_T\n",
      "  _inputs_flat = [gradients, outputs]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"EluGrad\", 1, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"EluGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "_fractional_avg_pool_outputs = [\"output\", \"row_pooling_sequence\",\n",
      "                               \"col_pooling_sequence\"]\n",
      "_FractionalAvgPoolOutput = _collections.namedtuple(\n",
      "    \"FractionalAvgPool\", _fractional_avg_pool_outputs)\n",
      "\n",
      "\n",
      "@tf_export('nn.fractional_avg_pool')\n",
      "def fractional_avg_pool(value, pooling_ratio, pseudo_random=False, overlapping=False, deterministic=False, seed=0, seed2=0, name=None):\n",
      "  r\"\"\"Performs fractional average pooling on the input.\n",
      "\n",
      "  Fractional average pooling is similar to Fractional max pooling in the pooling\n",
      "  region generation step. The only difference is that after pooling regions are\n",
      "  generated, a mean operation is performed instead of a max operation in each\n",
      "  pooling region.\n",
      "\n",
      "  Args:\n",
      "    value: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`.\n",
      "      4-D with shape `[batch, height, width, channels]`.\n",
      "    pooling_ratio: A list of `floats` that has length `>= 4`.\n",
      "      Pooling ratio for each dimension of `value`, currently only\n",
      "      supports row and col dimension and should be >= 1.0. For example, a valid\n",
      "      pooling ratio looks like [1.0, 1.44, 1.73, 1.0]. The first and last elements\n",
      "      must be 1.0 because we don't allow pooling on batch and channels\n",
      "      dimensions. 1.44 and 1.73 are pooling ratio on height and width dimensions\n",
      "      respectively.\n",
      "    pseudo_random: An optional `bool`. Defaults to `False`.\n",
      "      When set to True, generates the pooling sequence in a\n",
      "      pseudorandom fashion, otherwise, in a random fashion. Check paper [Benjamin\n",
      "      Graham, Fractional Max-Pooling](http://arxiv.org/abs/1412.6071) for\n",
      "      difference between pseudorandom and random.\n",
      "    overlapping: An optional `bool`. Defaults to `False`.\n",
      "      When set to True, it means when pooling, the values at the boundary\n",
      "      of adjacent pooling cells are used by both cells. For example:\n",
      "\n",
      "      `index  0  1  2  3  4`\n",
      "\n",
      "      `value  20 5  16 3  7`\n",
      "\n",
      "      If the pooling sequence is [0, 2, 4], then 16, at index 2 will be used twice.\n",
      "      The result would be [41/3, 26/3] for fractional avg pooling.\n",
      "    deterministic: An optional `bool`. Defaults to `False`.\n",
      "      When set to True, a fixed pooling region will be used when\n",
      "      iterating over a FractionalAvgPool node in the computation graph. Mainly used\n",
      "      in unit test to make FractionalAvgPool deterministic.\n",
      "    seed: An optional `int`. Defaults to `0`.\n",
      "      If either seed or seed2 are set to be non-zero, the random number\n",
      "      generator is seeded by the given seed.  Otherwise, it is seeded by a\n",
      "      random seed.\n",
      "    seed2: An optional `int`. Defaults to `0`.\n",
      "      An second seed to avoid seed collision.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (output, row_pooling_sequence, col_pooling_sequence).\n",
      "\n",
      "    output: A `Tensor`. Has the same type as `value`.\n",
      "    row_pooling_sequence: A `Tensor` of type `int64`.\n",
      "    col_pooling_sequence: A `Tensor` of type `int64`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(pooling_ratio, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'pooling_ratio' argument to \"\n",
      "          \"'fractional_avg_pool' Op, not %r.\" % pooling_ratio)\n",
      "    pooling_ratio = [_execute.make_float(_f, \"pooling_ratio\") for _f in pooling_ratio]\n",
      "    if pseudo_random is None:\n",
      "      pseudo_random = False\n",
      "    pseudo_random = _execute.make_bool(pseudo_random, \"pseudo_random\")\n",
      "    if overlapping is None:\n",
      "      overlapping = False\n",
      "    overlapping = _execute.make_bool(overlapping, \"overlapping\")\n",
      "    if deterministic is None:\n",
      "      deterministic = False\n",
      "    deterministic = _execute.make_bool(deterministic, \"deterministic\")\n",
      "    if seed is None:\n",
      "      seed = 0\n",
      "    seed = _execute.make_int(seed, \"seed\")\n",
      "    if seed2 is None:\n",
      "      seed2 = 0\n",
      "    seed2 = _execute.make_int(seed2, \"seed2\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"FractionalAvgPool\", value=value, pooling_ratio=pooling_ratio,\n",
      "        pseudo_random=pseudo_random, overlapping=overlapping,\n",
      "        deterministic=deterministic, seed=seed, seed2=seed2, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"pooling_ratio\", _op.get_attr(\"pooling_ratio\"), \"pseudo_random\",\n",
      "              _op.get_attr(\"pseudo_random\"), \"overlapping\",\n",
      "              _op.get_attr(\"overlapping\"), \"deterministic\",\n",
      "              _op.get_attr(\"deterministic\"), \"seed\", _op.get_attr(\"seed\"),\n",
      "              \"seed2\", _op.get_attr(\"seed2\"), \"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"FractionalAvgPool\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _FractionalAvgPoolOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"FractionalAvgPool\", name, _ctx._post_execution_callbacks, value,\n",
      "        \"pooling_ratio\", pooling_ratio, \"pseudo_random\", pseudo_random,\n",
      "        \"overlapping\", overlapping, \"deterministic\", deterministic, \"seed\",\n",
      "        seed, \"seed2\", seed2)\n",
      "      _result = _FractionalAvgPoolOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return fractional_avg_pool_eager_fallback(\n",
      "          value, pooling_ratio=pooling_ratio, pseudo_random=pseudo_random,\n",
      "          overlapping=overlapping, deterministic=deterministic, seed=seed,\n",
      "          seed2=seed2, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def fractional_avg_pool_eager_fallback(value, pooling_ratio, pseudo_random=False, overlapping=False, deterministic=False, seed=0, seed2=0, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function fractional_avg_pool\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(pooling_ratio, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'pooling_ratio' argument to \"\n",
      "        \"'fractional_avg_pool' Op, not %r.\" % pooling_ratio)\n",
      "  pooling_ratio = [_execute.make_float(_f, \"pooling_ratio\") for _f in pooling_ratio]\n",
      "  if pseudo_random is None:\n",
      "    pseudo_random = False\n",
      "  pseudo_random = _execute.make_bool(pseudo_random, \"pseudo_random\")\n",
      "  if overlapping is None:\n",
      "    overlapping = False\n",
      "  overlapping = _execute.make_bool(overlapping, \"overlapping\")\n",
      "  if deterministic is None:\n",
      "    deterministic = False\n",
      "  deterministic = _execute.make_bool(deterministic, \"deterministic\")\n",
      "  if seed is None:\n",
      "    seed = 0\n",
      "  seed = _execute.make_int(seed, \"seed\")\n",
      "  if seed2 is None:\n",
      "    seed2 = 0\n",
      "  seed2 = _execute.make_int(seed2, \"seed2\")\n",
      "  _attr_T, (value,) = _execute.args_to_matching_eager([value], _ctx)\n",
      "  _inputs_flat = [value]\n",
      "  _attrs = (\"pooling_ratio\", pooling_ratio, \"pseudo_random\", pseudo_random,\n",
      "  \"overlapping\", overlapping, \"deterministic\", deterministic, \"seed\", seed,\n",
      "  \"seed2\", seed2, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"FractionalAvgPool\", 3, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"FractionalAvgPool\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _FractionalAvgPoolOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "def fractional_avg_pool_grad(orig_input_tensor_shape, out_backprop, row_pooling_sequence, col_pooling_sequence, overlapping=False, name=None):\n",
      "  r\"\"\"Computes gradient of the FractionalAvgPool function.\n",
      "\n",
      "  Unlike FractionalMaxPoolGrad, we don't need to find arg_max for\n",
      "  FractionalAvgPoolGrad, we just need to evenly back-propagate each element of\n",
      "  out_backprop to those indices that form the same pooling cell. Therefore, we\n",
      "  just need to know the shape of original input tensor, instead of the whole\n",
      "  tensor.\n",
      "\n",
      "  Args:\n",
      "    orig_input_tensor_shape: A `Tensor` of type `int64`.\n",
      "      Original input tensor shape for `fractional_avg_pool`\n",
      "    out_backprop: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`.\n",
      "      4-D with shape `[batch, height, width, channels]`.  Gradients\n",
      "      w.r.t. the output of `fractional_avg_pool`.\n",
      "    row_pooling_sequence: A `Tensor` of type `int64`.\n",
      "      row pooling sequence, form pooling region with\n",
      "      col_pooling_sequence.\n",
      "    col_pooling_sequence: A `Tensor` of type `int64`.\n",
      "      column pooling sequence, form pooling region with\n",
      "      row_pooling sequence.\n",
      "    overlapping: An optional `bool`. Defaults to `False`.\n",
      "      When set to True, it means when pooling, the values at the boundary\n",
      "      of adjacent pooling cells are used by both cells. For example:\n",
      "\n",
      "      `index  0  1  2  3  4`\n",
      "\n",
      "      `value  20 5  16 3  7`\n",
      "\n",
      "      If the pooling sequence is [0, 2, 4], then 16, at index 2 will be used twice.\n",
      "      The result would be [41/3, 26/3] for fractional avg pooling.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `out_backprop`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if overlapping is None:\n",
      "      overlapping = False\n",
      "    overlapping = _execute.make_bool(overlapping, \"overlapping\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"FractionalAvgPoolGrad\",\n",
      "        orig_input_tensor_shape=orig_input_tensor_shape,\n",
      "        out_backprop=out_backprop, row_pooling_sequence=row_pooling_sequence,\n",
      "        col_pooling_sequence=col_pooling_sequence, overlapping=overlapping,\n",
      "        name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"overlapping\", _op.get_attr(\"overlapping\"), \"T\",\n",
      "              _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"FractionalAvgPoolGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"FractionalAvgPoolGrad\", name, _ctx._post_execution_callbacks,\n",
      "        orig_input_tensor_shape, out_backprop, row_pooling_sequence,\n",
      "        col_pooling_sequence, \"overlapping\", overlapping)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return fractional_avg_pool_grad_eager_fallback(\n",
      "          orig_input_tensor_shape, out_backprop, row_pooling_sequence,\n",
      "          col_pooling_sequence, overlapping=overlapping, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def fractional_avg_pool_grad_eager_fallback(orig_input_tensor_shape, out_backprop, row_pooling_sequence, col_pooling_sequence, overlapping=False, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function fractional_avg_pool_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if overlapping is None:\n",
      "    overlapping = False\n",
      "  overlapping = _execute.make_bool(overlapping, \"overlapping\")\n",
      "  _attr_T, (out_backprop,) = _execute.args_to_matching_eager([out_backprop], _ctx)\n",
      "  orig_input_tensor_shape = _ops.convert_to_tensor(orig_input_tensor_shape, _dtypes.int64)\n",
      "  row_pooling_sequence = _ops.convert_to_tensor(row_pooling_sequence, _dtypes.int64)\n",
      "  col_pooling_sequence = _ops.convert_to_tensor(col_pooling_sequence, _dtypes.int64)\n",
      "  _inputs_flat = [orig_input_tensor_shape, out_backprop, row_pooling_sequence, col_pooling_sequence]\n",
      "  _attrs = (\"overlapping\", overlapping, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"FractionalAvgPoolGrad\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"FractionalAvgPoolGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "_fractional_max_pool_outputs = [\"output\", \"row_pooling_sequence\",\n",
      "                               \"col_pooling_sequence\"]\n",
      "_FractionalMaxPoolOutput = _collections.namedtuple(\n",
      "    \"FractionalMaxPool\", _fractional_max_pool_outputs)\n",
      "\n",
      "\n",
      "@tf_export('nn.fractional_max_pool')\n",
      "def fractional_max_pool(value, pooling_ratio, pseudo_random=False, overlapping=False, deterministic=False, seed=0, seed2=0, name=None):\n",
      "  r\"\"\"Performs fractional max pooling on the input.\n",
      "\n",
      "  Fractional max pooling is slightly different than regular max pooling.  In\n",
      "  regular max pooling, you downsize an input set by taking the maximum value of\n",
      "  smaller N x N subsections of the set (often 2x2), and try to reduce the set by\n",
      "  a factor of N, where N is an integer.  Fractional max pooling, as you might\n",
      "  expect from the word \"fractional\", means that the overall reduction ratio N\n",
      "  does not have to be an integer.\n",
      "\n",
      "  The sizes of the pooling regions are generated randomly but are fairly uniform.\n",
      "  For example, let's look at the height dimension, and the constraints on the\n",
      "  list of rows that will be pool boundaries.\n",
      "\n",
      "  First we define the following:\n",
      "\n",
      "  1.  input_row_length : the number of rows from the input set\n",
      "  2.  output_row_length : which will be smaller than the input\n",
      "  3.  alpha = input_row_length / output_row_length : our reduction ratio\n",
      "  4.  K = floor(alpha)\n",
      "  5.  row_pooling_sequence : this is the result list of pool boundary rows\n",
      "\n",
      "  Then, row_pooling_sequence should satisfy:\n",
      "\n",
      "  1.  a[0] = 0 : the first value of the sequence is 0\n",
      "  2.  a[end] = input_row_length : the last value of the sequence is the size\n",
      "  3.  K <= (a[i+1] - a[i]) <= K+1 : all intervals are K or K+1 size\n",
      "  4.  length(row_pooling_sequence) = output_row_length+1\n",
      "\n",
      "  For more details on fractional max pooling, see this paper:\n",
      "  [Benjamin Graham, Fractional Max-Pooling](http://arxiv.org/abs/1412.6071)\n",
      "\n",
      "  Args:\n",
      "    value: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`.\n",
      "      4-D with shape `[batch, height, width, channels]`.\n",
      "    pooling_ratio: A list of `floats` that has length `>= 4`.\n",
      "      Pooling ratio for each dimension of `value`, currently only\n",
      "      supports row and col dimension and should be >= 1.0. For example, a valid\n",
      "      pooling ratio looks like [1.0, 1.44, 1.73, 1.0]. The first and last elements\n",
      "      must be 1.0 because we don't allow pooling on batch and channels\n",
      "      dimensions. 1.44 and 1.73 are pooling ratio on height and width dimensions\n",
      "      respectively.\n",
      "    pseudo_random: An optional `bool`. Defaults to `False`.\n",
      "      When set to True, generates the pooling sequence in a\n",
      "      pseudorandom fashion, otherwise, in a random fashion. Check paper [Benjamin\n",
      "      Graham, Fractional Max-Pooling](http://arxiv.org/abs/1412.6071) for\n",
      "      difference between pseudorandom and random.\n",
      "    overlapping: An optional `bool`. Defaults to `False`.\n",
      "      When set to True, it means when pooling, the values at the boundary\n",
      "      of adjacent pooling cells are used by both cells. For example:\n",
      "\n",
      "      `index  0  1  2  3  4`\n",
      "\n",
      "      `value  20 5  16 3  7`\n",
      "\n",
      "      If the pooling sequence is [0, 2, 4], then 16, at index 2 will be used twice.\n",
      "      The result would be [20, 16] for fractional max pooling.\n",
      "    deterministic: An optional `bool`. Defaults to `False`.\n",
      "      When set to True, a fixed pooling region will be used when\n",
      "      iterating over a FractionalMaxPool node in the computation graph. Mainly used\n",
      "      in unit test to make FractionalMaxPool deterministic.\n",
      "    seed: An optional `int`. Defaults to `0`.\n",
      "      If either seed or seed2 are set to be non-zero, the random number\n",
      "      generator is seeded by the given seed.  Otherwise, it is seeded by a\n",
      "      random seed.\n",
      "    seed2: An optional `int`. Defaults to `0`.\n",
      "      An second seed to avoid seed collision.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (output, row_pooling_sequence, col_pooling_sequence).\n",
      "\n",
      "    output: A `Tensor`. Has the same type as `value`.\n",
      "    row_pooling_sequence: A `Tensor` of type `int64`.\n",
      "    col_pooling_sequence: A `Tensor` of type `int64`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(pooling_ratio, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'pooling_ratio' argument to \"\n",
      "          \"'fractional_max_pool' Op, not %r.\" % pooling_ratio)\n",
      "    pooling_ratio = [_execute.make_float(_f, \"pooling_ratio\") for _f in pooling_ratio]\n",
      "    if pseudo_random is None:\n",
      "      pseudo_random = False\n",
      "    pseudo_random = _execute.make_bool(pseudo_random, \"pseudo_random\")\n",
      "    if overlapping is None:\n",
      "      overlapping = False\n",
      "    overlapping = _execute.make_bool(overlapping, \"overlapping\")\n",
      "    if deterministic is None:\n",
      "      deterministic = False\n",
      "    deterministic = _execute.make_bool(deterministic, \"deterministic\")\n",
      "    if seed is None:\n",
      "      seed = 0\n",
      "    seed = _execute.make_int(seed, \"seed\")\n",
      "    if seed2 is None:\n",
      "      seed2 = 0\n",
      "    seed2 = _execute.make_int(seed2, \"seed2\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"FractionalMaxPool\", value=value, pooling_ratio=pooling_ratio,\n",
      "        pseudo_random=pseudo_random, overlapping=overlapping,\n",
      "        deterministic=deterministic, seed=seed, seed2=seed2, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"pooling_ratio\", _op.get_attr(\"pooling_ratio\"), \"pseudo_random\",\n",
      "              _op.get_attr(\"pseudo_random\"), \"overlapping\",\n",
      "              _op.get_attr(\"overlapping\"), \"deterministic\",\n",
      "              _op.get_attr(\"deterministic\"), \"seed\", _op.get_attr(\"seed\"),\n",
      "              \"seed2\", _op.get_attr(\"seed2\"), \"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"FractionalMaxPool\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _FractionalMaxPoolOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"FractionalMaxPool\", name, _ctx._post_execution_callbacks, value,\n",
      "        \"pooling_ratio\", pooling_ratio, \"pseudo_random\", pseudo_random,\n",
      "        \"overlapping\", overlapping, \"deterministic\", deterministic, \"seed\",\n",
      "        seed, \"seed2\", seed2)\n",
      "      _result = _FractionalMaxPoolOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return fractional_max_pool_eager_fallback(\n",
      "          value, pooling_ratio=pooling_ratio, pseudo_random=pseudo_random,\n",
      "          overlapping=overlapping, deterministic=deterministic, seed=seed,\n",
      "          seed2=seed2, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def fractional_max_pool_eager_fallback(value, pooling_ratio, pseudo_random=False, overlapping=False, deterministic=False, seed=0, seed2=0, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function fractional_max_pool\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(pooling_ratio, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'pooling_ratio' argument to \"\n",
      "        \"'fractional_max_pool' Op, not %r.\" % pooling_ratio)\n",
      "  pooling_ratio = [_execute.make_float(_f, \"pooling_ratio\") for _f in pooling_ratio]\n",
      "  if pseudo_random is None:\n",
      "    pseudo_random = False\n",
      "  pseudo_random = _execute.make_bool(pseudo_random, \"pseudo_random\")\n",
      "  if overlapping is None:\n",
      "    overlapping = False\n",
      "  overlapping = _execute.make_bool(overlapping, \"overlapping\")\n",
      "  if deterministic is None:\n",
      "    deterministic = False\n",
      "  deterministic = _execute.make_bool(deterministic, \"deterministic\")\n",
      "  if seed is None:\n",
      "    seed = 0\n",
      "  seed = _execute.make_int(seed, \"seed\")\n",
      "  if seed2 is None:\n",
      "    seed2 = 0\n",
      "  seed2 = _execute.make_int(seed2, \"seed2\")\n",
      "  _attr_T, (value,) = _execute.args_to_matching_eager([value], _ctx)\n",
      "  _inputs_flat = [value]\n",
      "  _attrs = (\"pooling_ratio\", pooling_ratio, \"pseudo_random\", pseudo_random,\n",
      "  \"overlapping\", overlapping, \"deterministic\", deterministic, \"seed\", seed,\n",
      "  \"seed2\", seed2, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"FractionalMaxPool\", 3, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"FractionalMaxPool\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _FractionalMaxPoolOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "def fractional_max_pool_grad(orig_input, orig_output, out_backprop, row_pooling_sequence, col_pooling_sequence, overlapping=False, name=None):\n",
      "  r\"\"\"Computes gradient of the FractionalMaxPool function.\n",
      "\n",
      "  Args:\n",
      "    orig_input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`.\n",
      "      Original input for `fractional_max_pool`\n",
      "    orig_output: A `Tensor`. Must have the same type as `orig_input`.\n",
      "      Original output for `fractional_max_pool`\n",
      "    out_backprop: A `Tensor`. Must have the same type as `orig_input`.\n",
      "      4-D with shape `[batch, height, width, channels]`.  Gradients\n",
      "      w.r.t. the output of `fractional_max_pool`.\n",
      "    row_pooling_sequence: A `Tensor` of type `int64`.\n",
      "      row pooling sequence, form pooling region with\n",
      "      col_pooling_sequence.\n",
      "    col_pooling_sequence: A `Tensor` of type `int64`.\n",
      "      column pooling sequence, form pooling region with\n",
      "      row_pooling sequence.\n",
      "    overlapping: An optional `bool`. Defaults to `False`.\n",
      "      When set to True, it means when pooling, the values at the boundary\n",
      "      of adjacent pooling cells are used by both cells. For example:\n",
      "\n",
      "      `index  0  1  2  3  4`\n",
      "\n",
      "      `value  20 5  16 3  7`\n",
      "\n",
      "      If the pooling sequence is [0, 2, 4], then 16, at index 2 will be used twice.\n",
      "      The result would be [20, 16] for fractional max pooling.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `orig_input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if overlapping is None:\n",
      "      overlapping = False\n",
      "    overlapping = _execute.make_bool(overlapping, \"overlapping\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"FractionalMaxPoolGrad\", orig_input=orig_input,\n",
      "        orig_output=orig_output, out_backprop=out_backprop,\n",
      "        row_pooling_sequence=row_pooling_sequence,\n",
      "        col_pooling_sequence=col_pooling_sequence, overlapping=overlapping,\n",
      "        name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"overlapping\", _op.get_attr(\"overlapping\"), \"T\",\n",
      "              _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"FractionalMaxPoolGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"FractionalMaxPoolGrad\", name, _ctx._post_execution_callbacks,\n",
      "        orig_input, orig_output, out_backprop, row_pooling_sequence,\n",
      "        col_pooling_sequence, \"overlapping\", overlapping)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return fractional_max_pool_grad_eager_fallback(\n",
      "          orig_input, orig_output, out_backprop, row_pooling_sequence,\n",
      "          col_pooling_sequence, overlapping=overlapping, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def fractional_max_pool_grad_eager_fallback(orig_input, orig_output, out_backprop, row_pooling_sequence, col_pooling_sequence, overlapping=False, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function fractional_max_pool_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if overlapping is None:\n",
      "    overlapping = False\n",
      "  overlapping = _execute.make_bool(overlapping, \"overlapping\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([orig_input, orig_output, out_backprop], _ctx)\n",
      "  (orig_input, orig_output, out_backprop) = _inputs_T\n",
      "  row_pooling_sequence = _ops.convert_to_tensor(row_pooling_sequence, _dtypes.int64)\n",
      "  col_pooling_sequence = _ops.convert_to_tensor(col_pooling_sequence, _dtypes.int64)\n",
      "  _inputs_flat = [orig_input, orig_output, out_backprop, row_pooling_sequence, col_pooling_sequence]\n",
      "  _attrs = (\"overlapping\", overlapping, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"FractionalMaxPoolGrad\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"FractionalMaxPoolGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "__fused_batch_norm_outputs = [\"y\", \"batch_mean\", \"batch_variance\",\n",
      "                             \"reserve_space_1\", \"reserve_space_2\"]\n",
      "_FusedBatchNormOutput = _collections.namedtuple(\n",
      "    \"FusedBatchNorm\", __fused_batch_norm_outputs)\n",
      "\n",
      "\n",
      "def _fused_batch_norm(x, scale, offset, mean, variance, epsilon=0.0001, data_format=\"NHWC\", is_training=True, name=None):\n",
      "  r\"\"\"Batch normalization.\n",
      "\n",
      "  Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\n",
      "  The size of 1D Tensors matches the dimension C of the 4D Tensors.\n",
      "\n",
      "  Args:\n",
      "    x: A `Tensor`. Must be one of the following types: `float32`.\n",
      "      A 4D Tensor for input data.\n",
      "    scale: A `Tensor`. Must have the same type as `x`.\n",
      "      A 1D Tensor for scaling factor, to scale the normalized x.\n",
      "    offset: A `Tensor`. Must have the same type as `x`.\n",
      "      A 1D Tensor for offset, to shift to the normalized x.\n",
      "    mean: A `Tensor`. Must have the same type as `x`.\n",
      "      A 1D Tensor for population mean. Used for inference only;\n",
      "      must be empty for training.\n",
      "    variance: A `Tensor`. Must have the same type as `x`.\n",
      "      A 1D Tensor for population variance. Used for inference only;\n",
      "      must be empty for training.\n",
      "    epsilon: An optional `float`. Defaults to `0.0001`.\n",
      "      A small float number added to the variance of x.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      The data format for x and y. Either \"NHWC\" (default) or \"NCHW\".\n",
      "    is_training: An optional `bool`. Defaults to `True`.\n",
      "      A bool value to indicate the operation is for training (default)\n",
      "      or inference.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (y, batch_mean, batch_variance, reserve_space_1, reserve_space_2).\n",
      "\n",
      "    y: A `Tensor`. Has the same type as `x`.\n",
      "    batch_mean: A `Tensor`. Has the same type as `x`.\n",
      "    batch_variance: A `Tensor`. Has the same type as `x`.\n",
      "    reserve_space_1: A `Tensor`. Has the same type as `x`.\n",
      "    reserve_space_2: A `Tensor`. Has the same type as `x`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if epsilon is None:\n",
      "      epsilon = 0.0001\n",
      "    epsilon = _execute.make_float(epsilon, \"epsilon\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    if is_training is None:\n",
      "      is_training = True\n",
      "    is_training = _execute.make_bool(is_training, \"is_training\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"FusedBatchNorm\", x=x, scale=scale, offset=offset, mean=mean,\n",
      "        variance=variance, epsilon=epsilon, data_format=data_format,\n",
      "        is_training=is_training, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"epsilon\", _op.get_attr(\"epsilon\"),\n",
      "              \"data_format\", _op.get_attr(\"data_format\"), \"is_training\",\n",
      "              _op.get_attr(\"is_training\"))\n",
      "    _execute.record_gradient(\n",
      "      \"FusedBatchNorm\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _FusedBatchNormOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"FusedBatchNorm\", name, _ctx._post_execution_callbacks, x, scale,\n",
      "        offset, mean, variance, \"epsilon\", epsilon, \"data_format\",\n",
      "        data_format, \"is_training\", is_training)\n",
      "      _result = _FusedBatchNormOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return _fused_batch_norm_eager_fallback(\n",
      "          x, scale, offset, mean, variance, epsilon=epsilon,\n",
      "          data_format=data_format, is_training=is_training, name=name,\n",
      "          ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def _fused_batch_norm_eager_fallback(x, scale, offset, mean, variance, epsilon=0.0001, data_format=\"NHWC\", is_training=True, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function _fused_batch_norm\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if epsilon is None:\n",
      "    epsilon = 0.0001\n",
      "  epsilon = _execute.make_float(epsilon, \"epsilon\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  if is_training is None:\n",
      "    is_training = True\n",
      "  is_training = _execute.make_bool(is_training, \"is_training\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([x, scale, offset, mean, variance], _ctx)\n",
      "  (x, scale, offset, mean, variance) = _inputs_T\n",
      "  _inputs_flat = [x, scale, offset, mean, variance]\n",
      "  _attrs = (\"T\", _attr_T, \"epsilon\", epsilon, \"data_format\", data_format,\n",
      "  \"is_training\", is_training)\n",
      "  _result = _execute.execute(b\"FusedBatchNorm\", 5, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"FusedBatchNorm\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _FusedBatchNormOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "_fused_batch_norm_grad_outputs = [\"x_backprop\", \"scale_backprop\",\n",
      "                                 \"offset_backprop\", \"reserve_space_3\",\n",
      "                                 \"reserve_space_4\"]\n",
      "_FusedBatchNormGradOutput = _collections.namedtuple(\n",
      "    \"FusedBatchNormGrad\", _fused_batch_norm_grad_outputs)\n",
      "\n",
      "\n",
      "def fused_batch_norm_grad(y_backprop, x, scale, reserve_space_1, reserve_space_2, epsilon=0.0001, data_format=\"NHWC\", is_training=True, name=None):\n",
      "  r\"\"\"Gradient for batch normalization.\n",
      "\n",
      "  Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\n",
      "  The size of 1D Tensors matches the dimension C of the 4D Tensors.\n",
      "\n",
      "  Args:\n",
      "    y_backprop: A `Tensor`. Must be one of the following types: `float32`.\n",
      "      A 4D Tensor for the gradient with respect to y.\n",
      "    x: A `Tensor`. Must have the same type as `y_backprop`.\n",
      "      A 4D Tensor for input data.\n",
      "    scale: A `Tensor`. Must have the same type as `y_backprop`.\n",
      "      A 1D Tensor for scaling factor, to scale the normalized x.\n",
      "    reserve_space_1: A `Tensor`. Must have the same type as `y_backprop`.\n",
      "      When is_training is True, a 1D Tensor for the computed batch\n",
      "      mean to be reused in gradient computation. When is_training is\n",
      "      False, a 1D Tensor for the population mean to be reused in both\n",
      "      1st and 2nd order gradient computation.\n",
      "    reserve_space_2: A `Tensor`. Must have the same type as `y_backprop`.\n",
      "      When is_training is True, a 1D Tensor for the computed batch\n",
      "      variance (inverted variance in the cuDNN case) to be reused in\n",
      "      gradient computation. When is_training is False, a 1D Tensor\n",
      "      for the population variance to be reused in both 1st and 2nd\n",
      "      order gradient computation.\n",
      "    epsilon: An optional `float`. Defaults to `0.0001`.\n",
      "      A small float number added to the variance of x.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      The data format for y_backprop, x, x_backprop.\n",
      "      Either \"NHWC\" (default) or \"NCHW\".\n",
      "    is_training: An optional `bool`. Defaults to `True`.\n",
      "      A bool value to indicate the operation is for training (default)\n",
      "      or inference.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (x_backprop, scale_backprop, offset_backprop, reserve_space_3, reserve_space_4).\n",
      "\n",
      "    x_backprop: A `Tensor`. Has the same type as `y_backprop`.\n",
      "    scale_backprop: A `Tensor`. Has the same type as `y_backprop`.\n",
      "    offset_backprop: A `Tensor`. Has the same type as `y_backprop`.\n",
      "    reserve_space_3: A `Tensor`. Has the same type as `y_backprop`.\n",
      "    reserve_space_4: A `Tensor`. Has the same type as `y_backprop`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if epsilon is None:\n",
      "      epsilon = 0.0001\n",
      "    epsilon = _execute.make_float(epsilon, \"epsilon\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    if is_training is None:\n",
      "      is_training = True\n",
      "    is_training = _execute.make_bool(is_training, \"is_training\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"FusedBatchNormGrad\", y_backprop=y_backprop, x=x, scale=scale,\n",
      "        reserve_space_1=reserve_space_1, reserve_space_2=reserve_space_2,\n",
      "        epsilon=epsilon, data_format=data_format, is_training=is_training,\n",
      "        name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"epsilon\", _op.get_attr(\"epsilon\"),\n",
      "              \"data_format\", _op.get_attr(\"data_format\"), \"is_training\",\n",
      "              _op.get_attr(\"is_training\"))\n",
      "    _execute.record_gradient(\n",
      "      \"FusedBatchNormGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _FusedBatchNormGradOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"FusedBatchNormGrad\", name, _ctx._post_execution_callbacks,\n",
      "        y_backprop, x, scale, reserve_space_1, reserve_space_2, \"epsilon\",\n",
      "        epsilon, \"data_format\", data_format, \"is_training\", is_training)\n",
      "      _result = _FusedBatchNormGradOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return fused_batch_norm_grad_eager_fallback(\n",
      "          y_backprop, x, scale, reserve_space_1, reserve_space_2,\n",
      "          epsilon=epsilon, data_format=data_format, is_training=is_training,\n",
      "          name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def fused_batch_norm_grad_eager_fallback(y_backprop, x, scale, reserve_space_1, reserve_space_2, epsilon=0.0001, data_format=\"NHWC\", is_training=True, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function fused_batch_norm_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if epsilon is None:\n",
      "    epsilon = 0.0001\n",
      "  epsilon = _execute.make_float(epsilon, \"epsilon\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  if is_training is None:\n",
      "    is_training = True\n",
      "  is_training = _execute.make_bool(is_training, \"is_training\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([y_backprop, x, scale, reserve_space_1, reserve_space_2], _ctx)\n",
      "  (y_backprop, x, scale, reserve_space_1, reserve_space_2) = _inputs_T\n",
      "  _inputs_flat = [y_backprop, x, scale, reserve_space_1, reserve_space_2]\n",
      "  _attrs = (\"T\", _attr_T, \"epsilon\", epsilon, \"data_format\", data_format,\n",
      "  \"is_training\", is_training)\n",
      "  _result = _execute.execute(b\"FusedBatchNormGrad\", 5, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"FusedBatchNormGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _FusedBatchNormGradOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "_fused_batch_norm_grad_v2_outputs = [\"x_backprop\", \"scale_backprop\",\n",
      "                                    \"offset_backprop\", \"reserve_space_3\",\n",
      "                                    \"reserve_space_4\"]\n",
      "_FusedBatchNormGradV2Output = _collections.namedtuple(\n",
      "    \"FusedBatchNormGradV2\", _fused_batch_norm_grad_v2_outputs)\n",
      "\n",
      "\n",
      "def fused_batch_norm_grad_v2(y_backprop, x, scale, reserve_space_1, reserve_space_2, epsilon=0.0001, data_format=\"NHWC\", is_training=True, name=None):\n",
      "  r\"\"\"Gradient for batch normalization.\n",
      "\n",
      "  Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\n",
      "  The size of 1D Tensors matches the dimension C of the 4D Tensors.\n",
      "\n",
      "  Args:\n",
      "    y_backprop: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`.\n",
      "      A 4D Tensor for the gradient with respect to y.\n",
      "    x: A `Tensor`. Must have the same type as `y_backprop`.\n",
      "      A 4D Tensor for input data.\n",
      "    scale: A `Tensor` of type `float32`.\n",
      "      A 1D Tensor for scaling factor, to scale the normalized x.\n",
      "    reserve_space_1: A `Tensor`. Must be one of the following types: `float32`.\n",
      "      When is_training is True, a 1D Tensor for the computed batch\n",
      "      mean to be reused in gradient computation. When is_training is\n",
      "      False, a 1D Tensor for the population mean to be reused in both\n",
      "      1st and 2nd order gradient computation.\n",
      "    reserve_space_2: A `Tensor`. Must have the same type as `reserve_space_1`.\n",
      "      When is_training is True, a 1D Tensor for the computed batch\n",
      "      variance (inverted variance in the cuDNN case) to be reused in\n",
      "      gradient computation. When is_training is False, a 1D Tensor\n",
      "      for the population variance to be reused in both 1st and 2nd\n",
      "      order gradient computation.\n",
      "    epsilon: An optional `float`. Defaults to `0.0001`.\n",
      "      A small float number added to the variance of x.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      The data format for y_backprop, x, x_backprop.\n",
      "      Either \"NHWC\" (default) or \"NCHW\".\n",
      "    is_training: An optional `bool`. Defaults to `True`.\n",
      "      A bool value to indicate the operation is for training (default)\n",
      "      or inference.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (x_backprop, scale_backprop, offset_backprop, reserve_space_3, reserve_space_4).\n",
      "\n",
      "    x_backprop: A `Tensor`. Has the same type as `y_backprop`.\n",
      "    scale_backprop: A `Tensor`. Has the same type as `reserve_space_1`.\n",
      "    offset_backprop: A `Tensor`. Has the same type as `reserve_space_1`.\n",
      "    reserve_space_3: A `Tensor`. Has the same type as `reserve_space_1`.\n",
      "    reserve_space_4: A `Tensor`. Has the same type as `reserve_space_1`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if epsilon is None:\n",
      "      epsilon = 0.0001\n",
      "    epsilon = _execute.make_float(epsilon, \"epsilon\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    if is_training is None:\n",
      "      is_training = True\n",
      "    is_training = _execute.make_bool(is_training, \"is_training\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"FusedBatchNormGradV2\", y_backprop=y_backprop, x=x, scale=scale,\n",
      "        reserve_space_1=reserve_space_1, reserve_space_2=reserve_space_2,\n",
      "        epsilon=epsilon, data_format=data_format, is_training=is_training,\n",
      "        name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"U\", _op.get_attr(\"U\"), \"epsilon\",\n",
      "              _op.get_attr(\"epsilon\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"), \"is_training\",\n",
      "              _op.get_attr(\"is_training\"))\n",
      "    _execute.record_gradient(\n",
      "      \"FusedBatchNormGradV2\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _FusedBatchNormGradV2Output._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"FusedBatchNormGradV2\", name, _ctx._post_execution_callbacks,\n",
      "        y_backprop, x, scale, reserve_space_1, reserve_space_2, \"epsilon\",\n",
      "        epsilon, \"data_format\", data_format, \"is_training\", is_training)\n",
      "      _result = _FusedBatchNormGradV2Output._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return fused_batch_norm_grad_v2_eager_fallback(\n",
      "          y_backprop, x, scale, reserve_space_1, reserve_space_2,\n",
      "          epsilon=epsilon, data_format=data_format, is_training=is_training,\n",
      "          name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def fused_batch_norm_grad_v2_eager_fallback(y_backprop, x, scale, reserve_space_1, reserve_space_2, epsilon=0.0001, data_format=\"NHWC\", is_training=True, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function fused_batch_norm_grad_v2\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if epsilon is None:\n",
      "    epsilon = 0.0001\n",
      "  epsilon = _execute.make_float(epsilon, \"epsilon\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  if is_training is None:\n",
      "    is_training = True\n",
      "  is_training = _execute.make_bool(is_training, \"is_training\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([y_backprop, x], _ctx)\n",
      "  (y_backprop, x) = _inputs_T\n",
      "  _attr_U, _inputs_U = _execute.args_to_matching_eager([reserve_space_1, reserve_space_2], _ctx)\n",
      "  (reserve_space_1, reserve_space_2) = _inputs_U\n",
      "  scale = _ops.convert_to_tensor(scale, _dtypes.float32)\n",
      "  _inputs_flat = [y_backprop, x, scale, reserve_space_1, reserve_space_2]\n",
      "  _attrs = (\"T\", _attr_T, \"U\", _attr_U, \"epsilon\", epsilon, \"data_format\",\n",
      "  data_format, \"is_training\", is_training)\n",
      "  _result = _execute.execute(b\"FusedBatchNormGradV2\", 5, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"FusedBatchNormGradV2\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _FusedBatchNormGradV2Output._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "_fused_batch_norm_v2_outputs = [\"y\", \"batch_mean\", \"batch_variance\",\n",
      "                               \"reserve_space_1\", \"reserve_space_2\"]\n",
      "_FusedBatchNormV2Output = _collections.namedtuple(\n",
      "    \"FusedBatchNormV2\", _fused_batch_norm_v2_outputs)\n",
      "\n",
      "\n",
      "def fused_batch_norm_v2(x, scale, offset, mean, variance, epsilon=0.0001, data_format=\"NHWC\", is_training=True, name=None):\n",
      "  r\"\"\"Batch normalization.\n",
      "\n",
      "  Note that the size of 4D Tensors are defined by either \"NHWC\" or \"NCHW\".\n",
      "  The size of 1D Tensors matches the dimension C of the 4D Tensors.\n",
      "\n",
      "  Args:\n",
      "    x: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`.\n",
      "      A 4D Tensor for input data.\n",
      "    scale: A `Tensor`. Must be one of the following types: `float32`.\n",
      "      A 1D Tensor for scaling factor, to scale the normalized x.\n",
      "    offset: A `Tensor`. Must have the same type as `scale`.\n",
      "      A 1D Tensor for offset, to shift to the normalized x.\n",
      "    mean: A `Tensor`. Must have the same type as `scale`.\n",
      "      A 1D Tensor for population mean. Used for inference only;\n",
      "      must be empty for training.\n",
      "    variance: A `Tensor`. Must have the same type as `scale`.\n",
      "      A 1D Tensor for population variance. Used for inference only;\n",
      "      must be empty for training.\n",
      "    epsilon: An optional `float`. Defaults to `0.0001`.\n",
      "      A small float number added to the variance of x.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      The data format for x and y. Either \"NHWC\" (default) or \"NCHW\".\n",
      "    is_training: An optional `bool`. Defaults to `True`.\n",
      "      A bool value to indicate the operation is for training (default)\n",
      "      or inference.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (y, batch_mean, batch_variance, reserve_space_1, reserve_space_2).\n",
      "\n",
      "    y: A `Tensor`. Has the same type as `x`.\n",
      "    batch_mean: A `Tensor`. Has the same type as `scale`.\n",
      "    batch_variance: A `Tensor`. Has the same type as `scale`.\n",
      "    reserve_space_1: A `Tensor`. Has the same type as `scale`.\n",
      "    reserve_space_2: A `Tensor`. Has the same type as `scale`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if epsilon is None:\n",
      "      epsilon = 0.0001\n",
      "    epsilon = _execute.make_float(epsilon, \"epsilon\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    if is_training is None:\n",
      "      is_training = True\n",
      "    is_training = _execute.make_bool(is_training, \"is_training\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"FusedBatchNormV2\", x=x, scale=scale, offset=offset, mean=mean,\n",
      "        variance=variance, epsilon=epsilon, data_format=data_format,\n",
      "        is_training=is_training, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"U\", _op.get_attr(\"U\"), \"epsilon\",\n",
      "              _op.get_attr(\"epsilon\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"), \"is_training\",\n",
      "              _op.get_attr(\"is_training\"))\n",
      "    _execute.record_gradient(\n",
      "      \"FusedBatchNormV2\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _FusedBatchNormV2Output._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"FusedBatchNormV2\", name, _ctx._post_execution_callbacks, x, scale,\n",
      "        offset, mean, variance, \"epsilon\", epsilon, \"data_format\",\n",
      "        data_format, \"is_training\", is_training)\n",
      "      _result = _FusedBatchNormV2Output._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return fused_batch_norm_v2_eager_fallback(\n",
      "          x, scale, offset, mean, variance, epsilon=epsilon,\n",
      "          data_format=data_format, is_training=is_training, name=name,\n",
      "          ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def fused_batch_norm_v2_eager_fallback(x, scale, offset, mean, variance, epsilon=0.0001, data_format=\"NHWC\", is_training=True, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function fused_batch_norm_v2\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if epsilon is None:\n",
      "    epsilon = 0.0001\n",
      "  epsilon = _execute.make_float(epsilon, \"epsilon\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  if is_training is None:\n",
      "    is_training = True\n",
      "  is_training = _execute.make_bool(is_training, \"is_training\")\n",
      "  _attr_T, (x,) = _execute.args_to_matching_eager([x], _ctx)\n",
      "  _attr_U, _inputs_U = _execute.args_to_matching_eager([scale, offset, mean, variance], _ctx)\n",
      "  (scale, offset, mean, variance) = _inputs_U\n",
      "  _inputs_flat = [x, scale, offset, mean, variance]\n",
      "  _attrs = (\"T\", _attr_T, \"U\", _attr_U, \"epsilon\", epsilon, \"data_format\",\n",
      "  data_format, \"is_training\", is_training)\n",
      "  _result = _execute.execute(b\"FusedBatchNormV2\", 5, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"FusedBatchNormV2\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _FusedBatchNormV2Output._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "def fused_pad_conv2d(input, paddings, filter, mode, strides, padding, name=None):\n",
      "  r\"\"\"Performs a padding as a preprocess during a convolution.\n",
      "\n",
      "  Similar to FusedResizeAndPadConv2d, this op allows for an optimized\n",
      "  implementation where the spatial padding transformation stage is fused with the\n",
      "  im2col lookup, but in this case without the bilinear filtering required for\n",
      "  resizing. Fusing the padding prevents the need to write out the intermediate\n",
      "  results as whole tensors, reducing memory pressure, and we can get some latency\n",
      "  gains by merging the transformation calculations.\n",
      "  The data_format attribute for Conv2D isn't supported by this op, and 'NHWC'\n",
      "  order is used instead.\n",
      "  Internally this op uses a single per-graph scratch buffer, which means that it\n",
      "  will block if multiple versions are being run in parallel. This is because this\n",
      "  operator is primarily an optimization to minimize memory usage.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.\n",
      "      4-D with shape `[batch, in_height, in_width, in_channels]`.\n",
      "    paddings: A `Tensor` of type `int32`.\n",
      "      A two-column matrix specifying the padding sizes. The number of\n",
      "      rows must be the same as the rank of `input`.\n",
      "    filter: A `Tensor`. Must have the same type as `input`. 4-D with shape\n",
      "      `[filter_height, filter_width, in_channels, out_channels]`.\n",
      "    mode: A `string` from: `\"REFLECT\", \"SYMMETRIC\"`.\n",
      "    strides: A list of `ints`.\n",
      "      1-D of length 4.  The stride of the sliding window for each dimension\n",
      "      of `input`. Must be in the same order as the dimension specified with format.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    mode = _execute.make_str(mode, \"mode\")\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'fused_pad_conv2d' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"FusedPadConv2D\", input=input, paddings=paddings, filter=filter,\n",
      "        mode=mode, strides=strides, padding=padding, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"mode\", _op.get_attr(\"mode\"), \"strides\",\n",
      "              _op.get_attr(\"strides\"), \"padding\", _op.get_attr(\"padding\"))\n",
      "    _execute.record_gradient(\n",
      "      \"FusedPadConv2D\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"FusedPadConv2D\", name, _ctx._post_execution_callbacks, input,\n",
      "        paddings, filter, \"mode\", mode, \"strides\", strides, \"padding\",\n",
      "        padding)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return fused_pad_conv2d_eager_fallback(\n",
      "          input, paddings, filter, mode=mode, strides=strides,\n",
      "          padding=padding, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def fused_pad_conv2d_eager_fallback(input, paddings, filter, mode, strides, padding, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function fused_pad_conv2d\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  mode = _execute.make_str(mode, \"mode\")\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'fused_pad_conv2d' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter], _ctx)\n",
      "  (input, filter) = _inputs_T\n",
      "  paddings = _ops.convert_to_tensor(paddings, _dtypes.int32)\n",
      "  _inputs_flat = [input, paddings, filter]\n",
      "  _attrs = (\"T\", _attr_T, \"mode\", mode, \"strides\", strides, \"padding\",\n",
      "  padding)\n",
      "  _result = _execute.execute(b\"FusedPadConv2D\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"FusedPadConv2D\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def fused_resize_and_pad_conv2d(input, size, paddings, filter, mode, strides, padding, resize_align_corners=False, name=None):\n",
      "  r\"\"\"Performs a resize and padding as a preprocess during a convolution.\n",
      "\n",
      "  It's often possible to do spatial transformations more efficiently as part of\n",
      "  the packing stage of a convolution, so this op allows for an optimized\n",
      "  implementation where these stages are fused together. This prevents the need to\n",
      "  write out the intermediate results as whole tensors, reducing memory pressure,\n",
      "  and we can get some latency gains by merging the transformation calculations.\n",
      "  The data_format attribute for Conv2D isn't supported by this op, and defaults to\n",
      "  'NHWC' order.\n",
      "  Internally this op uses a single per-graph scratch buffer, which means that it\n",
      "  will block if multiple versions are being run in parallel. This is because this\n",
      "  operator is primarily an optimization to minimize memory usage.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`.\n",
      "      4-D with shape `[batch, in_height, in_width, in_channels]`.\n",
      "    size: A `Tensor` of type `int32`.\n",
      "      A 1-D int32 Tensor of 2 elements: `new_height, new_width`.  The\n",
      "      new size for the images.\n",
      "    paddings: A `Tensor` of type `int32`.\n",
      "      A two-column matrix specifying the padding sizes. The number of\n",
      "      rows must be the same as the rank of `input`.\n",
      "    filter: A `Tensor`. Must have the same type as `input`. 4-D with shape\n",
      "      `[filter_height, filter_width, in_channels, out_channels]`.\n",
      "    mode: A `string` from: `\"REFLECT\", \"SYMMETRIC\"`.\n",
      "    strides: A list of `ints`.\n",
      "      1-D of length 4.  The stride of the sliding window for each dimension\n",
      "      of `input`. Must be in the same order as the dimension specified with format.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    resize_align_corners: An optional `bool`. Defaults to `False`.\n",
      "      If true, the centers of the 4 corner pixels of the input and output tensors are\n",
      "      aligned, preserving the values at the corner pixels. Defaults to false.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    mode = _execute.make_str(mode, \"mode\")\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'fused_resize_and_pad_conv2d' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if resize_align_corners is None:\n",
      "      resize_align_corners = False\n",
      "    resize_align_corners = _execute.make_bool(resize_align_corners, \"resize_align_corners\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"FusedResizeAndPadConv2D\", input=input, size=size, paddings=paddings,\n",
      "        filter=filter, mode=mode, strides=strides, padding=padding,\n",
      "        resize_align_corners=resize_align_corners, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"resize_align_corners\",\n",
      "              _op.get_attr(\"resize_align_corners\"), \"mode\",\n",
      "              _op.get_attr(\"mode\"), \"strides\", _op.get_attr(\"strides\"),\n",
      "              \"padding\", _op.get_attr(\"padding\"))\n",
      "    _execute.record_gradient(\n",
      "      \"FusedResizeAndPadConv2D\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"FusedResizeAndPadConv2D\", name, _ctx._post_execution_callbacks,\n",
      "        input, size, paddings, filter, \"resize_align_corners\",\n",
      "        resize_align_corners, \"mode\", mode, \"strides\", strides, \"padding\",\n",
      "        padding)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return fused_resize_and_pad_conv2d_eager_fallback(\n",
      "          input, size, paddings, filter,\n",
      "          resize_align_corners=resize_align_corners, mode=mode,\n",
      "          strides=strides, padding=padding, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def fused_resize_and_pad_conv2d_eager_fallback(input, size, paddings, filter, mode, strides, padding, resize_align_corners=False, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function fused_resize_and_pad_conv2d\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  mode = _execute.make_str(mode, \"mode\")\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'fused_resize_and_pad_conv2d' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if resize_align_corners is None:\n",
      "    resize_align_corners = False\n",
      "  resize_align_corners = _execute.make_bool(resize_align_corners, \"resize_align_corners\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter], _ctx)\n",
      "  (input, filter) = _inputs_T\n",
      "  size = _ops.convert_to_tensor(size, _dtypes.int32)\n",
      "  paddings = _ops.convert_to_tensor(paddings, _dtypes.int32)\n",
      "  _inputs_flat = [input, size, paddings, filter]\n",
      "  _attrs = (\"T\", _attr_T, \"resize_align_corners\", resize_align_corners,\n",
      "  \"mode\", mode, \"strides\", strides, \"padding\", padding)\n",
      "  _result = _execute.execute(b\"FusedResizeAndPadConv2D\", 1,\n",
      "                             inputs=_inputs_flat, attrs=_attrs, ctx=_ctx,\n",
      "                             name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"FusedResizeAndPadConv2D\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def in_top_k(predictions, targets, k, name=None):\n",
      "  r\"\"\"Says whether the targets are in the top `K` predictions.\n",
      "\n",
      "  This outputs a `batch_size` bool array, an entry `out[i]` is `true` if the\n",
      "  prediction for the target class is among the top `k` predictions among\n",
      "  all predictions for example `i`. Note that the behavior of `InTopK` differs\n",
      "  from the `TopK` op in its handling of ties; if multiple classes have the\n",
      "  same prediction value and straddle the top-`k` boundary, all of those\n",
      "  classes are considered to be in the top `k`.\n",
      "\n",
      "  More formally, let\n",
      "\n",
      "    \\\\(predictions_i\\\\) be the predictions for all classes for example `i`,\n",
      "    \\\\(targets_i\\\\) be the target class for example `i`,\n",
      "    \\\\(out_i\\\\) be the output for example `i`,\n",
      "\n",
      "  $$out_i = predictions_{i, targets_i} \\in TopKIncludingTies(predictions_i)$$\n",
      "\n",
      "  Args:\n",
      "    predictions: A `Tensor` of type `float32`.\n",
      "      A `batch_size` x `classes` tensor.\n",
      "    targets: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "      A `batch_size` vector of class ids.\n",
      "    k: An `int`. Number of top elements to look at for computing precision.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor` of type `bool`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    k = _execute.make_int(k, \"k\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"InTopK\", predictions=predictions, targets=targets, k=k, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"k\", _op.get_attr(\"k\"), \"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"InTopK\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"InTopK\", name,\n",
      "        _ctx._post_execution_callbacks, predictions, targets, \"k\", k)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return in_top_k_eager_fallback(\n",
      "          predictions, targets, k=k, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def in_top_k_eager_fallback(predictions, targets, k, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function in_top_k\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  k = _execute.make_int(k, \"k\")\n",
      "  _attr_T, (targets,) = _execute.args_to_matching_eager([targets], _ctx, _dtypes.int32)\n",
      "  predictions = _ops.convert_to_tensor(predictions, _dtypes.float32)\n",
      "  _inputs_flat = [predictions, targets]\n",
      "  _attrs = (\"k\", k, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"InTopK\", 1, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"InTopK\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def in_top_kv2(predictions, targets, k, name=None):\n",
      "  r\"\"\"Says whether the targets are in the top `K` predictions.\n",
      "\n",
      "  This outputs a `batch_size` bool array, an entry `out[i]` is `true` if the\n",
      "  prediction for the target class is among the top `k` predictions among\n",
      "  all predictions for example `i`. Note that the behavior of `InTopK` differs\n",
      "  from the `TopK` op in its handling of ties; if multiple classes have the\n",
      "  same prediction value and straddle the top-`k` boundary, all of those\n",
      "  classes are considered to be in the top `k`.\n",
      "\n",
      "  More formally, let\n",
      "\n",
      "    \\\\(predictions_i\\\\) be the predictions for all classes for example `i`,\n",
      "    \\\\(targets_i\\\\) be the target class for example `i`,\n",
      "    \\\\(out_i\\\\) be the output for example `i`,\n",
      "\n",
      "  $$out_i = predictions_{i, targets_i} \\in TopKIncludingTies(predictions_i)$$\n",
      "\n",
      "  Args:\n",
      "    predictions: A `Tensor` of type `float32`.\n",
      "      A `batch_size` x `classes` tensor.\n",
      "    targets: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "      A `batch_size` vector of class ids.\n",
      "    k: A `Tensor`. Must have the same type as `targets`.\n",
      "      Number of top elements to look at for computing precision.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor` of type `bool`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"InTopKV2\", predictions=predictions, targets=targets, k=k, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"InTopKV2\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"InTopKV2\",\n",
      "        name, _ctx._post_execution_callbacks, predictions, targets, k)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return in_top_kv2_eager_fallback(\n",
      "          predictions, targets, k, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def in_top_kv2_eager_fallback(predictions, targets, k, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function in_top_kv2\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([targets, k], _ctx, _dtypes.int32)\n",
      "  (targets, k) = _inputs_T\n",
      "  predictions = _ops.convert_to_tensor(predictions, _dtypes.float32)\n",
      "  _inputs_flat = [predictions, targets, k]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"InTopKV2\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"InTopKV2\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.l2_loss')\n",
      "def l2_loss(t, name=None):\n",
      "  r\"\"\"L2 Loss.\n",
      "\n",
      "  Computes half the L2 norm of a tensor without the `sqrt`:\n",
      "\n",
      "      output = sum(t ** 2) / 2\n",
      "\n",
      "  Args:\n",
      "    t: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      Typically 2-D, but may have any dimensions.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `t`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"L2Loss\", t=t, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"L2Loss\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"L2Loss\", name,\n",
      "        _ctx._post_execution_callbacks, t)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return l2_loss_eager_fallback(\n",
      "          t, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def l2_loss_eager_fallback(t, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function l2_loss\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, (t,) = _execute.args_to_matching_eager([t], _ctx)\n",
      "  _inputs_flat = [t]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"L2Loss\", 1, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"L2Loss\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.local_response_normalization', 'nn.lrn')\n",
      "def lrn(input, depth_radius=5, bias=1, alpha=1, beta=0.5, name=None):\n",
      "  r\"\"\"Local Response Normalization.\n",
      "\n",
      "  The 4-D `input` tensor is treated as a 3-D array of 1-D vectors (along the last\n",
      "  dimension), and each vector is normalized independently.  Within a given vector,\n",
      "  each component is divided by the weighted, squared sum of inputs within\n",
      "  `depth_radius`.  In detail,\n",
      "\n",
      "      sqr_sum[a, b, c, d] =\n",
      "          sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)\n",
      "      output = input / (bias + alpha * sqr_sum) ** beta\n",
      "\n",
      "  For details, see [Krizhevsky et al., ImageNet classification with deep\n",
      "  convolutional neural networks (NIPS 2012)](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks).\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`.\n",
      "      4-D.\n",
      "    depth_radius: An optional `int`. Defaults to `5`.\n",
      "      0-D.  Half-width of the 1-D normalization window.\n",
      "    bias: An optional `float`. Defaults to `1`.\n",
      "      An offset (usually positive to avoid dividing by 0).\n",
      "    alpha: An optional `float`. Defaults to `1`.\n",
      "      A scale factor, usually positive.\n",
      "    beta: An optional `float`. Defaults to `0.5`. An exponent.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if depth_radius is None:\n",
      "      depth_radius = 5\n",
      "    depth_radius = _execute.make_int(depth_radius, \"depth_radius\")\n",
      "    if bias is None:\n",
      "      bias = 1\n",
      "    bias = _execute.make_float(bias, \"bias\")\n",
      "    if alpha is None:\n",
      "      alpha = 1\n",
      "    alpha = _execute.make_float(alpha, \"alpha\")\n",
      "    if beta is None:\n",
      "      beta = 0.5\n",
      "    beta = _execute.make_float(beta, \"beta\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"LRN\", input=input, depth_radius=depth_radius, bias=bias, alpha=alpha,\n",
      "        beta=beta, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"depth_radius\", _op.get_attr(\"depth_radius\"), \"bias\",\n",
      "              _op.get_attr(\"bias\"), \"alpha\", _op.get_attr(\"alpha\"), \"beta\",\n",
      "              _op.get_attr(\"beta\"), \"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"LRN\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"LRN\", name,\n",
      "        _ctx._post_execution_callbacks, input, \"depth_radius\", depth_radius,\n",
      "        \"bias\", bias, \"alpha\", alpha, \"beta\", beta)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return lrn_eager_fallback(\n",
      "          input, depth_radius=depth_radius, bias=bias, alpha=alpha, beta=beta,\n",
      "          name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def lrn_eager_fallback(input, depth_radius=5, bias=1, alpha=1, beta=0.5, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function lrn\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if depth_radius is None:\n",
      "    depth_radius = 5\n",
      "  depth_radius = _execute.make_int(depth_radius, \"depth_radius\")\n",
      "  if bias is None:\n",
      "    bias = 1\n",
      "  bias = _execute.make_float(bias, \"bias\")\n",
      "  if alpha is None:\n",
      "    alpha = 1\n",
      "  alpha = _execute.make_float(alpha, \"alpha\")\n",
      "  if beta is None:\n",
      "    beta = 0.5\n",
      "  beta = _execute.make_float(beta, \"beta\")\n",
      "  _attr_T, (input,) = _execute.args_to_matching_eager([input], _ctx, _dtypes.float32)\n",
      "  _inputs_flat = [input]\n",
      "  _attrs = (\"depth_radius\", depth_radius, \"bias\", bias, \"alpha\", alpha,\n",
      "  \"beta\", beta, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"LRN\", 1, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"LRN\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def lrn_grad(input_grads, input_image, output_image, depth_radius=5, bias=1, alpha=1, beta=0.5, name=None):\n",
      "  r\"\"\"Gradients for Local Response Normalization.\n",
      "\n",
      "  Args:\n",
      "    input_grads: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`.\n",
      "      4-D with shape `[batch, height, width, channels]`.\n",
      "    input_image: A `Tensor`. Must have the same type as `input_grads`.\n",
      "      4-D with shape `[batch, height, width, channels]`.\n",
      "    output_image: A `Tensor`. Must have the same type as `input_grads`.\n",
      "      4-D with shape `[batch, height, width, channels]`.\n",
      "    depth_radius: An optional `int`. Defaults to `5`. A depth radius.\n",
      "    bias: An optional `float`. Defaults to `1`.\n",
      "      An offset (usually > 0 to avoid dividing by 0).\n",
      "    alpha: An optional `float`. Defaults to `1`.\n",
      "      A scale factor, usually positive.\n",
      "    beta: An optional `float`. Defaults to `0.5`. An exponent.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input_grads`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if depth_radius is None:\n",
      "      depth_radius = 5\n",
      "    depth_radius = _execute.make_int(depth_radius, \"depth_radius\")\n",
      "    if bias is None:\n",
      "      bias = 1\n",
      "    bias = _execute.make_float(bias, \"bias\")\n",
      "    if alpha is None:\n",
      "      alpha = 1\n",
      "    alpha = _execute.make_float(alpha, \"alpha\")\n",
      "    if beta is None:\n",
      "      beta = 0.5\n",
      "    beta = _execute.make_float(beta, \"beta\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"LRNGrad\", input_grads=input_grads, input_image=input_image,\n",
      "        output_image=output_image, depth_radius=depth_radius, bias=bias,\n",
      "        alpha=alpha, beta=beta, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"depth_radius\", _op.get_attr(\"depth_radius\"), \"bias\",\n",
      "              _op.get_attr(\"bias\"), \"alpha\", _op.get_attr(\"alpha\"), \"beta\",\n",
      "              _op.get_attr(\"beta\"), \"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"LRNGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"LRNGrad\",\n",
      "        name, _ctx._post_execution_callbacks, input_grads, input_image,\n",
      "        output_image, \"depth_radius\", depth_radius, \"bias\", bias, \"alpha\",\n",
      "        alpha, \"beta\", beta)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return lrn_grad_eager_fallback(\n",
      "          input_grads, input_image, output_image, depth_radius=depth_radius,\n",
      "          bias=bias, alpha=alpha, beta=beta, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def lrn_grad_eager_fallback(input_grads, input_image, output_image, depth_radius=5, bias=1, alpha=1, beta=0.5, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function lrn_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if depth_radius is None:\n",
      "    depth_radius = 5\n",
      "  depth_radius = _execute.make_int(depth_radius, \"depth_radius\")\n",
      "  if bias is None:\n",
      "    bias = 1\n",
      "  bias = _execute.make_float(bias, \"bias\")\n",
      "  if alpha is None:\n",
      "    alpha = 1\n",
      "  alpha = _execute.make_float(alpha, \"alpha\")\n",
      "  if beta is None:\n",
      "    beta = 0.5\n",
      "  beta = _execute.make_float(beta, \"beta\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input_grads, input_image, output_image], _ctx, _dtypes.float32)\n",
      "  (input_grads, input_image, output_image) = _inputs_T\n",
      "  _inputs_flat = [input_grads, input_image, output_image]\n",
      "  _attrs = (\"depth_radius\", depth_radius, \"bias\", bias, \"alpha\", alpha,\n",
      "  \"beta\", beta, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"LRNGrad\", 1, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"LRNGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def log_softmax(logits, name=None):\n",
      "  r\"\"\"Computes log softmax activations.\n",
      "\n",
      "  For each batch `i` and class `j` we have\n",
      "\n",
      "      logsoftmax[i, j] = logits[i, j] - log(sum(exp(logits[i])))\n",
      "\n",
      "  Args:\n",
      "    logits: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      2-D with shape `[batch_size, num_classes]`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `logits`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"LogSoftmax\", logits=logits, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"LogSoftmax\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"LogSoftmax\",\n",
      "        name, _ctx._post_execution_callbacks, logits)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return log_softmax_eager_fallback(\n",
      "          logits, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def log_softmax_eager_fallback(logits, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function log_softmax\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, (logits,) = _execute.args_to_matching_eager([logits], _ctx)\n",
      "  _inputs_flat = [logits]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"LogSoftmax\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"LogSoftmax\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def max_pool(input, ksize, strides, padding, data_format=\"NHWC\", name=None):\n",
      "  r\"\"\"Performs max pooling on the input.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `qint8`.\n",
      "      4-D input to pool over.\n",
      "    ksize: A list of `ints` that has length `>= 4`.\n",
      "      The size of the window for each dimension of the input tensor.\n",
      "    strides: A list of `ints` that has length `>= 4`.\n",
      "      The stride of the sliding window for each dimension of the\n",
      "      input tensor.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\", \"NCHW_VECT_C\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the data is stored in the order of:\n",
      "          [batch, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, in_channels, in_height, in_width].\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(ksize, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'ksize' argument to \"\n",
      "          \"'max_pool' Op, not %r.\" % ksize)\n",
      "    ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'max_pool' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"MaxPool\", input=input, ksize=ksize, strides=strides, padding=padding,\n",
      "        data_format=data_format, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"ksize\", _op.get_attr(\"ksize\"),\n",
      "              \"strides\", _op.get_attr(\"strides\"), \"padding\",\n",
      "              _op.get_attr(\"padding\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"))\n",
      "    _execute.record_gradient(\n",
      "      \"MaxPool\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"MaxPool\",\n",
      "        name, _ctx._post_execution_callbacks, input, \"ksize\", ksize,\n",
      "        \"strides\", strides, \"padding\", padding, \"data_format\", data_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return max_pool_eager_fallback(\n",
      "          input, ksize=ksize, strides=strides, padding=padding,\n",
      "          data_format=data_format, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def max_pool_eager_fallback(input, ksize, strides, padding, data_format=\"NHWC\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function max_pool\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(ksize, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'ksize' argument to \"\n",
      "        \"'max_pool' Op, not %r.\" % ksize)\n",
      "  ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'max_pool' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  _attr_T, (input,) = _execute.args_to_matching_eager([input], _ctx, _dtypes.float32)\n",
      "  _inputs_flat = [input]\n",
      "  _attrs = (\"T\", _attr_T, \"ksize\", ksize, \"strides\", strides, \"padding\",\n",
      "  padding, \"data_format\", data_format)\n",
      "  _result = _execute.execute(b\"MaxPool\", 1, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"MaxPool\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.max_pool3d')\n",
      "def max_pool3d(input, ksize, strides, padding, data_format=\"NDHWC\", name=None):\n",
      "  r\"\"\"Performs 3D max pooling on the input.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`.\n",
      "      Shape `[batch, depth, rows, cols, channels]` tensor to pool over.\n",
      "    ksize: A list of `ints` that has length `>= 5`.\n",
      "      1-D tensor of length 5. The size of the window for each dimension of\n",
      "      the input tensor. Must have `ksize[0] = ksize[4] = 1`.\n",
      "    strides: A list of `ints` that has length `>= 5`.\n",
      "      1-D tensor of length 5. The stride of the sliding window for each\n",
      "      dimension of `input`. Must have `strides[0] = strides[4] = 1`.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NDHWC\", \"NCDHW\"`. Defaults to `\"NDHWC\"`.\n",
      "      The data format of the input and output data. With the\n",
      "      default format \"NDHWC\", the data is stored in the order of:\n",
      "          [batch, in_depth, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCDHW\", the data storage order is:\n",
      "          [batch, in_channels, in_depth, in_height, in_width].\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(ksize, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'ksize' argument to \"\n",
      "          \"'max_pool3d' Op, not %r.\" % ksize)\n",
      "    ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'max_pool3d' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NDHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"MaxPool3D\", input=input, ksize=ksize, strides=strides,\n",
      "        padding=padding, data_format=data_format, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"ksize\", _op.get_attr(\"ksize\"), \"strides\",\n",
      "              _op.get_attr(\"strides\"), \"padding\", _op.get_attr(\"padding\"),\n",
      "              \"data_format\", _op.get_attr(\"data_format\"), \"T\",\n",
      "              _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"MaxPool3D\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"MaxPool3D\",\n",
      "        name, _ctx._post_execution_callbacks, input, \"ksize\", ksize,\n",
      "        \"strides\", strides, \"padding\", padding, \"data_format\", data_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return max_pool3d_eager_fallback(\n",
      "          input, ksize=ksize, strides=strides, padding=padding,\n",
      "          data_format=data_format, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def max_pool3d_eager_fallback(input, ksize, strides, padding, data_format=\"NDHWC\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function max_pool3d\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(ksize, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'ksize' argument to \"\n",
      "        \"'max_pool3d' Op, not %r.\" % ksize)\n",
      "  ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'max_pool3d' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NDHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  _attr_T, (input,) = _execute.args_to_matching_eager([input], _ctx)\n",
      "  _inputs_flat = [input]\n",
      "  _attrs = (\"ksize\", ksize, \"strides\", strides, \"padding\", padding,\n",
      "  \"data_format\", data_format, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"MaxPool3D\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"MaxPool3D\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def max_pool3d_grad(orig_input, orig_output, grad, ksize, strides, padding, data_format=\"NDHWC\", name=None):\n",
      "  r\"\"\"Computes gradients of max pooling function.\n",
      "\n",
      "  Args:\n",
      "    orig_input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`.\n",
      "      The original input tensor.\n",
      "    orig_output: A `Tensor`. Must have the same type as `orig_input`.\n",
      "      The original output tensor.\n",
      "    grad: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`.\n",
      "      Output backprop of shape `[batch, depth, rows, cols, channels]`.\n",
      "    ksize: A list of `ints` that has length `>= 5`.\n",
      "      1-D tensor of length 5. The size of the window for each dimension of\n",
      "      the input tensor. Must have `ksize[0] = ksize[4] = 1`.\n",
      "    strides: A list of `ints` that has length `>= 5`.\n",
      "      1-D tensor of length 5. The stride of the sliding window for each\n",
      "      dimension of `input`. Must have `strides[0] = strides[4] = 1`.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NDHWC\", \"NCDHW\"`. Defaults to `\"NDHWC\"`.\n",
      "      The data format of the input and output data. With the\n",
      "      default format \"NDHWC\", the data is stored in the order of:\n",
      "          [batch, in_depth, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCDHW\", the data storage order is:\n",
      "          [batch, in_channels, in_depth, in_height, in_width].\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `grad`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(ksize, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'ksize' argument to \"\n",
      "          \"'max_pool3d_grad' Op, not %r.\" % ksize)\n",
      "    ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'max_pool3d_grad' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NDHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"MaxPool3DGrad\", orig_input=orig_input, orig_output=orig_output,\n",
      "        grad=grad, ksize=ksize, strides=strides, padding=padding,\n",
      "        data_format=data_format, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"ksize\", _op.get_attr(\"ksize\"), \"strides\",\n",
      "              _op.get_attr(\"strides\"), \"padding\", _op.get_attr(\"padding\"),\n",
      "              \"data_format\", _op.get_attr(\"data_format\"), \"T\",\n",
      "              _op.get_attr(\"T\"), \"TInput\", _op.get_attr(\"TInput\"))\n",
      "    _execute.record_gradient(\n",
      "      \"MaxPool3DGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"MaxPool3DGrad\", name, _ctx._post_execution_callbacks, orig_input,\n",
      "        orig_output, grad, \"ksize\", ksize, \"strides\", strides, \"padding\",\n",
      "        padding, \"data_format\", data_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return max_pool3d_grad_eager_fallback(\n",
      "          orig_input, orig_output, grad, ksize=ksize, strides=strides,\n",
      "          padding=padding, data_format=data_format, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def max_pool3d_grad_eager_fallback(orig_input, orig_output, grad, ksize, strides, padding, data_format=\"NDHWC\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function max_pool3d_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(ksize, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'ksize' argument to \"\n",
      "        \"'max_pool3d_grad' Op, not %r.\" % ksize)\n",
      "  ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'max_pool3d_grad' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NDHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  _attr_T, (grad,) = _execute.args_to_matching_eager([grad], _ctx, _dtypes.float32)\n",
      "  _attr_TInput, _inputs_TInput = _execute.args_to_matching_eager([orig_input, orig_output], _ctx, _dtypes.float32)\n",
      "  (orig_input, orig_output) = _inputs_TInput\n",
      "  _inputs_flat = [orig_input, orig_output, grad]\n",
      "  _attrs = (\"ksize\", ksize, \"strides\", strides, \"padding\", padding,\n",
      "  \"data_format\", data_format, \"T\", _attr_T, \"TInput\", _attr_TInput)\n",
      "  _result = _execute.execute(b\"MaxPool3DGrad\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"MaxPool3DGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def max_pool3d_grad_grad(orig_input, orig_output, grad, ksize, strides, padding, data_format=\"NDHWC\", name=None):\n",
      "  r\"\"\"Computes second-order gradients of the maxpooling function.\n",
      "\n",
      "  Args:\n",
      "    orig_input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      The original input tensor.\n",
      "    orig_output: A `Tensor`. Must have the same type as `orig_input`.\n",
      "      The original output tensor.\n",
      "    grad: A `Tensor`. Must have the same type as `orig_input`.\n",
      "      Output backprop of shape `[batch, depth, rows, cols, channels]`.\n",
      "    ksize: A list of `ints` that has length `>= 5`.\n",
      "      1-D tensor of length 5. The size of the window for each dimension of\n",
      "      the input tensor. Must have `ksize[0] = ksize[4] = 1`.\n",
      "    strides: A list of `ints` that has length `>= 5`.\n",
      "      1-D tensor of length 5. The stride of the sliding window for each\n",
      "      dimension of `input`. Must have `strides[0] = strides[4] = 1`.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NDHWC\", \"NCDHW\"`. Defaults to `\"NDHWC\"`.\n",
      "      The data format of the input and output data. With the\n",
      "      default format \"NDHWC\", the data is stored in the order of:\n",
      "          [batch, in_depth, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCDHW\", the data storage order is:\n",
      "          [batch, in_channels, in_depth, in_height, in_width].\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `orig_input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(ksize, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'ksize' argument to \"\n",
      "          \"'max_pool3d_grad_grad' Op, not %r.\" % ksize)\n",
      "    ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'max_pool3d_grad_grad' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NDHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"MaxPool3DGradGrad\", orig_input=orig_input, orig_output=orig_output,\n",
      "        grad=grad, ksize=ksize, strides=strides, padding=padding,\n",
      "        data_format=data_format, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"ksize\", _op.get_attr(\"ksize\"), \"strides\",\n",
      "              _op.get_attr(\"strides\"), \"padding\", _op.get_attr(\"padding\"),\n",
      "              \"data_format\", _op.get_attr(\"data_format\"), \"T\",\n",
      "              _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"MaxPool3DGradGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"MaxPool3DGradGrad\", name, _ctx._post_execution_callbacks, orig_input,\n",
      "        orig_output, grad, \"ksize\", ksize, \"strides\", strides, \"padding\",\n",
      "        padding, \"data_format\", data_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return max_pool3d_grad_grad_eager_fallback(\n",
      "          orig_input, orig_output, grad, ksize=ksize, strides=strides,\n",
      "          padding=padding, data_format=data_format, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def max_pool3d_grad_grad_eager_fallback(orig_input, orig_output, grad, ksize, strides, padding, data_format=\"NDHWC\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function max_pool3d_grad_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(ksize, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'ksize' argument to \"\n",
      "        \"'max_pool3d_grad_grad' Op, not %r.\" % ksize)\n",
      "  ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'max_pool3d_grad_grad' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NDHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([orig_input, orig_output, grad], _ctx)\n",
      "  (orig_input, orig_output, grad) = _inputs_T\n",
      "  _inputs_flat = [orig_input, orig_output, grad]\n",
      "  _attrs = (\"ksize\", ksize, \"strides\", strides, \"padding\", padding,\n",
      "  \"data_format\", data_format, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"MaxPool3DGradGrad\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"MaxPool3DGradGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def max_pool_grad(orig_input, orig_output, grad, ksize, strides, padding, data_format=\"NHWC\", name=None):\n",
      "  r\"\"\"Computes gradients of the maxpooling function.\n",
      "\n",
      "  Args:\n",
      "    orig_input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      The original input tensor.\n",
      "    orig_output: A `Tensor`. Must have the same type as `orig_input`.\n",
      "      The original output tensor.\n",
      "    grad: A `Tensor`. Must have the same type as `orig_input`.\n",
      "      4-D.  Gradients w.r.t. the output of `max_pool`.\n",
      "    ksize: A list of `ints` that has length `>= 4`.\n",
      "      The size of the window for each dimension of the input tensor.\n",
      "    strides: A list of `ints` that has length `>= 4`.\n",
      "      The stride of the sliding window for each dimension of the\n",
      "      input tensor.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the data is stored in the order of:\n",
      "          [batch, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, in_channels, in_height, in_width].\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `orig_input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(ksize, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'ksize' argument to \"\n",
      "          \"'max_pool_grad' Op, not %r.\" % ksize)\n",
      "    ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'max_pool_grad' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"MaxPoolGrad\", orig_input=orig_input, orig_output=orig_output,\n",
      "        grad=grad, ksize=ksize, strides=strides, padding=padding,\n",
      "        data_format=data_format, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"ksize\", _op.get_attr(\"ksize\"), \"strides\",\n",
      "              _op.get_attr(\"strides\"), \"padding\", _op.get_attr(\"padding\"),\n",
      "              \"data_format\", _op.get_attr(\"data_format\"), \"T\",\n",
      "              _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"MaxPoolGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"MaxPoolGrad\",\n",
      "        name, _ctx._post_execution_callbacks, orig_input, orig_output, grad,\n",
      "        \"ksize\", ksize, \"strides\", strides, \"padding\", padding, \"data_format\",\n",
      "        data_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return max_pool_grad_eager_fallback(\n",
      "          orig_input, orig_output, grad, ksize=ksize, strides=strides,\n",
      "          padding=padding, data_format=data_format, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def max_pool_grad_eager_fallback(orig_input, orig_output, grad, ksize, strides, padding, data_format=\"NHWC\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function max_pool_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(ksize, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'ksize' argument to \"\n",
      "        \"'max_pool_grad' Op, not %r.\" % ksize)\n",
      "  ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'max_pool_grad' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([orig_input, orig_output, grad], _ctx, _dtypes.float32)\n",
      "  (orig_input, orig_output, grad) = _inputs_T\n",
      "  _inputs_flat = [orig_input, orig_output, grad]\n",
      "  _attrs = (\"ksize\", ksize, \"strides\", strides, \"padding\", padding,\n",
      "  \"data_format\", data_format, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"MaxPoolGrad\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"MaxPoolGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def max_pool_grad_grad(orig_input, orig_output, grad, ksize, strides, padding, data_format=\"NHWC\", name=None):\n",
      "  r\"\"\"Computes second-order gradients of the maxpooling function.\n",
      "\n",
      "  Args:\n",
      "    orig_input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      The original input tensor.\n",
      "    orig_output: A `Tensor`. Must have the same type as `orig_input`.\n",
      "      The original output tensor.\n",
      "    grad: A `Tensor`. Must have the same type as `orig_input`.\n",
      "      4-D.  Gradients of gradients w.r.t. the input of `max_pool`.\n",
      "    ksize: A list of `ints` that has length `>= 4`.\n",
      "      The size of the window for each dimension of the input tensor.\n",
      "    strides: A list of `ints` that has length `>= 4`.\n",
      "      The stride of the sliding window for each dimension of the\n",
      "      input tensor.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the data is stored in the order of:\n",
      "          [batch, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, in_channels, in_height, in_width].\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `orig_input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(ksize, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'ksize' argument to \"\n",
      "          \"'max_pool_grad_grad' Op, not %r.\" % ksize)\n",
      "    ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'max_pool_grad_grad' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"MaxPoolGradGrad\", orig_input=orig_input, orig_output=orig_output,\n",
      "        grad=grad, ksize=ksize, strides=strides, padding=padding,\n",
      "        data_format=data_format, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"ksize\", _op.get_attr(\"ksize\"), \"strides\",\n",
      "              _op.get_attr(\"strides\"), \"padding\", _op.get_attr(\"padding\"),\n",
      "              \"data_format\", _op.get_attr(\"data_format\"), \"T\",\n",
      "              _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"MaxPoolGradGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"MaxPoolGradGrad\", name, _ctx._post_execution_callbacks, orig_input,\n",
      "        orig_output, grad, \"ksize\", ksize, \"strides\", strides, \"padding\",\n",
      "        padding, \"data_format\", data_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return max_pool_grad_grad_eager_fallback(\n",
      "          orig_input, orig_output, grad, ksize=ksize, strides=strides,\n",
      "          padding=padding, data_format=data_format, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def max_pool_grad_grad_eager_fallback(orig_input, orig_output, grad, ksize, strides, padding, data_format=\"NHWC\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function max_pool_grad_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(ksize, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'ksize' argument to \"\n",
      "        \"'max_pool_grad_grad' Op, not %r.\" % ksize)\n",
      "  ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'max_pool_grad_grad' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([orig_input, orig_output, grad], _ctx)\n",
      "  (orig_input, orig_output, grad) = _inputs_T\n",
      "  _inputs_flat = [orig_input, orig_output, grad]\n",
      "  _attrs = (\"ksize\", ksize, \"strides\", strides, \"padding\", padding,\n",
      "  \"data_format\", data_format, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"MaxPoolGradGrad\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"MaxPoolGradGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def max_pool_grad_grad_v2(orig_input, orig_output, grad, ksize, strides, padding, data_format=\"NHWC\", name=None):\n",
      "  r\"\"\"Computes second-order gradients of the maxpooling function.\n",
      "\n",
      "  Args:\n",
      "    orig_input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      The original input tensor.\n",
      "    orig_output: A `Tensor`. Must have the same type as `orig_input`.\n",
      "      The original output tensor.\n",
      "    grad: A `Tensor`. Must have the same type as `orig_input`.\n",
      "      4-D.  Gradients of gradients w.r.t. the input of `max_pool`.\n",
      "    ksize: A `Tensor` of type `int32`.\n",
      "      The size of the window for each dimension of the input tensor.\n",
      "    strides: A `Tensor` of type `int32`.\n",
      "      The stride of the sliding window for each dimension of the\n",
      "      input tensor.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the data is stored in the order of:\n",
      "          [batch, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, in_channels, in_height, in_width].\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `orig_input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"MaxPoolGradGradV2\", orig_input=orig_input, orig_output=orig_output,\n",
      "        grad=grad, ksize=ksize, strides=strides, padding=padding,\n",
      "        data_format=data_format, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"padding\", _op.get_attr(\"padding\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"), \"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"MaxPoolGradGradV2\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"MaxPoolGradGradV2\", name, _ctx._post_execution_callbacks, orig_input,\n",
      "        orig_output, grad, ksize, strides, \"padding\", padding, \"data_format\",\n",
      "        data_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return max_pool_grad_grad_v2_eager_fallback(\n",
      "          orig_input, orig_output, grad, ksize, strides, padding=padding,\n",
      "          data_format=data_format, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def max_pool_grad_grad_v2_eager_fallback(orig_input, orig_output, grad, ksize, strides, padding, data_format=\"NHWC\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function max_pool_grad_grad_v2\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([orig_input, orig_output, grad], _ctx)\n",
      "  (orig_input, orig_output, grad) = _inputs_T\n",
      "  ksize = _ops.convert_to_tensor(ksize, _dtypes.int32)\n",
      "  strides = _ops.convert_to_tensor(strides, _dtypes.int32)\n",
      "  _inputs_flat = [orig_input, orig_output, grad, ksize, strides]\n",
      "  _attrs = (\"padding\", padding, \"data_format\", data_format, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"MaxPoolGradGradV2\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"MaxPoolGradGradV2\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def max_pool_grad_grad_with_argmax(input, grad, argmax, ksize, strides, padding, name=None):\n",
      "  r\"\"\"Computes second-order gradients of the maxpooling function.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      The original input.\n",
      "    grad: A `Tensor`. Must have the same type as `input`.\n",
      "      4-D with shape `[batch, height, width, channels]`.  Gradients w.r.t. the\n",
      "      input of `max_pool`.\n",
      "    argmax: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "      The indices of the maximum values chosen for each output of `max_pool`.\n",
      "    ksize: A list of `ints` that has length `>= 4`.\n",
      "      The size of the window for each dimension of the input tensor.\n",
      "    strides: A list of `ints` that has length `>= 4`.\n",
      "      The stride of the sliding window for each dimension of the\n",
      "      input tensor.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(ksize, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'ksize' argument to \"\n",
      "          \"'max_pool_grad_grad_with_argmax' Op, not %r.\" % ksize)\n",
      "    ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'max_pool_grad_grad_with_argmax' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"MaxPoolGradGradWithArgmax\", input=input, grad=grad, argmax=argmax,\n",
      "        ksize=ksize, strides=strides, padding=padding, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"ksize\", _op.get_attr(\"ksize\"), \"strides\",\n",
      "              _op.get_attr(\"strides\"), \"padding\", _op.get_attr(\"padding\"),\n",
      "              \"Targmax\", _op.get_attr(\"Targmax\"), \"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"MaxPoolGradGradWithArgmax\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"MaxPoolGradGradWithArgmax\", name, _ctx._post_execution_callbacks,\n",
      "        input, grad, argmax, \"ksize\", ksize, \"strides\", strides, \"padding\",\n",
      "        padding)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return max_pool_grad_grad_with_argmax_eager_fallback(\n",
      "          input, grad, argmax, ksize=ksize, strides=strides, padding=padding,\n",
      "          name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def max_pool_grad_grad_with_argmax_eager_fallback(input, grad, argmax, ksize, strides, padding, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function max_pool_grad_grad_with_argmax\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(ksize, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'ksize' argument to \"\n",
      "        \"'max_pool_grad_grad_with_argmax' Op, not %r.\" % ksize)\n",
      "  ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'max_pool_grad_grad_with_argmax' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  _attr_Targmax, (argmax,) = _execute.args_to_matching_eager([argmax], _ctx)\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, grad], _ctx)\n",
      "  (input, grad) = _inputs_T\n",
      "  _inputs_flat = [input, grad, argmax]\n",
      "  _attrs = (\"ksize\", ksize, \"strides\", strides, \"padding\", padding, \"Targmax\",\n",
      "  _attr_Targmax, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"MaxPoolGradGradWithArgmax\", 1,\n",
      "                             inputs=_inputs_flat, attrs=_attrs, ctx=_ctx,\n",
      "                             name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"MaxPoolGradGradWithArgmax\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def max_pool_grad_v2(orig_input, orig_output, grad, ksize, strides, padding, data_format=\"NHWC\", name=None):\n",
      "  r\"\"\"Computes gradients of the maxpooling function.\n",
      "\n",
      "  Args:\n",
      "    orig_input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      The original input tensor.\n",
      "    orig_output: A `Tensor`. Must have the same type as `orig_input`.\n",
      "      The original output tensor.\n",
      "    grad: A `Tensor`. Must have the same type as `orig_input`.\n",
      "      4-D.  Gradients w.r.t. the output of `max_pool`.\n",
      "    ksize: A `Tensor` of type `int32`.\n",
      "      The size of the window for each dimension of the input tensor.\n",
      "    strides: A `Tensor` of type `int32`.\n",
      "      The stride of the sliding window for each dimension of the\n",
      "      input tensor.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the data is stored in the order of:\n",
      "          [batch, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, in_channels, in_height, in_width].\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `orig_input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"MaxPoolGradV2\", orig_input=orig_input, orig_output=orig_output,\n",
      "        grad=grad, ksize=ksize, strides=strides, padding=padding,\n",
      "        data_format=data_format, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"padding\", _op.get_attr(\"padding\"), \"data_format\",\n",
      "              _op.get_attr(\"data_format\"), \"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"MaxPoolGradV2\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"MaxPoolGradV2\", name, _ctx._post_execution_callbacks, orig_input,\n",
      "        orig_output, grad, ksize, strides, \"padding\", padding, \"data_format\",\n",
      "        data_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return max_pool_grad_v2_eager_fallback(\n",
      "          orig_input, orig_output, grad, ksize, strides, padding=padding,\n",
      "          data_format=data_format, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def max_pool_grad_v2_eager_fallback(orig_input, orig_output, grad, ksize, strides, padding, data_format=\"NHWC\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function max_pool_grad_v2\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([orig_input, orig_output, grad], _ctx, _dtypes.float32)\n",
      "  (orig_input, orig_output, grad) = _inputs_T\n",
      "  ksize = _ops.convert_to_tensor(ksize, _dtypes.int32)\n",
      "  strides = _ops.convert_to_tensor(strides, _dtypes.int32)\n",
      "  _inputs_flat = [orig_input, orig_output, grad, ksize, strides]\n",
      "  _attrs = (\"padding\", padding, \"data_format\", data_format, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"MaxPoolGradV2\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"MaxPoolGradV2\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def max_pool_grad_with_argmax(input, grad, argmax, ksize, strides, padding, name=None):\n",
      "  r\"\"\"Computes gradients of the maxpooling function.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      The original input.\n",
      "    grad: A `Tensor`. Must have the same type as `input`.\n",
      "      4-D with shape `[batch, height, width, channels]`.  Gradients w.r.t. the\n",
      "      output of `max_pool`.\n",
      "    argmax: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "      The indices of the maximum values chosen for each output of `max_pool`.\n",
      "    ksize: A list of `ints` that has length `>= 4`.\n",
      "      The size of the window for each dimension of the input tensor.\n",
      "    strides: A list of `ints` that has length `>= 4`.\n",
      "      The stride of the sliding window for each dimension of the\n",
      "      input tensor.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(ksize, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'ksize' argument to \"\n",
      "          \"'max_pool_grad_with_argmax' Op, not %r.\" % ksize)\n",
      "    ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'max_pool_grad_with_argmax' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"MaxPoolGradWithArgmax\", input=input, grad=grad, argmax=argmax,\n",
      "        ksize=ksize, strides=strides, padding=padding, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"ksize\", _op.get_attr(\"ksize\"), \"strides\",\n",
      "              _op.get_attr(\"strides\"), \"padding\", _op.get_attr(\"padding\"),\n",
      "              \"Targmax\", _op.get_attr(\"Targmax\"), \"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"MaxPoolGradWithArgmax\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"MaxPoolGradWithArgmax\", name, _ctx._post_execution_callbacks, input,\n",
      "        grad, argmax, \"ksize\", ksize, \"strides\", strides, \"padding\", padding)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return max_pool_grad_with_argmax_eager_fallback(\n",
      "          input, grad, argmax, ksize=ksize, strides=strides, padding=padding,\n",
      "          name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def max_pool_grad_with_argmax_eager_fallback(input, grad, argmax, ksize, strides, padding, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function max_pool_grad_with_argmax\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(ksize, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'ksize' argument to \"\n",
      "        \"'max_pool_grad_with_argmax' Op, not %r.\" % ksize)\n",
      "  ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'max_pool_grad_with_argmax' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  _attr_Targmax, (argmax,) = _execute.args_to_matching_eager([argmax], _ctx)\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, grad], _ctx)\n",
      "  (input, grad) = _inputs_T\n",
      "  _inputs_flat = [input, grad, argmax]\n",
      "  _attrs = (\"ksize\", ksize, \"strides\", strides, \"padding\", padding, \"Targmax\",\n",
      "  _attr_Targmax, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"MaxPoolGradWithArgmax\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"MaxPoolGradWithArgmax\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def max_pool_v2(input, ksize, strides, padding, data_format=\"NHWC\", name=None):\n",
      "  r\"\"\"Performs max pooling on the input.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, `qint8`.\n",
      "      4-D input to pool over.\n",
      "    ksize: A `Tensor` of type `int32`.\n",
      "      The size of the window for each dimension of the input tensor.\n",
      "    strides: A `Tensor` of type `int32`.\n",
      "      The stride of the sliding window for each dimension of the\n",
      "      input tensor.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    data_format: An optional `string` from: `\"NHWC\", \"NCHW\", \"NCHW_VECT_C\"`. Defaults to `\"NHWC\"`.\n",
      "      Specify the data format of the input and output data. With the\n",
      "      default format \"NHWC\", the data is stored in the order of:\n",
      "          [batch, in_height, in_width, in_channels].\n",
      "      Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "          [batch, in_channels, in_height, in_width].\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if data_format is None:\n",
      "      data_format = \"NHWC\"\n",
      "    data_format = _execute.make_str(data_format, \"data_format\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"MaxPoolV2\", input=input, ksize=ksize, strides=strides,\n",
      "        padding=padding, data_format=data_format, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"padding\", _op.get_attr(\"padding\"),\n",
      "              \"data_format\", _op.get_attr(\"data_format\"))\n",
      "    _execute.record_gradient(\n",
      "      \"MaxPoolV2\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"MaxPoolV2\",\n",
      "        name, _ctx._post_execution_callbacks, input, ksize, strides,\n",
      "        \"padding\", padding, \"data_format\", data_format)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return max_pool_v2_eager_fallback(\n",
      "          input, ksize, strides, padding=padding, data_format=data_format,\n",
      "          name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def max_pool_v2_eager_fallback(input, ksize, strides, padding, data_format=\"NHWC\", name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function max_pool_v2\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if data_format is None:\n",
      "    data_format = \"NHWC\"\n",
      "  data_format = _execute.make_str(data_format, \"data_format\")\n",
      "  _attr_T, (input,) = _execute.args_to_matching_eager([input], _ctx, _dtypes.float32)\n",
      "  ksize = _ops.convert_to_tensor(ksize, _dtypes.int32)\n",
      "  strides = _ops.convert_to_tensor(strides, _dtypes.int32)\n",
      "  _inputs_flat = [input, ksize, strides]\n",
      "  _attrs = (\"T\", _attr_T, \"padding\", padding, \"data_format\", data_format)\n",
      "  _result = _execute.execute(b\"MaxPoolV2\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"MaxPoolV2\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "_max_pool_with_argmax_outputs = [\"output\", \"argmax\"]\n",
      "_MaxPoolWithArgmaxOutput = _collections.namedtuple(\n",
      "    \"MaxPoolWithArgmax\", _max_pool_with_argmax_outputs)\n",
      "\n",
      "\n",
      "@tf_export('nn.max_pool_with_argmax')\n",
      "def max_pool_with_argmax(input, ksize, strides, padding, Targmax=_dtypes.int64, name=None):\n",
      "  r\"\"\"Performs max pooling on the input and outputs both max values and indices.\n",
      "\n",
      "  The indices in `argmax` are flattened, so that a maximum value at position\n",
      "  `[b, y, x, c]` becomes flattened index\n",
      "  `((b * height + y) * width + x) * channels + c`.\n",
      "\n",
      "  The indices returned are always in `[0, height) x [0, width)` before flattening,\n",
      "  even if padding is involved and the mathematically correct answer is outside\n",
      "  (either negative or too large).  This is a bug, but fixing it is difficult to do\n",
      "  in a safe backwards compatible way, especially due to flattening.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      4-D with shape `[batch, height, width, channels]`.  Input to pool over.\n",
      "    ksize: A list of `ints` that has length `>= 4`.\n",
      "      The size of the window for each dimension of the input tensor.\n",
      "    strides: A list of `ints` that has length `>= 4`.\n",
      "      The stride of the sliding window for each dimension of the\n",
      "      input tensor.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    Targmax: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int64`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (output, argmax).\n",
      "\n",
      "    output: A `Tensor`. Has the same type as `input`.\n",
      "    argmax: A `Tensor` of type `Targmax`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(ksize, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'ksize' argument to \"\n",
      "          \"'max_pool_with_argmax' Op, not %r.\" % ksize)\n",
      "    ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'max_pool_with_argmax' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if Targmax is None:\n",
      "      Targmax = _dtypes.int64\n",
      "    Targmax = _execute.make_type(Targmax, \"Targmax\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"MaxPoolWithArgmax\", input=input, ksize=ksize, strides=strides,\n",
      "        padding=padding, Targmax=Targmax, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"ksize\", _op.get_attr(\"ksize\"), \"strides\",\n",
      "              _op.get_attr(\"strides\"), \"Targmax\", _op.get_attr(\"Targmax\"),\n",
      "              \"padding\", _op.get_attr(\"padding\"), \"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"MaxPoolWithArgmax\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _MaxPoolWithArgmaxOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"MaxPoolWithArgmax\", name, _ctx._post_execution_callbacks, input,\n",
      "        \"ksize\", ksize, \"strides\", strides, \"Targmax\", Targmax, \"padding\",\n",
      "        padding)\n",
      "      _result = _MaxPoolWithArgmaxOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return max_pool_with_argmax_eager_fallback(\n",
      "          input, ksize=ksize, strides=strides, Targmax=Targmax,\n",
      "          padding=padding, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def max_pool_with_argmax_eager_fallback(input, ksize, strides, padding, Targmax=_dtypes.int64, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function max_pool_with_argmax\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(ksize, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'ksize' argument to \"\n",
      "        \"'max_pool_with_argmax' Op, not %r.\" % ksize)\n",
      "  ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'max_pool_with_argmax' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if Targmax is None:\n",
      "    Targmax = _dtypes.int64\n",
      "  Targmax = _execute.make_type(Targmax, \"Targmax\")\n",
      "  _attr_T, (input,) = _execute.args_to_matching_eager([input], _ctx)\n",
      "  _inputs_flat = [input]\n",
      "  _attrs = (\"ksize\", ksize, \"strides\", strides, \"Targmax\", Targmax, \"padding\",\n",
      "  padding, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"MaxPoolWithArgmax\", 2, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"MaxPoolWithArgmax\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _MaxPoolWithArgmaxOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "def nth_element(input, n, reverse=False, name=None):\n",
      "  r\"\"\"Finds values of the `n`-th order statistic for the last dimension.\n",
      "\n",
      "  If the input is a vector (rank-1), finds the entries which is the nth-smallest\n",
      "  value in the vector and outputs their values as scalar tensor.\n",
      "\n",
      "  For matrices (resp. higher rank input), computes the entries which is the\n",
      "  nth-smallest value in each row (resp. vector along the last dimension). Thus,\n",
      "\n",
      "      values.shape = input.shape[:-1]\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      1-D or higher with last dimension at least `n+1`.\n",
      "    n: A `Tensor` of type `int32`.\n",
      "      0-D. Position of sorted vector to select along the last dimension (along\n",
      "      each row for matrices). Valid range of n is `[0, input.shape[:-1])`\n",
      "    reverse: An optional `bool`. Defaults to `False`.\n",
      "      When set to True, find the nth-largest value in the vector and vice\n",
      "      versa.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `input`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if reverse is None:\n",
      "      reverse = False\n",
      "    reverse = _execute.make_bool(reverse, \"reverse\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"NthElement\", input=input, n=n, reverse=reverse, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"reverse\", _op.get_attr(\"reverse\"), \"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"NthElement\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"NthElement\",\n",
      "        name, _ctx._post_execution_callbacks, input, n, \"reverse\", reverse)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return nth_element_eager_fallback(\n",
      "          input, n, reverse=reverse, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def nth_element_eager_fallback(input, n, reverse=False, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function nth_element\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if reverse is None:\n",
      "    reverse = False\n",
      "  reverse = _execute.make_bool(reverse, \"reverse\")\n",
      "  _attr_T, (input,) = _execute.args_to_matching_eager([input], _ctx)\n",
      "  n = _ops.convert_to_tensor(n, _dtypes.int32)\n",
      "  _inputs_flat = [input, n]\n",
      "  _attrs = (\"reverse\", reverse, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"NthElement\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"NthElement\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "_quantized_avg_pool_outputs = [\"output\", \"min_output\", \"max_output\"]\n",
      "_QuantizedAvgPoolOutput = _collections.namedtuple(\n",
      "    \"QuantizedAvgPool\", _quantized_avg_pool_outputs)\n",
      "\n",
      "\n",
      "@tf_export('nn.quantized_avg_pool')\n",
      "def quantized_avg_pool(input, min_input, max_input, ksize, strides, padding, name=None):\n",
      "  r\"\"\"Produces the average pool of the input tensor for quantized types.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n",
      "      4-D with shape `[batch, height, width, channels]`.\n",
      "    min_input: A `Tensor` of type `float32`.\n",
      "      The float value that the lowest quantized input value represents.\n",
      "    max_input: A `Tensor` of type `float32`.\n",
      "      The float value that the highest quantized input value represents.\n",
      "    ksize: A list of `ints`.\n",
      "      The size of the window for each dimension of the input tensor.\n",
      "      The length must be 4 to match the number of dimensions of the input.\n",
      "    strides: A list of `ints`.\n",
      "      The stride of the sliding window for each dimension of the input\n",
      "      tensor.  The length must be 4 to match the number of dimensions of the input.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (output, min_output, max_output).\n",
      "\n",
      "    output: A `Tensor`. Has the same type as `input`.\n",
      "    min_output: A `Tensor` of type `float32`.\n",
      "    max_output: A `Tensor` of type `float32`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(ksize, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'ksize' argument to \"\n",
      "          \"'quantized_avg_pool' Op, not %r.\" % ksize)\n",
      "    ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'quantized_avg_pool' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"QuantizedAvgPool\", input=input, min_input=min_input,\n",
      "        max_input=max_input, ksize=ksize, strides=strides, padding=padding,\n",
      "        name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"ksize\", _op.get_attr(\"ksize\"),\n",
      "              \"strides\", _op.get_attr(\"strides\"), \"padding\",\n",
      "              _op.get_attr(\"padding\"))\n",
      "    _execute.record_gradient(\n",
      "      \"QuantizedAvgPool\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _QuantizedAvgPoolOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"QuantizedAvgPool\", name, _ctx._post_execution_callbacks, input,\n",
      "        min_input, max_input, \"ksize\", ksize, \"strides\", strides, \"padding\",\n",
      "        padding)\n",
      "      _result = _QuantizedAvgPoolOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return quantized_avg_pool_eager_fallback(\n",
      "          input, min_input, max_input, ksize=ksize, strides=strides,\n",
      "          padding=padding, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def quantized_avg_pool_eager_fallback(input, min_input, max_input, ksize, strides, padding, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function quantized_avg_pool\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(ksize, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'ksize' argument to \"\n",
      "        \"'quantized_avg_pool' Op, not %r.\" % ksize)\n",
      "  ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'quantized_avg_pool' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  _attr_T, (input,) = _execute.args_to_matching_eager([input], _ctx)\n",
      "  min_input = _ops.convert_to_tensor(min_input, _dtypes.float32)\n",
      "  max_input = _ops.convert_to_tensor(max_input, _dtypes.float32)\n",
      "  _inputs_flat = [input, min_input, max_input]\n",
      "  _attrs = (\"T\", _attr_T, \"ksize\", ksize, \"strides\", strides, \"padding\",\n",
      "  padding)\n",
      "  _result = _execute.execute(b\"QuantizedAvgPool\", 3, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"QuantizedAvgPool\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _QuantizedAvgPoolOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "_quantized_batch_norm_with_global_normalization_outputs = [\"result\",\n",
      "                                                          \"result_min\",\n",
      "                                                          \"result_max\"]\n",
      "_QuantizedBatchNormWithGlobalNormalizationOutput = _collections.namedtuple(\n",
      "    \"QuantizedBatchNormWithGlobalNormalization\",\n",
      "    _quantized_batch_norm_with_global_normalization_outputs)\n",
      "\n",
      "\n",
      "def quantized_batch_norm_with_global_normalization(t, t_min, t_max, m, m_min, m_max, v, v_min, v_max, beta, beta_min, beta_max, gamma, gamma_min, gamma_max, out_type, variance_epsilon, scale_after_normalization, name=None):\n",
      "  r\"\"\"Quantized Batch normalization.\n",
      "\n",
      "  This op is deprecated and will be removed in the future. Prefer\n",
      "  `tf.nn.batch_normalization`.\n",
      "\n",
      "  Args:\n",
      "    t: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n",
      "      A 4D input Tensor.\n",
      "    t_min: A `Tensor` of type `float32`.\n",
      "      The value represented by the lowest quantized input.\n",
      "    t_max: A `Tensor` of type `float32`.\n",
      "      The value represented by the highest quantized input.\n",
      "    m: A `Tensor`. Must have the same type as `t`.\n",
      "      A 1D mean Tensor with size matching the last dimension of t.\n",
      "      This is the first output from tf.nn.moments,\n",
      "      or a saved moving average thereof.\n",
      "    m_min: A `Tensor` of type `float32`.\n",
      "      The value represented by the lowest quantized mean.\n",
      "    m_max: A `Tensor` of type `float32`.\n",
      "      The value represented by the highest quantized mean.\n",
      "    v: A `Tensor`. Must have the same type as `t`.\n",
      "      A 1D variance Tensor with size matching the last dimension of t.\n",
      "      This is the second output from tf.nn.moments,\n",
      "      or a saved moving average thereof.\n",
      "    v_min: A `Tensor` of type `float32`.\n",
      "      The value represented by the lowest quantized variance.\n",
      "    v_max: A `Tensor` of type `float32`.\n",
      "      The value represented by the highest quantized variance.\n",
      "    beta: A `Tensor`. Must have the same type as `t`.\n",
      "      A 1D beta Tensor with size matching the last dimension of t.\n",
      "      An offset to be added to the normalized tensor.\n",
      "    beta_min: A `Tensor` of type `float32`.\n",
      "      The value represented by the lowest quantized offset.\n",
      "    beta_max: A `Tensor` of type `float32`.\n",
      "      The value represented by the highest quantized offset.\n",
      "    gamma: A `Tensor`. Must have the same type as `t`.\n",
      "      A 1D gamma Tensor with size matching the last dimension of t.\n",
      "      If \"scale_after_normalization\" is true, this tensor will be multiplied\n",
      "      with the normalized tensor.\n",
      "    gamma_min: A `Tensor` of type `float32`.\n",
      "      The value represented by the lowest quantized gamma.\n",
      "    gamma_max: A `Tensor` of type `float32`.\n",
      "      The value represented by the highest quantized gamma.\n",
      "    out_type: A `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`.\n",
      "    variance_epsilon: A `float`. A small float number to avoid dividing by 0.\n",
      "    scale_after_normalization: A `bool`.\n",
      "      A bool indicating whether the resulted tensor\n",
      "      needs to be multiplied with gamma.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (result, result_min, result_max).\n",
      "\n",
      "    result: A `Tensor` of type `out_type`.\n",
      "    result_min: A `Tensor` of type `float32`.\n",
      "    result_max: A `Tensor` of type `float32`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    out_type = _execute.make_type(out_type, \"out_type\")\n",
      "    variance_epsilon = _execute.make_float(variance_epsilon, \"variance_epsilon\")\n",
      "    scale_after_normalization = _execute.make_bool(scale_after_normalization, \"scale_after_normalization\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"QuantizedBatchNormWithGlobalNormalization\", t=t, t_min=t_min,\n",
      "        t_max=t_max, m=m, m_min=m_min, m_max=m_max, v=v, v_min=v_min,\n",
      "        v_max=v_max, beta=beta, beta_min=beta_min, beta_max=beta_max,\n",
      "        gamma=gamma, gamma_min=gamma_min, gamma_max=gamma_max,\n",
      "        out_type=out_type, variance_epsilon=variance_epsilon,\n",
      "        scale_after_normalization=scale_after_normalization, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"Tinput\", _op.get_attr(\"Tinput\"), \"out_type\",\n",
      "              _op.get_attr(\"out_type\"), \"variance_epsilon\",\n",
      "              _op.get_attr(\"variance_epsilon\"), \"scale_after_normalization\",\n",
      "              _op.get_attr(\"scale_after_normalization\"))\n",
      "    _execute.record_gradient(\n",
      "      \"QuantizedBatchNormWithGlobalNormalization\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _QuantizedBatchNormWithGlobalNormalizationOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"QuantizedBatchNormWithGlobalNormalization\", name,\n",
      "        _ctx._post_execution_callbacks, t, t_min, t_max, m, m_min, m_max, v,\n",
      "        v_min, v_max, beta, beta_min, beta_max, gamma, gamma_min, gamma_max,\n",
      "        \"out_type\", out_type, \"variance_epsilon\", variance_epsilon,\n",
      "        \"scale_after_normalization\", scale_after_normalization)\n",
      "      _result = _QuantizedBatchNormWithGlobalNormalizationOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return quantized_batch_norm_with_global_normalization_eager_fallback(\n",
      "          t, t_min, t_max, m, m_min, m_max, v, v_min, v_max, beta, beta_min,\n",
      "          beta_max, gamma, gamma_min, gamma_max, out_type=out_type,\n",
      "          variance_epsilon=variance_epsilon,\n",
      "          scale_after_normalization=scale_after_normalization, name=name,\n",
      "          ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def quantized_batch_norm_with_global_normalization_eager_fallback(t, t_min, t_max, m, m_min, m_max, v, v_min, v_max, beta, beta_min, beta_max, gamma, gamma_min, gamma_max, out_type, variance_epsilon, scale_after_normalization, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function quantized_batch_norm_with_global_normalization\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  out_type = _execute.make_type(out_type, \"out_type\")\n",
      "  variance_epsilon = _execute.make_float(variance_epsilon, \"variance_epsilon\")\n",
      "  scale_after_normalization = _execute.make_bool(scale_after_normalization, \"scale_after_normalization\")\n",
      "  _attr_Tinput, _inputs_Tinput = _execute.args_to_matching_eager([t, m, v, beta, gamma], _ctx)\n",
      "  (t, m, v, beta, gamma) = _inputs_Tinput\n",
      "  t_min = _ops.convert_to_tensor(t_min, _dtypes.float32)\n",
      "  t_max = _ops.convert_to_tensor(t_max, _dtypes.float32)\n",
      "  m_min = _ops.convert_to_tensor(m_min, _dtypes.float32)\n",
      "  m_max = _ops.convert_to_tensor(m_max, _dtypes.float32)\n",
      "  v_min = _ops.convert_to_tensor(v_min, _dtypes.float32)\n",
      "  v_max = _ops.convert_to_tensor(v_max, _dtypes.float32)\n",
      "  beta_min = _ops.convert_to_tensor(beta_min, _dtypes.float32)\n",
      "  beta_max = _ops.convert_to_tensor(beta_max, _dtypes.float32)\n",
      "  gamma_min = _ops.convert_to_tensor(gamma_min, _dtypes.float32)\n",
      "  gamma_max = _ops.convert_to_tensor(gamma_max, _dtypes.float32)\n",
      "  _inputs_flat = [t, t_min, t_max, m, m_min, m_max, v, v_min, v_max, beta, beta_min, beta_max, gamma, gamma_min, gamma_max]\n",
      "  _attrs = (\"Tinput\", _attr_Tinput, \"out_type\", out_type, \"variance_epsilon\",\n",
      "  variance_epsilon, \"scale_after_normalization\", scale_after_normalization)\n",
      "  _result = _execute.execute(b\"QuantizedBatchNormWithGlobalNormalization\", 3,\n",
      "                             inputs=_inputs_flat, attrs=_attrs, ctx=_ctx,\n",
      "                             name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"QuantizedBatchNormWithGlobalNormalization\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _QuantizedBatchNormWithGlobalNormalizationOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "_quantized_bias_add_outputs = [\"output\", \"min_out\", \"max_out\"]\n",
      "_QuantizedBiasAddOutput = _collections.namedtuple(\n",
      "    \"QuantizedBiasAdd\", _quantized_bias_add_outputs)\n",
      "\n",
      "\n",
      "def quantized_bias_add(input, bias, min_input, max_input, min_bias, max_bias, out_type, name=None):\n",
      "  r\"\"\"Adds Tensor 'bias' to Tensor 'input' for Quantized types.\n",
      "\n",
      "  Broadcasts the values of bias on dimensions 0..N-2 of 'input'.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n",
      "    bias: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n",
      "      A 1D bias Tensor with size matching the last dimension of 'input'.\n",
      "    min_input: A `Tensor` of type `float32`.\n",
      "      The float value that the lowest quantized input value represents.\n",
      "    max_input: A `Tensor` of type `float32`.\n",
      "      The float value that the highest quantized input value represents.\n",
      "    min_bias: A `Tensor` of type `float32`.\n",
      "      The float value that the lowest quantized bias value represents.\n",
      "    max_bias: A `Tensor` of type `float32`.\n",
      "      The float value that the highest quantized bias value represents.\n",
      "    out_type: A `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (output, min_out, max_out).\n",
      "\n",
      "    output: A `Tensor` of type `out_type`.\n",
      "    min_out: A `Tensor` of type `float32`.\n",
      "    max_out: A `Tensor` of type `float32`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    out_type = _execute.make_type(out_type, \"out_type\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"QuantizedBiasAdd\", input=input, bias=bias, min_input=min_input,\n",
      "        max_input=max_input, min_bias=min_bias, max_bias=max_bias,\n",
      "        out_type=out_type, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T1\", _op.get_attr(\"T1\"), \"T2\", _op.get_attr(\"T2\"), \"out_type\",\n",
      "              _op.get_attr(\"out_type\"))\n",
      "    _execute.record_gradient(\n",
      "      \"QuantizedBiasAdd\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _QuantizedBiasAddOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"QuantizedBiasAdd\", name, _ctx._post_execution_callbacks, input, bias,\n",
      "        min_input, max_input, min_bias, max_bias, \"out_type\", out_type)\n",
      "      _result = _QuantizedBiasAddOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return quantized_bias_add_eager_fallback(\n",
      "          input, bias, min_input, max_input, min_bias, max_bias,\n",
      "          out_type=out_type, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def quantized_bias_add_eager_fallback(input, bias, min_input, max_input, min_bias, max_bias, out_type, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function quantized_bias_add\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  out_type = _execute.make_type(out_type, \"out_type\")\n",
      "  _attr_T1, (input,) = _execute.args_to_matching_eager([input], _ctx)\n",
      "  _attr_T2, (bias,) = _execute.args_to_matching_eager([bias], _ctx)\n",
      "  min_input = _ops.convert_to_tensor(min_input, _dtypes.float32)\n",
      "  max_input = _ops.convert_to_tensor(max_input, _dtypes.float32)\n",
      "  min_bias = _ops.convert_to_tensor(min_bias, _dtypes.float32)\n",
      "  max_bias = _ops.convert_to_tensor(max_bias, _dtypes.float32)\n",
      "  _inputs_flat = [input, bias, min_input, max_input, min_bias, max_bias]\n",
      "  _attrs = (\"T1\", _attr_T1, \"T2\", _attr_T2, \"out_type\", out_type)\n",
      "  _result = _execute.execute(b\"QuantizedBiasAdd\", 3, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"QuantizedBiasAdd\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _QuantizedBiasAddOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "_quantized_conv2d_outputs = [\"output\", \"min_output\", \"max_output\"]\n",
      "_QuantizedConv2DOutput = _collections.namedtuple(\n",
      "    \"QuantizedConv2D\", _quantized_conv2d_outputs)\n",
      "\n",
      "\n",
      "@tf_export('nn.quantized_conv2d')\n",
      "def quantized_conv2d(input, filter, min_input, max_input, min_filter, max_filter, strides, padding, out_type=_dtypes.qint32, dilations=[1, 1, 1, 1], name=None):\n",
      "  r\"\"\"Computes a 2D convolution given quantized 4D input and filter tensors.\n",
      "\n",
      "  The inputs are quantized tensors where the lowest value represents the real\n",
      "  number of the associated minimum, and the highest represents the maximum.\n",
      "  This means that you can only interpret the quantized output in the same way, by\n",
      "  taking the returned minimum and maximum values into account.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n",
      "    filter: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n",
      "      filter's input_depth dimension must match input's depth dimensions.\n",
      "    min_input: A `Tensor` of type `float32`.\n",
      "      The float value that the lowest quantized input value represents.\n",
      "    max_input: A `Tensor` of type `float32`.\n",
      "      The float value that the highest quantized input value represents.\n",
      "    min_filter: A `Tensor` of type `float32`.\n",
      "      The float value that the lowest quantized filter value represents.\n",
      "    max_filter: A `Tensor` of type `float32`.\n",
      "      The float value that the highest quantized filter value represents.\n",
      "    strides: A list of `ints`.\n",
      "      The stride of the sliding window for each dimension of the input\n",
      "      tensor.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    out_type: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.qint32`.\n",
      "    dilations: An optional list of `ints`. Defaults to `[1, 1, 1, 1]`.\n",
      "      1-D tensor of length 4.  The dilation factor for each dimension of\n",
      "      `input`. If set to k > 1, there will be k-1 skipped cells between each\n",
      "      filter element on that dimension. The dimension order is determined by the\n",
      "      value of `data_format`, see above for details. Dilations in the batch and\n",
      "      depth dimensions must be 1.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (output, min_output, max_output).\n",
      "\n",
      "    output: A `Tensor` of type `out_type`.\n",
      "    min_output: A `Tensor` of type `float32`.\n",
      "    max_output: A `Tensor` of type `float32`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'quantized_conv2d' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    if out_type is None:\n",
      "      out_type = _dtypes.qint32\n",
      "    out_type = _execute.make_type(out_type, \"out_type\")\n",
      "    if dilations is None:\n",
      "      dilations = [1, 1, 1, 1]\n",
      "    if not isinstance(dilations, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'dilations' argument to \"\n",
      "          \"'quantized_conv2d' Op, not %r.\" % dilations)\n",
      "    dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"QuantizedConv2D\", input=input, filter=filter, min_input=min_input,\n",
      "        max_input=max_input, min_filter=min_filter, max_filter=max_filter,\n",
      "        strides=strides, padding=padding, out_type=out_type,\n",
      "        dilations=dilations, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"Tinput\", _op.get_attr(\"Tinput\"), \"Tfilter\",\n",
      "              _op.get_attr(\"Tfilter\"), \"out_type\", _op.get_attr(\"out_type\"),\n",
      "              \"strides\", _op.get_attr(\"strides\"), \"padding\",\n",
      "              _op.get_attr(\"padding\"), \"dilations\", _op.get_attr(\"dilations\"))\n",
      "    _execute.record_gradient(\n",
      "      \"QuantizedConv2D\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _QuantizedConv2DOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"QuantizedConv2D\", name, _ctx._post_execution_callbacks, input,\n",
      "        filter, min_input, max_input, min_filter, max_filter, \"out_type\",\n",
      "        out_type, \"strides\", strides, \"padding\", padding, \"dilations\",\n",
      "        dilations)\n",
      "      _result = _QuantizedConv2DOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return quantized_conv2d_eager_fallback(\n",
      "          input, filter, min_input, max_input, min_filter, max_filter,\n",
      "          out_type=out_type, strides=strides, padding=padding,\n",
      "          dilations=dilations, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def quantized_conv2d_eager_fallback(input, filter, min_input, max_input, min_filter, max_filter, strides, padding, out_type=_dtypes.qint32, dilations=[1, 1, 1, 1], name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function quantized_conv2d\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'quantized_conv2d' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  if out_type is None:\n",
      "    out_type = _dtypes.qint32\n",
      "  out_type = _execute.make_type(out_type, \"out_type\")\n",
      "  if dilations is None:\n",
      "    dilations = [1, 1, 1, 1]\n",
      "  if not isinstance(dilations, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'dilations' argument to \"\n",
      "        \"'quantized_conv2d' Op, not %r.\" % dilations)\n",
      "  dilations = [_execute.make_int(_i, \"dilations\") for _i in dilations]\n",
      "  _attr_Tinput, (input,) = _execute.args_to_matching_eager([input], _ctx)\n",
      "  _attr_Tfilter, (filter,) = _execute.args_to_matching_eager([filter], _ctx)\n",
      "  min_input = _ops.convert_to_tensor(min_input, _dtypes.float32)\n",
      "  max_input = _ops.convert_to_tensor(max_input, _dtypes.float32)\n",
      "  min_filter = _ops.convert_to_tensor(min_filter, _dtypes.float32)\n",
      "  max_filter = _ops.convert_to_tensor(max_filter, _dtypes.float32)\n",
      "  _inputs_flat = [input, filter, min_input, max_input, min_filter, max_filter]\n",
      "  _attrs = (\"Tinput\", _attr_Tinput, \"Tfilter\", _attr_Tfilter, \"out_type\",\n",
      "  out_type, \"strides\", strides, \"padding\", padding, \"dilations\", dilations)\n",
      "  _result = _execute.execute(b\"QuantizedConv2D\", 3, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"QuantizedConv2D\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _QuantizedConv2DOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "_quantized_max_pool_outputs = [\"output\", \"min_output\", \"max_output\"]\n",
      "_QuantizedMaxPoolOutput = _collections.namedtuple(\n",
      "    \"QuantizedMaxPool\", _quantized_max_pool_outputs)\n",
      "\n",
      "\n",
      "@tf_export('nn.quantized_max_pool')\n",
      "def quantized_max_pool(input, min_input, max_input, ksize, strides, padding, name=None):\n",
      "  r\"\"\"Produces the max pool of the input tensor for quantized types.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n",
      "      The 4D (batch x rows x cols x depth) Tensor to MaxReduce over.\n",
      "    min_input: A `Tensor` of type `float32`.\n",
      "      The float value that the lowest quantized input value represents.\n",
      "    max_input: A `Tensor` of type `float32`.\n",
      "      The float value that the highest quantized input value represents.\n",
      "    ksize: A list of `ints`.\n",
      "      The size of the window for each dimension of the input tensor.\n",
      "      The length must be 4 to match the number of dimensions of the input.\n",
      "    strides: A list of `ints`.\n",
      "      The stride of the sliding window for each dimension of the input\n",
      "      tensor. The length must be 4 to match the number of dimensions of the input.\n",
      "    padding: A `string` from: `\"SAME\", \"VALID\"`.\n",
      "      The type of padding algorithm to use.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (output, min_output, max_output).\n",
      "\n",
      "    output: A `Tensor`. Has the same type as `input`.\n",
      "    min_output: A `Tensor` of type `float32`.\n",
      "    max_output: A `Tensor` of type `float32`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if not isinstance(ksize, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'ksize' argument to \"\n",
      "          \"'quantized_max_pool' Op, not %r.\" % ksize)\n",
      "    ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "    if not isinstance(strides, (list, tuple)):\n",
      "      raise TypeError(\n",
      "          \"Expected list for 'strides' argument to \"\n",
      "          \"'quantized_max_pool' Op, not %r.\" % strides)\n",
      "    strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "    padding = _execute.make_str(padding, \"padding\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"QuantizedMaxPool\", input=input, min_input=min_input,\n",
      "        max_input=max_input, ksize=ksize, strides=strides, padding=padding,\n",
      "        name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"ksize\", _op.get_attr(\"ksize\"),\n",
      "              \"strides\", _op.get_attr(\"strides\"), \"padding\",\n",
      "              _op.get_attr(\"padding\"))\n",
      "    _execute.record_gradient(\n",
      "      \"QuantizedMaxPool\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _QuantizedMaxPoolOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"QuantizedMaxPool\", name, _ctx._post_execution_callbacks, input,\n",
      "        min_input, max_input, \"ksize\", ksize, \"strides\", strides, \"padding\",\n",
      "        padding)\n",
      "      _result = _QuantizedMaxPoolOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return quantized_max_pool_eager_fallback(\n",
      "          input, min_input, max_input, ksize=ksize, strides=strides,\n",
      "          padding=padding, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def quantized_max_pool_eager_fallback(input, min_input, max_input, ksize, strides, padding, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function quantized_max_pool\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if not isinstance(ksize, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'ksize' argument to \"\n",
      "        \"'quantized_max_pool' Op, not %r.\" % ksize)\n",
      "  ksize = [_execute.make_int(_i, \"ksize\") for _i in ksize]\n",
      "  if not isinstance(strides, (list, tuple)):\n",
      "    raise TypeError(\n",
      "        \"Expected list for 'strides' argument to \"\n",
      "        \"'quantized_max_pool' Op, not %r.\" % strides)\n",
      "  strides = [_execute.make_int(_i, \"strides\") for _i in strides]\n",
      "  padding = _execute.make_str(padding, \"padding\")\n",
      "  _attr_T, (input,) = _execute.args_to_matching_eager([input], _ctx)\n",
      "  min_input = _ops.convert_to_tensor(min_input, _dtypes.float32)\n",
      "  max_input = _ops.convert_to_tensor(max_input, _dtypes.float32)\n",
      "  _inputs_flat = [input, min_input, max_input]\n",
      "  _attrs = (\"T\", _attr_T, \"ksize\", ksize, \"strides\", strides, \"padding\",\n",
      "  padding)\n",
      "  _result = _execute.execute(b\"QuantizedMaxPool\", 3, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"QuantizedMaxPool\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _QuantizedMaxPoolOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "_quantized_relu_outputs = [\"activations\", \"min_activations\",\n",
      "                          \"max_activations\"]\n",
      "_QuantizedReluOutput = _collections.namedtuple(\n",
      "    \"QuantizedRelu\", _quantized_relu_outputs)\n",
      "\n",
      "\n",
      "def quantized_relu(features, min_features, max_features, out_type=_dtypes.quint8, name=None):\n",
      "  r\"\"\"Computes Quantized Rectified Linear: `max(features, 0)`\n",
      "\n",
      "  Args:\n",
      "    features: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n",
      "    min_features: A `Tensor` of type `float32`.\n",
      "      The float value that the lowest quantized value represents.\n",
      "    max_features: A `Tensor` of type `float32`.\n",
      "      The float value that the highest quantized value represents.\n",
      "    out_type: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.quint8`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (activations, min_activations, max_activations).\n",
      "\n",
      "    activations: A `Tensor` of type `out_type`.\n",
      "    min_activations: A `Tensor` of type `float32`.\n",
      "    max_activations: A `Tensor` of type `float32`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if out_type is None:\n",
      "      out_type = _dtypes.quint8\n",
      "    out_type = _execute.make_type(out_type, \"out_type\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"QuantizedRelu\", features=features, min_features=min_features,\n",
      "        max_features=max_features, out_type=out_type, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"Tinput\", _op.get_attr(\"Tinput\"), \"out_type\",\n",
      "              _op.get_attr(\"out_type\"))\n",
      "    _execute.record_gradient(\n",
      "      \"QuantizedRelu\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _QuantizedReluOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"QuantizedRelu\", name, _ctx._post_execution_callbacks, features,\n",
      "        min_features, max_features, \"out_type\", out_type)\n",
      "      _result = _QuantizedReluOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return quantized_relu_eager_fallback(\n",
      "          features, min_features, max_features, out_type=out_type, name=name,\n",
      "          ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def quantized_relu_eager_fallback(features, min_features, max_features, out_type=_dtypes.quint8, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function quantized_relu\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if out_type is None:\n",
      "    out_type = _dtypes.quint8\n",
      "  out_type = _execute.make_type(out_type, \"out_type\")\n",
      "  _attr_Tinput, (features,) = _execute.args_to_matching_eager([features], _ctx)\n",
      "  min_features = _ops.convert_to_tensor(min_features, _dtypes.float32)\n",
      "  max_features = _ops.convert_to_tensor(max_features, _dtypes.float32)\n",
      "  _inputs_flat = [features, min_features, max_features]\n",
      "  _attrs = (\"Tinput\", _attr_Tinput, \"out_type\", out_type)\n",
      "  _result = _execute.execute(b\"QuantizedRelu\", 3, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"QuantizedRelu\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _QuantizedReluOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "_quantized_relu6_outputs = [\"activations\", \"min_activations\",\n",
      "                           \"max_activations\"]\n",
      "_QuantizedRelu6Output = _collections.namedtuple(\n",
      "    \"QuantizedRelu6\", _quantized_relu6_outputs)\n",
      "\n",
      "\n",
      "def quantized_relu6(features, min_features, max_features, out_type=_dtypes.quint8, name=None):\n",
      "  r\"\"\"Computes Quantized Rectified Linear 6: `min(max(features, 0), 6)`\n",
      "\n",
      "  Args:\n",
      "    features: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n",
      "    min_features: A `Tensor` of type `float32`.\n",
      "      The float value that the lowest quantized value represents.\n",
      "    max_features: A `Tensor` of type `float32`.\n",
      "      The float value that the highest quantized value represents.\n",
      "    out_type: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.quint8`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (activations, min_activations, max_activations).\n",
      "\n",
      "    activations: A `Tensor` of type `out_type`.\n",
      "    min_activations: A `Tensor` of type `float32`.\n",
      "    max_activations: A `Tensor` of type `float32`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if out_type is None:\n",
      "      out_type = _dtypes.quint8\n",
      "    out_type = _execute.make_type(out_type, \"out_type\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"QuantizedRelu6\", features=features, min_features=min_features,\n",
      "        max_features=max_features, out_type=out_type, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"Tinput\", _op.get_attr(\"Tinput\"), \"out_type\",\n",
      "              _op.get_attr(\"out_type\"))\n",
      "    _execute.record_gradient(\n",
      "      \"QuantizedRelu6\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _QuantizedRelu6Output._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"QuantizedRelu6\", name, _ctx._post_execution_callbacks, features,\n",
      "        min_features, max_features, \"out_type\", out_type)\n",
      "      _result = _QuantizedRelu6Output._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return quantized_relu6_eager_fallback(\n",
      "          features, min_features, max_features, out_type=out_type, name=name,\n",
      "          ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def quantized_relu6_eager_fallback(features, min_features, max_features, out_type=_dtypes.quint8, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function quantized_relu6\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if out_type is None:\n",
      "    out_type = _dtypes.quint8\n",
      "  out_type = _execute.make_type(out_type, \"out_type\")\n",
      "  _attr_Tinput, (features,) = _execute.args_to_matching_eager([features], _ctx)\n",
      "  min_features = _ops.convert_to_tensor(min_features, _dtypes.float32)\n",
      "  max_features = _ops.convert_to_tensor(max_features, _dtypes.float32)\n",
      "  _inputs_flat = [features, min_features, max_features]\n",
      "  _attrs = (\"Tinput\", _attr_Tinput, \"out_type\", out_type)\n",
      "  _result = _execute.execute(b\"QuantizedRelu6\", 3, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"QuantizedRelu6\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _QuantizedRelu6Output._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "_quantized_relu_x_outputs = [\"activations\", \"min_activations\",\n",
      "                            \"max_activations\"]\n",
      "_QuantizedReluXOutput = _collections.namedtuple(\n",
      "    \"QuantizedReluX\", _quantized_relu_x_outputs)\n",
      "\n",
      "\n",
      "@tf_export('nn.quantized_relu_x')\n",
      "def quantized_relu_x(features, max_value, min_features, max_features, out_type=_dtypes.quint8, name=None):\n",
      "  r\"\"\"Computes Quantized Rectified Linear X: `min(max(features, 0), max_value)`\n",
      "\n",
      "  Args:\n",
      "    features: A `Tensor`. Must be one of the following types: `qint8`, `quint8`, `qint32`, `qint16`, `quint16`.\n",
      "    max_value: A `Tensor` of type `float32`.\n",
      "    min_features: A `Tensor` of type `float32`.\n",
      "      The float value that the lowest quantized value represents.\n",
      "    max_features: A `Tensor` of type `float32`.\n",
      "      The float value that the highest quantized value represents.\n",
      "    out_type: An optional `tf.DType` from: `tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16`. Defaults to `tf.quint8`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (activations, min_activations, max_activations).\n",
      "\n",
      "    activations: A `Tensor` of type `out_type`.\n",
      "    min_activations: A `Tensor` of type `float32`.\n",
      "    max_activations: A `Tensor` of type `float32`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if out_type is None:\n",
      "      out_type = _dtypes.quint8\n",
      "    out_type = _execute.make_type(out_type, \"out_type\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"QuantizedReluX\", features=features, max_value=max_value,\n",
      "        min_features=min_features, max_features=max_features,\n",
      "        out_type=out_type, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"Tinput\", _op.get_attr(\"Tinput\"), \"out_type\",\n",
      "              _op.get_attr(\"out_type\"))\n",
      "    _execute.record_gradient(\n",
      "      \"QuantizedReluX\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _QuantizedReluXOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"QuantizedReluX\", name, _ctx._post_execution_callbacks, features,\n",
      "        max_value, min_features, max_features, \"out_type\", out_type)\n",
      "      _result = _QuantizedReluXOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return quantized_relu_x_eager_fallback(\n",
      "          features, max_value, min_features, max_features, out_type=out_type,\n",
      "          name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def quantized_relu_x_eager_fallback(features, max_value, min_features, max_features, out_type=_dtypes.quint8, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function quantized_relu_x\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if out_type is None:\n",
      "    out_type = _dtypes.quint8\n",
      "  out_type = _execute.make_type(out_type, \"out_type\")\n",
      "  _attr_Tinput, (features,) = _execute.args_to_matching_eager([features], _ctx)\n",
      "  max_value = _ops.convert_to_tensor(max_value, _dtypes.float32)\n",
      "  min_features = _ops.convert_to_tensor(min_features, _dtypes.float32)\n",
      "  max_features = _ops.convert_to_tensor(max_features, _dtypes.float32)\n",
      "  _inputs_flat = [features, max_value, min_features, max_features]\n",
      "  _attrs = (\"Tinput\", _attr_Tinput, \"out_type\", out_type)\n",
      "  _result = _execute.execute(b\"QuantizedReluX\", 3, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"QuantizedReluX\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _QuantizedReluXOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.relu')\n",
      "def relu(features, name=None):\n",
      "  r\"\"\"Computes rectified linear: `max(features, 0)`.\n",
      "\n",
      "  Args:\n",
      "    features: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`, `qint8`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `features`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Relu\", features=features, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Relu\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"Relu\", name,\n",
      "        _ctx._post_execution_callbacks, features)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return relu_eager_fallback(\n",
      "          features, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def relu_eager_fallback(features, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function relu\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, (features,) = _execute.args_to_matching_eager([features], _ctx)\n",
      "  _inputs_flat = [features]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"Relu\", 1, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Relu\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def relu6(features, name=None):\n",
      "  r\"\"\"Computes rectified linear 6: `min(max(features, 0), 6)`.\n",
      "\n",
      "  Args:\n",
      "    features: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `features`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Relu6\", features=features, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Relu6\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"Relu6\", name,\n",
      "        _ctx._post_execution_callbacks, features)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return relu6_eager_fallback(\n",
      "          features, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def relu6_eager_fallback(features, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function relu6\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, (features,) = _execute.args_to_matching_eager([features], _ctx)\n",
      "  _inputs_flat = [features]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"Relu6\", 1, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Relu6\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def relu6_grad(gradients, features, name=None):\n",
      "  r\"\"\"Computes rectified linear 6 gradients for a Relu6 operation.\n",
      "\n",
      "  Args:\n",
      "    gradients: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      The backpropagated gradients to the corresponding Relu6 operation.\n",
      "    features: A `Tensor`. Must have the same type as `gradients`.\n",
      "      The features passed as input to the corresponding Relu6 operation, or\n",
      "      its output; using either one produces the same result.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `gradients`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Relu6Grad\", gradients=gradients, features=features, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Relu6Grad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"Relu6Grad\",\n",
      "        name, _ctx._post_execution_callbacks, gradients, features)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return relu6_grad_eager_fallback(\n",
      "          gradients, features, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def relu6_grad_eager_fallback(gradients, features, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function relu6_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([gradients, features], _ctx)\n",
      "  (gradients, features) = _inputs_T\n",
      "  _inputs_flat = [gradients, features]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"Relu6Grad\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Relu6Grad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def relu_grad(gradients, features, name=None):\n",
      "  r\"\"\"Computes rectified linear gradients for a Relu operation.\n",
      "\n",
      "  Args:\n",
      "    gradients: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      The backpropagated gradients to the corresponding Relu operation.\n",
      "    features: A `Tensor`. Must have the same type as `gradients`.\n",
      "      The features passed as input to the corresponding Relu operation, OR\n",
      "      the outputs of that operation (both work equivalently).\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `gradients`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"ReluGrad\", gradients=gradients, features=features, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"ReluGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"ReluGrad\",\n",
      "        name, _ctx._post_execution_callbacks, gradients, features)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return relu_grad_eager_fallback(\n",
      "          gradients, features, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def relu_grad_eager_fallback(gradients, features, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function relu_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([gradients, features], _ctx)\n",
      "  (gradients, features) = _inputs_T\n",
      "  _inputs_flat = [gradients, features]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"ReluGrad\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"ReluGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.selu')\n",
      "def selu(features, name=None):\n",
      "  r\"\"\"Computes scaled exponential linear: `scale * alpha * (exp(features) - 1)`\n",
      "\n",
      "  if < 0, `scale * features` otherwise.\n",
      "\n",
      "  To be used together with\n",
      "  `initializer = tf.variance_scaling_initializer(factor=1.0, mode='FAN_IN')`.\n",
      "  For correct dropout, use `tf.contrib.nn.alpha_dropout`.\n",
      "\n",
      "  See [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n",
      "\n",
      "  Args:\n",
      "    features: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `features`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Selu\", features=features, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Selu\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"Selu\", name,\n",
      "        _ctx._post_execution_callbacks, features)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return selu_eager_fallback(\n",
      "          features, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def selu_eager_fallback(features, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function selu\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, (features,) = _execute.args_to_matching_eager([features], _ctx)\n",
      "  _inputs_flat = [features]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"Selu\", 1, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Selu\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def selu_grad(gradients, outputs, name=None):\n",
      "  r\"\"\"Computes gradients for the scaled exponential linear (Selu) operation.\n",
      "\n",
      "  Args:\n",
      "    gradients: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      The backpropagated gradients to the corresponding Selu operation.\n",
      "    outputs: A `Tensor`. Must have the same type as `gradients`.\n",
      "      The outputs of the corresponding Selu operation.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `gradients`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"SeluGrad\", gradients=gradients, outputs=outputs, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"SeluGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"SeluGrad\",\n",
      "        name, _ctx._post_execution_callbacks, gradients, outputs)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return selu_grad_eager_fallback(\n",
      "          gradients, outputs, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def selu_grad_eager_fallback(gradients, outputs, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function selu_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([gradients, outputs], _ctx)\n",
      "  (gradients, outputs) = _inputs_T\n",
      "  _inputs_flat = [gradients, outputs]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"SeluGrad\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"SeluGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def softmax(logits, name=None):\n",
      "  r\"\"\"Computes softmax activations.\n",
      "\n",
      "  For each batch `i` and class `j` we have\n",
      "\n",
      "      $$softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))$$\n",
      "\n",
      "  Args:\n",
      "    logits: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      2-D with shape `[batch_size, num_classes]`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `logits`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Softmax\", logits=logits, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Softmax\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"Softmax\",\n",
      "        name, _ctx._post_execution_callbacks, logits)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return softmax_eager_fallback(\n",
      "          logits, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def softmax_eager_fallback(logits, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function softmax\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, (logits,) = _execute.args_to_matching_eager([logits], _ctx)\n",
      "  _inputs_flat = [logits]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"Softmax\", 1, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Softmax\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "_softmax_cross_entropy_with_logits_outputs = [\"loss\", \"backprop\"]\n",
      "_SoftmaxCrossEntropyWithLogitsOutput = _collections.namedtuple(\n",
      "    \"SoftmaxCrossEntropyWithLogits\",\n",
      "    _softmax_cross_entropy_with_logits_outputs)\n",
      "\n",
      "\n",
      "def softmax_cross_entropy_with_logits(features, labels, name=None):\n",
      "  r\"\"\"Computes softmax cross entropy cost and gradients to backpropagate.\n",
      "\n",
      "  Inputs are the logits, not probabilities.\n",
      "\n",
      "  Args:\n",
      "    features: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      batch_size x num_classes matrix\n",
      "    labels: A `Tensor`. Must have the same type as `features`.\n",
      "      batch_size x num_classes matrix\n",
      "      The caller must ensure that each batch of labels represents a valid\n",
      "      probability distribution.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (loss, backprop).\n",
      "\n",
      "    loss: A `Tensor`. Has the same type as `features`.\n",
      "    backprop: A `Tensor`. Has the same type as `features`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"SoftmaxCrossEntropyWithLogits\", features=features, labels=labels,\n",
      "        name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"SoftmaxCrossEntropyWithLogits\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _SoftmaxCrossEntropyWithLogitsOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"SoftmaxCrossEntropyWithLogits\", name, _ctx._post_execution_callbacks,\n",
      "        features, labels)\n",
      "      _result = _SoftmaxCrossEntropyWithLogitsOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return softmax_cross_entropy_with_logits_eager_fallback(\n",
      "          features, labels, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def softmax_cross_entropy_with_logits_eager_fallback(features, labels, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function softmax_cross_entropy_with_logits\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([features, labels], _ctx)\n",
      "  (features, labels) = _inputs_T\n",
      "  _inputs_flat = [features, labels]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"SoftmaxCrossEntropyWithLogits\", 2,\n",
      "                             inputs=_inputs_flat, attrs=_attrs, ctx=_ctx,\n",
      "                             name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"SoftmaxCrossEntropyWithLogits\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _SoftmaxCrossEntropyWithLogitsOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('math.softplus', 'nn.softplus')\n",
      "def softplus(features, name=None):\n",
      "  r\"\"\"Computes softplus: `log(exp(features) + 1)`.\n",
      "\n",
      "  Args:\n",
      "    features: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `features`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Softplus\", features=features, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Softplus\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"Softplus\",\n",
      "        name, _ctx._post_execution_callbacks, features)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return softplus_eager_fallback(\n",
      "          features, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def softplus_eager_fallback(features, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function softplus\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, (features,) = _execute.args_to_matching_eager([features], _ctx)\n",
      "  _inputs_flat = [features]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"Softplus\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Softplus\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def softplus_grad(gradients, features, name=None):\n",
      "  r\"\"\"Computes softplus gradients for a softplus operation.\n",
      "\n",
      "  Args:\n",
      "    gradients: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      The backpropagated gradients to the corresponding softplus operation.\n",
      "    features: A `Tensor`. Must have the same type as `gradients`.\n",
      "      The features passed as input to the corresponding softplus operation.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `gradients`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"SoftplusGrad\", gradients=gradients, features=features, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"SoftplusGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"SoftplusGrad\",\n",
      "        name, _ctx._post_execution_callbacks, gradients, features)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return softplus_grad_eager_fallback(\n",
      "          gradients, features, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def softplus_grad_eager_fallback(gradients, features, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function softplus_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([gradients, features], _ctx)\n",
      "  (gradients, features) = _inputs_T\n",
      "  _inputs_flat = [gradients, features]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"SoftplusGrad\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"SoftplusGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "@tf_export('nn.softsign', 'math.softsign')\n",
      "def softsign(features, name=None):\n",
      "  r\"\"\"Computes softsign: `features / (abs(features) + 1)`.\n",
      "\n",
      "  Args:\n",
      "    features: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `features`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"Softsign\", features=features, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"Softsign\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"Softsign\",\n",
      "        name, _ctx._post_execution_callbacks, features)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return softsign_eager_fallback(\n",
      "          features, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def softsign_eager_fallback(features, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function softsign\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, (features,) = _execute.args_to_matching_eager([features], _ctx)\n",
      "  _inputs_flat = [features]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"Softsign\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"Softsign\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "def softsign_grad(gradients, features, name=None):\n",
      "  r\"\"\"Computes softsign gradients for a softsign operation.\n",
      "\n",
      "  Args:\n",
      "    gradients: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      The backpropagated gradients to the corresponding softsign operation.\n",
      "    features: A `Tensor`. Must have the same type as `gradients`.\n",
      "      The features passed as input to the corresponding softsign operation.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A `Tensor`. Has the same type as `gradients`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"SoftsignGrad\", gradients=gradients, features=features, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"SoftsignGrad\", _inputs_flat, _attrs, _result, name)\n",
      "    _result, = _result\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"SoftsignGrad\",\n",
      "        name, _ctx._post_execution_callbacks, gradients, features)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return softsign_grad_eager_fallback(\n",
      "          gradients, features, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def softsign_grad_eager_fallback(gradients, features, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function softsign_grad\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, _inputs_T = _execute.args_to_matching_eager([gradients, features], _ctx)\n",
      "  (gradients, features) = _inputs_T\n",
      "  _inputs_flat = [gradients, features]\n",
      "  _attrs = (\"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"SoftsignGrad\", 1, inputs=_inputs_flat,\n",
      "                             attrs=_attrs, ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"SoftsignGrad\", _inputs_flat, _attrs, _result, name)\n",
      "  _result, = _result\n",
      "  return _result\n",
      "\n",
      "\n",
      "_sparse_softmax_cross_entropy_with_logits_outputs = [\"loss\", \"backprop\"]\n",
      "_SparseSoftmaxCrossEntropyWithLogitsOutput = _collections.namedtuple(\n",
      "    \"SparseSoftmaxCrossEntropyWithLogits\",\n",
      "    _sparse_softmax_cross_entropy_with_logits_outputs)\n",
      "\n",
      "\n",
      "def sparse_softmax_cross_entropy_with_logits(features, labels, name=None):\n",
      "  r\"\"\"Computes softmax cross entropy cost and gradients to backpropagate.\n",
      "\n",
      "  Unlike `SoftmaxCrossEntropyWithLogits`, this operation does not accept\n",
      "  a matrix of label probabilities, but rather a single label per row\n",
      "  of features.  This label is considered to have probability 1.0 for the\n",
      "  given row.\n",
      "\n",
      "  Inputs are the logits, not probabilities.\n",
      "\n",
      "  Args:\n",
      "    features: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`.\n",
      "      batch_size x num_classes matrix\n",
      "    labels: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "      batch_size vector with values in [0, num_classes).\n",
      "      This is the label for the given minibatch entry.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (loss, backprop).\n",
      "\n",
      "    loss: A `Tensor`. Has the same type as `features`.\n",
      "    backprop: A `Tensor`. Has the same type as `features`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"SparseSoftmaxCrossEntropyWithLogits\", features=features,\n",
      "        labels=labels, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"T\", _op.get_attr(\"T\"), \"Tlabels\", _op.get_attr(\"Tlabels\"))\n",
      "    _execute.record_gradient(\n",
      "      \"SparseSoftmaxCrossEntropyWithLogits\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _SparseSoftmaxCrossEntropyWithLogitsOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name,\n",
      "        \"SparseSoftmaxCrossEntropyWithLogits\", name,\n",
      "        _ctx._post_execution_callbacks, features, labels)\n",
      "      _result = _SparseSoftmaxCrossEntropyWithLogitsOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return sparse_softmax_cross_entropy_with_logits_eager_fallback(\n",
      "          features, labels, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def sparse_softmax_cross_entropy_with_logits_eager_fallback(features, labels, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function sparse_softmax_cross_entropy_with_logits\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  _attr_T, (features,) = _execute.args_to_matching_eager([features], _ctx)\n",
      "  _attr_Tlabels, (labels,) = _execute.args_to_matching_eager([labels], _ctx, _dtypes.int64)\n",
      "  _inputs_flat = [features, labels]\n",
      "  _attrs = (\"T\", _attr_T, \"Tlabels\", _attr_Tlabels)\n",
      "  _result = _execute.execute(b\"SparseSoftmaxCrossEntropyWithLogits\", 2,\n",
      "                             inputs=_inputs_flat, attrs=_attrs, ctx=_ctx,\n",
      "                             name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"SparseSoftmaxCrossEntropyWithLogits\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _SparseSoftmaxCrossEntropyWithLogitsOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "_top_k_outputs = [\"values\", \"indices\"]\n",
      "_TopKOutput = _collections.namedtuple(\n",
      "    \"TopK\", _top_k_outputs)\n",
      "\n",
      "\n",
      "def top_k(input, k, sorted=True, name=None):\n",
      "  r\"\"\"Finds values and indices of the `k` largest elements for the last dimension.\n",
      "\n",
      "  If the input is a vector (rank-1), finds the `k` largest entries in the vector\n",
      "  and outputs their values and indices as vectors.  Thus `values[j]` is the\n",
      "  `j`-th largest entry in `input`, and its index is `indices[j]`.\n",
      "\n",
      "  For matrices (resp. higher rank input), computes the top `k` entries in each\n",
      "  row (resp. vector along the last dimension).  Thus,\n",
      "\n",
      "      values.shape = indices.shape = input.shape[:-1] + [k]\n",
      "\n",
      "  If two elements are equal, the lower-index element appears first.\n",
      "\n",
      "  If `k` varies dynamically, use `TopKV2` below.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      1-D or higher with last dimension at least `k`.\n",
      "    k: An `int` that is `>= 0`.\n",
      "      Number of top elements to look for along the last dimension (along each\n",
      "      row for matrices).\n",
      "    sorted: An optional `bool`. Defaults to `True`.\n",
      "      If true the resulting `k` elements will be sorted by the values in\n",
      "      descending order.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (values, indices).\n",
      "\n",
      "    values: A `Tensor`. Has the same type as `input`.\n",
      "    indices: A `Tensor` of type `int32`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    k = _execute.make_int(k, \"k\")\n",
      "    if sorted is None:\n",
      "      sorted = True\n",
      "    sorted = _execute.make_bool(sorted, \"sorted\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"TopK\", input=input, k=k, sorted=sorted, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"k\", _op.get_attr(\"k\"), \"sorted\", _op.get_attr(\"sorted\"), \"T\",\n",
      "              _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"TopK\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _TopKOutput._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"TopK\", name,\n",
      "        _ctx._post_execution_callbacks, input, \"k\", k, \"sorted\", sorted)\n",
      "      _result = _TopKOutput._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return top_k_eager_fallback(\n",
      "          input, k=k, sorted=sorted, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def top_k_eager_fallback(input, k, sorted=True, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function top_k\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  k = _execute.make_int(k, \"k\")\n",
      "  if sorted is None:\n",
      "    sorted = True\n",
      "  sorted = _execute.make_bool(sorted, \"sorted\")\n",
      "  _attr_T, (input,) = _execute.args_to_matching_eager([input], _ctx)\n",
      "  _inputs_flat = [input]\n",
      "  _attrs = (\"k\", k, \"sorted\", sorted, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"TopK\", 2, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"TopK\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _TopKOutput._make(_result)\n",
      "  return _result\n",
      "\n",
      "\n",
      "_top_kv2_outputs = [\"values\", \"indices\"]\n",
      "_TopKV2Output = _collections.namedtuple(\n",
      "    \"TopKV2\", _top_kv2_outputs)\n",
      "\n",
      "\n",
      "def top_kv2(input, k, sorted=True, name=None):\n",
      "  r\"\"\"Finds values and indices of the `k` largest elements for the last dimension.\n",
      "\n",
      "  If the input is a vector (rank-1), finds the `k` largest entries in the vector\n",
      "  and outputs their values and indices as vectors.  Thus `values[j]` is the\n",
      "  `j`-th largest entry in `input`, and its index is `indices[j]`.\n",
      "\n",
      "  For matrices (resp. higher rank input), computes the top `k` entries in each\n",
      "  row (resp. vector along the last dimension).  Thus,\n",
      "\n",
      "      values.shape = indices.shape = input.shape[:-1] + [k]\n",
      "\n",
      "  If two elements are equal, the lower-index element appears first.\n",
      "\n",
      "  Args:\n",
      "    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      "      1-D or higher with last dimension at least `k`.\n",
      "    k: A `Tensor` of type `int32`.\n",
      "      0-D.  Number of top elements to look for along the last dimension (along each\n",
      "      row for matrices).\n",
      "    sorted: An optional `bool`. Defaults to `True`.\n",
      "      If true the resulting `k` elements will be sorted by the values in\n",
      "      descending order.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A tuple of `Tensor` objects (values, indices).\n",
      "\n",
      "    values: A `Tensor`. Has the same type as `input`.\n",
      "    indices: A `Tensor` of type `int32`.\n",
      "  \"\"\"\n",
      "  _ctx = _context._context\n",
      "  if _ctx is None or not _ctx._eager_context.is_eager:\n",
      "    if sorted is None:\n",
      "      sorted = True\n",
      "    sorted = _execute.make_bool(sorted, \"sorted\")\n",
      "    _, _, _op = _op_def_lib._apply_op_helper(\n",
      "        \"TopKV2\", input=input, k=k, sorted=sorted, name=name)\n",
      "    _result = _op.outputs[:]\n",
      "    _inputs_flat = _op.inputs\n",
      "    _attrs = (\"sorted\", _op.get_attr(\"sorted\"), \"T\", _op.get_attr(\"T\"))\n",
      "    _execute.record_gradient(\n",
      "      \"TopKV2\", _inputs_flat, _attrs, _result, name)\n",
      "    _result = _TopKV2Output._make(_result)\n",
      "    return _result\n",
      "\n",
      "  else:\n",
      "    try:\n",
      "      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n",
      "        _ctx._context_handle, _ctx._eager_context.device_name, \"TopKV2\", name,\n",
      "        _ctx._post_execution_callbacks, input, k, \"sorted\", sorted)\n",
      "      _result = _TopKV2Output._make(_result)\n",
      "      return _result\n",
      "    except _core._FallbackException:\n",
      "      return top_kv2_eager_fallback(\n",
      "          input, k, sorted=sorted, name=name, ctx=_ctx)\n",
      "    except _core._NotOkStatusException as e:\n",
      "      if name is not None:\n",
      "        message = e.message + \" name: \" + name\n",
      "      else:\n",
      "        message = e.message\n",
      "      _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "\n",
      "\n",
      "def top_kv2_eager_fallback(input, k, sorted=True, name=None, ctx=None):\n",
      "  r\"\"\"This is the slowpath function for Eager mode.\n",
      "  This is for function top_kv2\n",
      "  \"\"\"\n",
      "  _ctx = ctx if ctx else _context.context()\n",
      "  if sorted is None:\n",
      "    sorted = True\n",
      "  sorted = _execute.make_bool(sorted, \"sorted\")\n",
      "  _attr_T, (input,) = _execute.args_to_matching_eager([input], _ctx)\n",
      "  k = _ops.convert_to_tensor(k, _dtypes.int32)\n",
      "  _inputs_flat = [input, k]\n",
      "  _attrs = (\"sorted\", sorted, \"T\", _attr_T)\n",
      "  _result = _execute.execute(b\"TopKV2\", 2, inputs=_inputs_flat, attrs=_attrs,\n",
      "                             ctx=_ctx, name=name)\n",
      "  _execute.record_gradient(\n",
      "      \"TopKV2\", _inputs_flat, _attrs, _result, name)\n",
      "  _result = _TopKV2Output._make(_result)\n",
      "  return _result\n",
      "\n",
      "def _InitOpDefLibrary(op_list_proto_bytes):\n",
      "  op_list = _op_def_pb2.OpList()\n",
      "  op_list.ParseFromString(op_list_proto_bytes)\n",
      "  _op_def_registry.register_op_list(op_list)\n",
      "  op_def_lib = _op_def_library.OpDefLibrary()\n",
      "  op_def_lib.add_op_list(op_list)\n",
      "  return op_def_lib\n",
      "# op {\n",
      "#   name: \"AvgPool\"\n",
      "#   input_arg {\n",
      "#     name: \"value\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"ksize\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"AvgPool3D\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"ksize\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 5\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 5\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NDHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NDHWC\"\n",
      "#         s: \"NCDHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"AvgPool3DGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"orig_input_shape\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"grad\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"ksize\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 5\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 5\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NDHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NDHWC\"\n",
      "#         s: \"NCDHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"AvgPoolGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"orig_input_shape\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"grad\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"ksize\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"BatchNormWithGlobalNormalization\"\n",
      "#   input_arg {\n",
      "#     name: \"t\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"m\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"v\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"beta\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"gamma\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"result\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_COMPLEX64\n",
      "#         type: DT_INT64\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_COMPLEX128\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"variance_epsilon\"\n",
      "#     type: \"float\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"scale_after_normalization\"\n",
      "#     type: \"bool\"\n",
      "#   }\n",
      "#   deprecation {\n",
      "#     version: 9\n",
      "#     explanation: \"Use tf.nn.batch_normalization()\"\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"BatchNormWithGlobalNormalizationGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"t\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"m\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"v\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"gamma\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"dx\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"dm\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"dv\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"db\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"dg\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_COMPLEX64\n",
      "#         type: DT_INT64\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_COMPLEX128\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"variance_epsilon\"\n",
      "#     type: \"float\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"scale_after_normalization\"\n",
      "#     type: \"bool\"\n",
      "#   }\n",
      "#   deprecation {\n",
      "#     version: 9\n",
      "#     explanation: \"Use tf.nn.batch_normalization()\"\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"BiasAdd\"\n",
      "#   input_arg {\n",
      "#     name: \"value\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"bias\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_COMPLEX64\n",
      "#         type: DT_INT64\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_COMPLEX128\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"BiasAddGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"out_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_COMPLEX64\n",
      "#         type: DT_INT64\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_COMPLEX128\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"BiasAddV1\"\n",
      "#   input_arg {\n",
      "#     name: \"value\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"bias\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_COMPLEX64\n",
      "#         type: DT_INT64\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_COMPLEX128\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Conv2D\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"use_cudnn_on_gpu\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: true\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"dilations\"\n",
      "#     type: \"list(int)\"\n",
      "#     default_value {\n",
      "#       list {\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Conv2DBackpropFilter\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter_sizes\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"out_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"use_cudnn_on_gpu\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: true\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"dilations\"\n",
      "#     type: \"list(int)\"\n",
      "#     default_value {\n",
      "#       list {\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Conv2DBackpropInput\"\n",
      "#   input_arg {\n",
      "#     name: \"input_sizes\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"out_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"use_cudnn_on_gpu\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: true\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"dilations\"\n",
      "#     type: \"list(int)\"\n",
      "#     default_value {\n",
      "#       list {\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Conv3D\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 5\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NDHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NDHWC\"\n",
      "#         s: \"NCDHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"dilations\"\n",
      "#     type: \"list(int)\"\n",
      "#     default_value {\n",
      "#       list {\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Conv3DBackpropFilter\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"out_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 5\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"dilations\"\n",
      "#     type: \"list(int)\"\n",
      "#     default_value {\n",
      "#       list {\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   deprecation {\n",
      "#     version: 10\n",
      "#     explanation: \"Use Conv3DBackpropFilterV2\"\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Conv3DBackpropFilterV2\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter_sizes\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"out_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 5\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NDHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NDHWC\"\n",
      "#         s: \"NCDHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"dilations\"\n",
      "#     type: \"list(int)\"\n",
      "#     default_value {\n",
      "#       list {\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Conv3DBackpropInput\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"out_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 5\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"dilations\"\n",
      "#     type: \"list(int)\"\n",
      "#     default_value {\n",
      "#       list {\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   deprecation {\n",
      "#     version: 10\n",
      "#     explanation: \"Use Conv3DBackpropInputV2\"\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Conv3DBackpropInputV2\"\n",
      "#   input_arg {\n",
      "#     name: \"input_sizes\"\n",
      "#     type_attr: \"Tshape\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"out_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 5\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NDHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NDHWC\"\n",
      "#         s: \"NCDHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"dilations\"\n",
      "#     type: \"list(int)\"\n",
      "#     default_value {\n",
      "#       list {\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"Tshape\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_INT32\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_INT32\n",
      "#         type: DT_INT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"DataFormatDimMap\"\n",
      "#   input_arg {\n",
      "#     name: \"x\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"y\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_INT32\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_INT32\n",
      "#         type: DT_INT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"src_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"dst_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NCHW\"\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"DataFormatVecPermute\"\n",
      "#   input_arg {\n",
      "#     name: \"x\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"y\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_INT32\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_INT32\n",
      "#         type: DT_INT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"src_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"dst_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NCHW\"\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"DepthwiseConv2dNative\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"dilations\"\n",
      "#     type: \"list(int)\"\n",
      "#     default_value {\n",
      "#       list {\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"DepthwiseConv2dNativeBackpropFilter\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter_sizes\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"out_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"dilations\"\n",
      "#     type: \"list(int)\"\n",
      "#     default_value {\n",
      "#       list {\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"DepthwiseConv2dNativeBackpropInput\"\n",
      "#   input_arg {\n",
      "#     name: \"input_sizes\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"out_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"dilations\"\n",
      "#     type: \"list(int)\"\n",
      "#     default_value {\n",
      "#       list {\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Dilation2D\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"rates\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Dilation2DBackpropFilter\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"out_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"filter_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"rates\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Dilation2DBackpropInput\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"out_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"in_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"rates\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Elu\"\n",
      "#   input_arg {\n",
      "#     name: \"features\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"activations\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"EluGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"gradients\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"outputs\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"backprops\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"FractionalAvgPool\"\n",
      "#   input_arg {\n",
      "#     name: \"value\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"row_pooling_sequence\"\n",
      "#     type: DT_INT64\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"col_pooling_sequence\"\n",
      "#     type: DT_INT64\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"pooling_ratio\"\n",
      "#     type: \"list(float)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"pseudo_random\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: false\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"overlapping\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: false\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"deterministic\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: false\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"seed\"\n",
      "#     type: \"int\"\n",
      "#     default_value {\n",
      "#       i: 0\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"seed2\"\n",
      "#     type: \"int\"\n",
      "#     default_value {\n",
      "#       i: 0\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_INT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"FractionalAvgPoolGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"orig_input_tensor_shape\"\n",
      "#     type: DT_INT64\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"out_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"row_pooling_sequence\"\n",
      "#     type: DT_INT64\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"col_pooling_sequence\"\n",
      "#     type: DT_INT64\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"overlapping\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: false\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_INT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"FractionalMaxPool\"\n",
      "#   input_arg {\n",
      "#     name: \"value\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"row_pooling_sequence\"\n",
      "#     type: DT_INT64\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"col_pooling_sequence\"\n",
      "#     type: DT_INT64\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"pooling_ratio\"\n",
      "#     type: \"list(float)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"pseudo_random\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: false\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"overlapping\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: false\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"deterministic\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: false\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"seed\"\n",
      "#     type: \"int\"\n",
      "#     default_value {\n",
      "#       i: 0\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"seed2\"\n",
      "#     type: \"int\"\n",
      "#     default_value {\n",
      "#       i: 0\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_INT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"FractionalMaxPoolGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"orig_input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"orig_output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"out_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"row_pooling_sequence\"\n",
      "#     type: DT_INT64\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"col_pooling_sequence\"\n",
      "#     type: DT_INT64\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"overlapping\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: false\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_INT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"FusedBatchNorm\"\n",
      "#   input_arg {\n",
      "#     name: \"x\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"scale\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"offset\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"mean\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"variance\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"y\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"batch_mean\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"batch_variance\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"reserve_space_1\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"reserve_space_2\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"epsilon\"\n",
      "#     type: \"float\"\n",
      "#     default_value {\n",
      "#       f: 0.0001\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"is_training\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: true\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"FusedBatchNormGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"y_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"x\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"scale\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"reserve_space_1\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"reserve_space_2\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"x_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"scale_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"offset_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"reserve_space_3\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"reserve_space_4\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"epsilon\"\n",
      "#     type: \"float\"\n",
      "#     default_value {\n",
      "#       f: 0.0001\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"is_training\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: true\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"FusedBatchNormGradV2\"\n",
      "#   input_arg {\n",
      "#     name: \"y_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"x\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"scale\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"reserve_space_1\"\n",
      "#     type_attr: \"U\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"reserve_space_2\"\n",
      "#     type_attr: \"U\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"x_backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"scale_backprop\"\n",
      "#     type_attr: \"U\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"offset_backprop\"\n",
      "#     type_attr: \"U\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"reserve_space_3\"\n",
      "#     type_attr: \"U\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"reserve_space_4\"\n",
      "#     type_attr: \"U\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"U\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"epsilon\"\n",
      "#     type: \"float\"\n",
      "#     default_value {\n",
      "#       f: 0.0001\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"is_training\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: true\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"FusedBatchNormV2\"\n",
      "#   input_arg {\n",
      "#     name: \"x\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"scale\"\n",
      "#     type_attr: \"U\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"offset\"\n",
      "#     type_attr: \"U\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"mean\"\n",
      "#     type_attr: \"U\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"variance\"\n",
      "#     type_attr: \"U\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"y\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"batch_mean\"\n",
      "#     type_attr: \"U\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"batch_variance\"\n",
      "#     type_attr: \"U\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"reserve_space_1\"\n",
      "#     type_attr: \"U\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"reserve_space_2\"\n",
      "#     type_attr: \"U\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"U\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"epsilon\"\n",
      "#     type: \"float\"\n",
      "#     default_value {\n",
      "#       f: 0.0001\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"is_training\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: true\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"FusedPadConv2D\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"paddings\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"mode\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"REFLECT\"\n",
      "#         s: \"SYMMETRIC\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"FusedResizeAndPadConv2D\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"size\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"paddings\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"resize_align_corners\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: false\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"mode\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"REFLECT\"\n",
      "#         s: \"SYMMETRIC\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"InTopK\"\n",
      "#   input_arg {\n",
      "#     name: \"predictions\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"targets\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"precision\"\n",
      "#     type: DT_BOOL\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"k\"\n",
      "#     type: \"int\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_INT32\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_INT32\n",
      "#         type: DT_INT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"InTopKV2\"\n",
      "#   input_arg {\n",
      "#     name: \"predictions\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"targets\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"k\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"precision\"\n",
      "#     type: DT_BOOL\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_INT32\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_INT32\n",
      "#         type: DT_INT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"L2Loss\"\n",
      "#   input_arg {\n",
      "#     name: \"t\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"LRN\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"depth_radius\"\n",
      "#     type: \"int\"\n",
      "#     default_value {\n",
      "#       i: 5\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"bias\"\n",
      "#     type: \"float\"\n",
      "#     default_value {\n",
      "#       f: 1\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"alpha\"\n",
      "#     type: \"float\"\n",
      "#     default_value {\n",
      "#       f: 1\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"beta\"\n",
      "#     type: \"float\"\n",
      "#     default_value {\n",
      "#       f: 0.5\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_FLOAT\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"LRNGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"input_grads\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"input_image\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"output_image\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"depth_radius\"\n",
      "#     type: \"int\"\n",
      "#     default_value {\n",
      "#       i: 5\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"bias\"\n",
      "#     type: \"float\"\n",
      "#     default_value {\n",
      "#       f: 1\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"alpha\"\n",
      "#     type: \"float\"\n",
      "#     default_value {\n",
      "#       f: 1\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"beta\"\n",
      "#     type: \"float\"\n",
      "#     default_value {\n",
      "#       f: 0.5\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_FLOAT\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"LogSoftmax\"\n",
      "#   input_arg {\n",
      "#     name: \"logits\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"logsoftmax\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"MaxPool\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_FLOAT\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_INT64\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_QINT8\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"ksize\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#         s: \"NCHW_VECT_C\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"MaxPool3D\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"ksize\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 5\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 5\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NDHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NDHWC\"\n",
      "#         s: \"NCDHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"MaxPool3DGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"orig_input\"\n",
      "#     type_attr: \"TInput\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"orig_output\"\n",
      "#     type_attr: \"TInput\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"grad\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"ksize\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 5\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 5\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NDHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NDHWC\"\n",
      "#         s: \"NCDHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_FLOAT\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"TInput\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_FLOAT\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"MaxPool3DGradGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"orig_input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"orig_output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"grad\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"ksize\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 5\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 5\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NDHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NDHWC\"\n",
      "#         s: \"NCDHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"MaxPoolGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"orig_input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"orig_output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"grad\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"ksize\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_FLOAT\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"MaxPoolGradGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"orig_input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"orig_output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"grad\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"ksize\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"MaxPoolGradGradV2\"\n",
      "#   input_arg {\n",
      "#     name: \"orig_input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"orig_output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"grad\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"ksize\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"strides\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"MaxPoolGradGradWithArgmax\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"grad\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"argmax\"\n",
      "#     type_attr: \"Targmax\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"ksize\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"Targmax\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_INT32\n",
      "#         type: DT_INT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"MaxPoolGradV2\"\n",
      "#   input_arg {\n",
      "#     name: \"orig_input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"orig_output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"grad\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"ksize\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"strides\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_FLOAT\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"MaxPoolGradWithArgmax\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"grad\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"argmax\"\n",
      "#     type_attr: \"Targmax\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"ksize\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"Targmax\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_INT32\n",
      "#         type: DT_INT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"MaxPoolV2\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"ksize\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"strides\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_FLOAT\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_INT64\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_QINT8\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"data_format\"\n",
      "#     type: \"string\"\n",
      "#     default_value {\n",
      "#       s: \"NHWC\"\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"NHWC\"\n",
      "#         s: \"NCHW\"\n",
      "#         s: \"NCHW_VECT_C\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"MaxPoolWithArgmax\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"argmax\"\n",
      "#     type_attr: \"Targmax\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"ksize\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#     has_minimum: true\n",
      "#     minimum: 4\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"Targmax\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_INT64\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_INT32\n",
      "#         type: DT_INT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"NthElement\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"n\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"values\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"reverse\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: false\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"QuantizedAvgPool\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"min_input\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"max_input\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"min_output\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"max_output\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"ksize\"\n",
      "#     type: \"list(int)\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"QuantizedBatchNormWithGlobalNormalization\"\n",
      "#   input_arg {\n",
      "#     name: \"t\"\n",
      "#     type_attr: \"Tinput\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"t_min\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"t_max\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"m\"\n",
      "#     type_attr: \"Tinput\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"m_min\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"m_max\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"v\"\n",
      "#     type_attr: \"Tinput\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"v_min\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"v_max\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"beta\"\n",
      "#     type_attr: \"Tinput\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"beta_min\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"beta_max\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"gamma\"\n",
      "#     type_attr: \"Tinput\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"gamma_min\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"gamma_max\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"result\"\n",
      "#     type_attr: \"out_type\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"result_min\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"result_max\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"Tinput\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"out_type\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"variance_epsilon\"\n",
      "#     type: \"float\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"scale_after_normalization\"\n",
      "#     type: \"bool\"\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"QuantizedBiasAdd\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T1\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"bias\"\n",
      "#     type_attr: \"T2\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"min_input\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"max_input\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"min_bias\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"max_bias\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"out_type\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"min_out\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"max_out\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T1\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T2\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"out_type\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"QuantizedConv2D\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"Tinput\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"filter\"\n",
      "#     type_attr: \"Tfilter\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"min_input\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"max_input\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"min_filter\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"max_filter\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"out_type\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"min_output\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"max_output\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"Tinput\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"Tfilter\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"out_type\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_QINT32\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"dilations\"\n",
      "#     type: \"list(int)\"\n",
      "#     default_value {\n",
      "#       list {\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#         i: 1\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"QuantizedMaxPool\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"min_input\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"max_input\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"output\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"min_output\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"max_output\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"ksize\"\n",
      "#     type: \"list(int)\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"strides\"\n",
      "#     type: \"list(int)\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"padding\"\n",
      "#     type: \"string\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         s: \"SAME\"\n",
      "#         s: \"VALID\"\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"QuantizedRelu\"\n",
      "#   input_arg {\n",
      "#     name: \"features\"\n",
      "#     type_attr: \"Tinput\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"min_features\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"max_features\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"activations\"\n",
      "#     type_attr: \"out_type\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"min_activations\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"max_activations\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"Tinput\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"out_type\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_QUINT8\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"QuantizedRelu6\"\n",
      "#   input_arg {\n",
      "#     name: \"features\"\n",
      "#     type_attr: \"Tinput\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"min_features\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"max_features\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"activations\"\n",
      "#     type_attr: \"out_type\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"min_activations\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"max_activations\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"Tinput\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"out_type\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_QUINT8\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"QuantizedReluX\"\n",
      "#   input_arg {\n",
      "#     name: \"features\"\n",
      "#     type_attr: \"Tinput\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"max_value\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"min_features\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"max_features\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"activations\"\n",
      "#     type_attr: \"out_type\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"min_activations\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"max_activations\"\n",
      "#     type: DT_FLOAT\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"Tinput\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"out_type\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_QUINT8\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_QINT8\n",
      "#         type: DT_QUINT8\n",
      "#         type: DT_QINT32\n",
      "#         type: DT_QINT16\n",
      "#         type: DT_QUINT16\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Relu\"\n",
      "#   input_arg {\n",
      "#     name: \"features\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"activations\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#         type: DT_QINT8\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Relu6\"\n",
      "#   input_arg {\n",
      "#     name: \"features\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"activations\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Relu6Grad\"\n",
      "#   input_arg {\n",
      "#     name: \"gradients\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"features\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"backprops\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"ReluGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"gradients\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"features\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"backprops\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Selu\"\n",
      "#   input_arg {\n",
      "#     name: \"features\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"activations\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"SeluGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"gradients\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"outputs\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"backprops\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Softmax\"\n",
      "#   input_arg {\n",
      "#     name: \"logits\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"softmax\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"SoftmaxCrossEntropyWithLogits\"\n",
      "#   input_arg {\n",
      "#     name: \"features\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"labels\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"loss\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Softplus\"\n",
      "#   input_arg {\n",
      "#     name: \"features\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"activations\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"SoftplusGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"gradients\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"features\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"backprops\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"Softsign\"\n",
      "#   input_arg {\n",
      "#     name: \"features\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"activations\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"SoftsignGrad\"\n",
      "#   input_arg {\n",
      "#     name: \"gradients\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"features\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"backprops\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"SparseSoftmaxCrossEntropyWithLogits\"\n",
      "#   input_arg {\n",
      "#     name: \"features\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"labels\"\n",
      "#     type_attr: \"Tlabels\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"loss\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"backprop\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_HALF\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"Tlabels\"\n",
      "#     type: \"type\"\n",
      "#     default_value {\n",
      "#       type: DT_INT64\n",
      "#     }\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_INT32\n",
      "#         type: DT_INT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"TopK\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"values\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"indices\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"k\"\n",
      "#     type: \"int\"\n",
      "#     has_minimum: true\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"sorted\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: true\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "#   deprecation {\n",
      "#     version: 7\n",
      "#     explanation: \"Use TopKV2 instead\"\n",
      "#   }\n",
      "# }\n",
      "# op {\n",
      "#   name: \"TopKV2\"\n",
      "#   input_arg {\n",
      "#     name: \"input\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   input_arg {\n",
      "#     name: \"k\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"values\"\n",
      "#     type_attr: \"T\"\n",
      "#   }\n",
      "#   output_arg {\n",
      "#     name: \"indices\"\n",
      "#     type: DT_INT32\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"sorted\"\n",
      "#     type: \"bool\"\n",
      "#     default_value {\n",
      "#       b: true\n",
      "#     }\n",
      "#   }\n",
      "#   attr {\n",
      "#     name: \"T\"\n",
      "#     type: \"type\"\n",
      "#     allowed_values {\n",
      "#       list {\n",
      "#         type: DT_FLOAT\n",
      "#         type: DT_DOUBLE\n",
      "#         type: DT_INT32\n",
      "#         type: DT_UINT8\n",
      "#         type: DT_INT16\n",
      "#         type: DT_INT8\n",
      "#         type: DT_INT64\n",
      "#         type: DT_BFLOAT16\n",
      "#         type: DT_UINT16\n",
      "#         type: DT_HALF\n",
      "#         type: DT_UINT32\n",
      "#         type: DT_UINT64\n",
      "#       }\n",
      "#     }\n",
      "#   }\n",
      "# }\n",
      "_op_def_lib = _InitOpDefLibrary(b\"\\n\\274\\001\\n\\007AvgPool\\022\\n\\n\\005value\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\026\\n\\005ksize\\022\\tlist(int)(\\0010\\004\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\004\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\n\\301\\001\\n\\tAvgPool3D\\022\\n\\n\\005input\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\026\\n\\005ksize\\022\\tlist(int)(\\0010\\005\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\005\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"0\\n\\013data_format\\022\\006string\\032\\007\\022\\005NDHWC:\\020\\n\\016\\022\\005NDHWC\\022\\005NCDHW\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\n\\332\\001\\n\\rAvgPool3DGrad\\022\\024\\n\\020orig_input_shape\\030\\003\\022\\t\\n\\004grad\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\026\\n\\005ksize\\022\\tlist(int)(\\0010\\005\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\005\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"0\\n\\013data_format\\022\\006string\\032\\007\\022\\005NDHWC:\\020\\n\\016\\022\\005NDHWC\\022\\005NCDHW\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\n\\325\\001\\n\\013AvgPoolGrad\\022\\024\\n\\020orig_input_shape\\030\\003\\022\\t\\n\\004grad\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\026\\n\\005ksize\\022\\tlist(int)(\\0010\\004\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\004\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\n\\343\\001\\n BatchNormWithGlobalNormalization\\022\\006\\n\\001t\\\"\\001T\\022\\006\\n\\001m\\\"\\001T\\022\\006\\n\\001v\\\"\\001T\\022\\t\\n\\004beta\\\"\\001T\\022\\n\\n\\005gamma\\\"\\001T\\032\\013\\n\\006result\\\"\\001T\\\" \\n\\001T\\022\\004type:\\025\\n\\0232\\021\\001\\002\\003\\004\\005\\006\\010\\t\\013\\014\\r\\016\\021\\022\\023\\026\\027\\\"\\031\\n\\020variance_epsilon\\022\\005float\\\"!\\n\\031scale_after_normalization\\022\\004boolB#\\010\\t\\022\\037Use tf.nn.batch_normalization()\\n\\213\\002\\n$BatchNormWithGlobalNormalizationGrad\\022\\006\\n\\001t\\\"\\001T\\022\\006\\n\\001m\\\"\\001T\\022\\006\\n\\001v\\\"\\001T\\022\\n\\n\\005gamma\\\"\\001T\\022\\r\\n\\010backprop\\\"\\001T\\032\\007\\n\\002dx\\\"\\001T\\032\\007\\n\\002dm\\\"\\001T\\032\\007\\n\\002dv\\\"\\001T\\032\\007\\n\\002db\\\"\\001T\\032\\007\\n\\002dg\\\"\\001T\\\" \\n\\001T\\022\\004type:\\025\\n\\0232\\021\\001\\002\\003\\004\\005\\006\\010\\t\\013\\014\\r\\016\\021\\022\\023\\026\\027\\\"\\031\\n\\020variance_epsilon\\022\\005float\\\"!\\n\\031scale_after_normalization\\022\\004boolB#\\010\\t\\022\\037Use tf.nn.batch_normalization()\\n~\\n\\007BiasAdd\\022\\n\\n\\005value\\\"\\001T\\022\\t\\n\\004bias\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\" \\n\\001T\\022\\004type:\\025\\n\\0232\\021\\001\\002\\003\\004\\005\\006\\010\\t\\013\\014\\r\\016\\021\\022\\023\\026\\027\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\n~\\n\\013BiasAddGrad\\022\\021\\n\\014out_backprop\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\" \\n\\001T\\022\\004type:\\025\\n\\0232\\021\\001\\002\\003\\004\\005\\006\\010\\t\\013\\014\\r\\016\\021\\022\\023\\026\\027\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\nQ\\n\\tBiasAddV1\\022\\n\\n\\005value\\\"\\001T\\022\\t\\n\\004bias\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\" \\n\\001T\\022\\004type:\\025\\n\\0232\\021\\001\\002\\003\\004\\005\\006\\010\\t\\013\\014\\r\\016\\021\\022\\023\\026\\027\\n\\354\\001\\n\\006Conv2D\\022\\n\\n\\005input\\\"\\001T\\022\\013\\n\\006filter\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\\"\\024\\n\\007strides\\022\\tlist(int)\\\"\\034\\n\\020use_cudnn_on_gpu\\022\\004bool\\032\\002(\\001\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\" \\n\\tdilations\\022\\tlist(int)\\032\\010\\n\\006\\032\\004\\001\\001\\001\\001\\n\\222\\002\\n\\024Conv2DBackpropFilter\\022\\n\\n\\005input\\\"\\001T\\022\\020\\n\\014filter_sizes\\030\\003\\022\\021\\n\\014out_backprop\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\\"\\024\\n\\007strides\\022\\tlist(int)\\\"\\034\\n\\020use_cudnn_on_gpu\\022\\004bool\\032\\002(\\001\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\" \\n\\tdilations\\022\\tlist(int)\\032\\010\\n\\006\\032\\004\\001\\001\\001\\001\\n\\221\\002\\n\\023Conv2DBackpropInput\\022\\017\\n\\013input_sizes\\030\\003\\022\\013\\n\\006filter\\\"\\001T\\022\\021\\n\\014out_backprop\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\\"\\024\\n\\007strides\\022\\tlist(int)\\\"\\034\\n\\020use_cudnn_on_gpu\\022\\004bool\\032\\002(\\001\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\" \\n\\tdilations\\022\\tlist(int)\\032\\010\\n\\006\\032\\004\\001\\001\\001\\001\\n\\326\\001\\n\\006Conv3D\\022\\n\\n\\005input\\\"\\001T\\022\\013\\n\\006filter\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\005\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"0\\n\\013data_format\\022\\006string\\032\\007\\022\\005NDHWC:\\020\\n\\016\\022\\005NDHWC\\022\\005NCDHW\\\"!\\n\\tdilations\\022\\tlist(int)\\032\\t\\n\\007\\032\\005\\001\\001\\001\\001\\001\\n\\344\\001\\n\\024Conv3DBackpropFilter\\022\\n\\n\\005input\\\"\\001T\\022\\013\\n\\006filter\\\"\\001T\\022\\021\\n\\014out_backprop\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\022\\n\\001T\\022\\004type:\\007\\n\\0052\\003\\023\\001\\002\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\005\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"!\\n\\tdilations\\022\\tlist(int)\\032\\t\\n\\007\\032\\005\\001\\001\\001\\001\\001B\\036\\010\\n\\022\\032Use Conv3DBackpropFilterV2\\n\\376\\001\\n\\026Conv3DBackpropFilterV2\\022\\n\\n\\005input\\\"\\001T\\022\\020\\n\\014filter_sizes\\030\\003\\022\\021\\n\\014out_backprop\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\005\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"0\\n\\013data_format\\022\\006string\\032\\007\\022\\005NDHWC:\\020\\n\\016\\022\\005NDHWC\\022\\005NCDHW\\\"!\\n\\tdilations\\022\\tlist(int)\\032\\t\\n\\007\\032\\005\\001\\001\\001\\001\\001\\n\\342\\001\\n\\023Conv3DBackpropInput\\022\\n\\n\\005input\\\"\\001T\\022\\013\\n\\006filter\\\"\\001T\\022\\021\\n\\014out_backprop\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\022\\n\\001T\\022\\004type:\\007\\n\\0052\\003\\023\\001\\002\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\005\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"!\\n\\tdilations\\022\\tlist(int)\\032\\t\\n\\007\\032\\005\\001\\001\\001\\001\\001B\\035\\010\\n\\022\\031Use Conv3DBackpropInputV2\\n\\237\\002\\n\\025Conv3DBackpropInputV2\\022\\025\\n\\013input_sizes\\\"\\006Tshape\\022\\013\\n\\006filter\\\"\\001T\\022\\021\\n\\014out_backprop\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\005\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"0\\n\\013data_format\\022\\006string\\032\\007\\022\\005NDHWC:\\020\\n\\016\\022\\005NDHWC\\022\\005NCDHW\\\"!\\n\\tdilations\\022\\tlist(int)\\032\\t\\n\\007\\032\\005\\001\\001\\001\\001\\001\\\"\\032\\n\\006Tshape\\022\\004type\\032\\0020\\003:\\006\\n\\0042\\002\\003\\t\\nu\\n\\020DataFormatDimMap\\022\\006\\n\\001x\\\"\\001T\\032\\006\\n\\001y\\\"\\001T\\\"\\025\\n\\001T\\022\\004type\\032\\0020\\003:\\006\\n\\0042\\002\\003\\t\\\"\\034\\n\\nsrc_format\\022\\006string\\032\\006\\022\\004NHWC\\\"\\034\\n\\ndst_format\\022\\006string\\032\\006\\022\\004NCHW\\ny\\n\\024DataFormatVecPermute\\022\\006\\n\\001x\\\"\\001T\\032\\006\\n\\001y\\\"\\001T\\\"\\025\\n\\001T\\022\\004type\\032\\0020\\003:\\006\\n\\0042\\002\\003\\t\\\"\\034\\n\\nsrc_format\\022\\006string\\032\\006\\022\\004NHWC\\\"\\034\\n\\ndst_format\\022\\006string\\032\\006\\022\\004NCHW\\n\\335\\001\\n\\025DepthwiseConv2dNative\\022\\n\\n\\005input\\\"\\001T\\022\\013\\n\\006filter\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\\"\\024\\n\\007strides\\022\\tlist(int)\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\" \\n\\tdilations\\022\\tlist(int)\\032\\010\\n\\006\\032\\004\\001\\001\\001\\001\\n\\203\\002\\n#DepthwiseConv2dNativeBackpropFilter\\022\\n\\n\\005input\\\"\\001T\\022\\020\\n\\014filter_sizes\\030\\003\\022\\021\\n\\014out_backprop\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\\"\\024\\n\\007strides\\022\\tlist(int)\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\" \\n\\tdilations\\022\\tlist(int)\\032\\010\\n\\006\\032\\004\\001\\001\\001\\001\\n\\202\\002\\n\\\"DepthwiseConv2dNativeBackpropInput\\022\\017\\n\\013input_sizes\\030\\003\\022\\013\\n\\006filter\\\"\\001T\\022\\021\\n\\014out_backprop\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\\"\\024\\n\\007strides\\022\\tlist(int)\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\" \\n\\tdilations\\022\\tlist(int)\\032\\010\\n\\006\\032\\004\\001\\001\\001\\001\\n\\245\\001\\n\\nDilation2D\\022\\n\\n\\005input\\\"\\001T\\022\\013\\n\\006filter\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\004\\\"\\026\\n\\005rates\\022\\tlist(int)(\\0010\\004\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\n\\317\\001\\n\\030Dilation2DBackpropFilter\\022\\n\\n\\005input\\\"\\001T\\022\\013\\n\\006filter\\\"\\001T\\022\\021\\n\\014out_backprop\\\"\\001T\\032\\024\\n\\017filter_backprop\\\"\\001T\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\004\\\"\\026\\n\\005rates\\022\\tlist(int)(\\0010\\004\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\n\\312\\001\\n\\027Dilation2DBackpropInput\\022\\n\\n\\005input\\\"\\001T\\022\\013\\n\\006filter\\\"\\001T\\022\\021\\n\\014out_backprop\\\"\\001T\\032\\020\\n\\013in_backprop\\\"\\001T\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\004\\\"\\026\\n\\005rates\\022\\tlist(int)(\\0010\\004\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\n;\\n\\003Elu\\022\\r\\n\\010features\\\"\\001T\\032\\020\\n\\013activations\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\nL\\n\\007EluGrad\\022\\016\\n\\tgradients\\\"\\001T\\022\\014\\n\\007outputs\\\"\\001T\\032\\016\\n\\tbackprops\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\n\\211\\002\\n\\021FractionalAvgPool\\022\\n\\n\\005value\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\032\\030\\n\\024row_pooling_sequence\\030\\t\\032\\030\\n\\024col_pooling_sequence\\030\\t\\\" \\n\\rpooling_ratio\\022\\013list(float)(\\0010\\004\\\"\\031\\n\\rpseudo_random\\022\\004bool\\032\\002(\\000\\\"\\027\\n\\013overlapping\\022\\004bool\\032\\002(\\000\\\"\\031\\n\\rdeterministic\\022\\004bool\\032\\002(\\000\\\"\\017\\n\\004seed\\022\\003int\\032\\002\\030\\000\\\"\\020\\n\\005seed2\\022\\003int\\032\\002\\030\\000\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\001\\002\\003\\t\\n\\266\\001\\n\\025FractionalAvgPoolGrad\\022\\033\\n\\027orig_input_tensor_shape\\030\\t\\022\\021\\n\\014out_backprop\\\"\\001T\\022\\030\\n\\024row_pooling_sequence\\030\\t\\022\\030\\n\\024col_pooling_sequence\\030\\t\\032\\013\\n\\006output\\\"\\001T\\\"\\027\\n\\013overlapping\\022\\004bool\\032\\002(\\000\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\001\\002\\003\\t\\n\\211\\002\\n\\021FractionalMaxPool\\022\\n\\n\\005value\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\032\\030\\n\\024row_pooling_sequence\\030\\t\\032\\030\\n\\024col_pooling_sequence\\030\\t\\\" \\n\\rpooling_ratio\\022\\013list(float)(\\0010\\004\\\"\\031\\n\\rpseudo_random\\022\\004bool\\032\\002(\\000\\\"\\027\\n\\013overlapping\\022\\004bool\\032\\002(\\000\\\"\\031\\n\\rdeterministic\\022\\004bool\\032\\002(\\000\\\"\\017\\n\\004seed\\022\\003int\\032\\002\\030\\000\\\"\\020\\n\\005seed2\\022\\003int\\032\\002\\030\\000\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\001\\002\\003\\t\\n\\274\\001\\n\\025FractionalMaxPoolGrad\\022\\017\\n\\norig_input\\\"\\001T\\022\\020\\n\\013orig_output\\\"\\001T\\022\\021\\n\\014out_backprop\\\"\\001T\\022\\030\\n\\024row_pooling_sequence\\030\\t\\022\\030\\n\\024col_pooling_sequence\\030\\t\\032\\013\\n\\006output\\\"\\001T\\\"\\027\\n\\013overlapping\\022\\004bool\\032\\002(\\000\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\001\\002\\003\\t\\n\\230\\002\\n\\016FusedBatchNorm\\022\\006\\n\\001x\\\"\\001T\\022\\n\\n\\005scale\\\"\\001T\\022\\013\\n\\006offset\\\"\\001T\\022\\t\\n\\004mean\\\"\\001T\\022\\r\\n\\010variance\\\"\\001T\\032\\006\\n\\001y\\\"\\001T\\032\\017\\n\\nbatch_mean\\\"\\001T\\032\\023\\n\\016batch_variance\\\"\\001T\\032\\024\\n\\017reserve_space_1\\\"\\001T\\032\\024\\n\\017reserve_space_2\\\"\\001T\\\"\\020\\n\\001T\\022\\004type:\\005\\n\\0032\\001\\001\\\"\\027\\n\\007epsilon\\022\\005float\\032\\005%\\027\\267\\3218\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\"\\027\\n\\013is_training\\022\\004bool\\032\\002(\\001\\n\\300\\002\\n\\022FusedBatchNormGrad\\022\\017\\n\\ny_backprop\\\"\\001T\\022\\006\\n\\001x\\\"\\001T\\022\\n\\n\\005scale\\\"\\001T\\022\\024\\n\\017reserve_space_1\\\"\\001T\\022\\024\\n\\017reserve_space_2\\\"\\001T\\032\\017\\n\\nx_backprop\\\"\\001T\\032\\023\\n\\016scale_backprop\\\"\\001T\\032\\024\\n\\017offset_backprop\\\"\\001T\\032\\024\\n\\017reserve_space_3\\\"\\001T\\032\\024\\n\\017reserve_space_4\\\"\\001T\\\"\\020\\n\\001T\\022\\004type:\\005\\n\\0032\\001\\001\\\"\\027\\n\\007epsilon\\022\\005float\\032\\005%\\027\\267\\3218\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\"\\027\\n\\013is_training\\022\\004bool\\032\\002(\\001\\n\\325\\002\\n\\024FusedBatchNormGradV2\\022\\017\\n\\ny_backprop\\\"\\001T\\022\\006\\n\\001x\\\"\\001T\\022\\t\\n\\005scale\\030\\001\\022\\024\\n\\017reserve_space_1\\\"\\001U\\022\\024\\n\\017reserve_space_2\\\"\\001U\\032\\017\\n\\nx_backprop\\\"\\001T\\032\\023\\n\\016scale_backprop\\\"\\001U\\032\\024\\n\\017offset_backprop\\\"\\001U\\032\\024\\n\\017reserve_space_3\\\"\\001U\\032\\024\\n\\017reserve_space_4\\\"\\001U\\\"\\022\\n\\001T\\022\\004type:\\007\\n\\0052\\003\\023\\016\\001\\\"\\020\\n\\001U\\022\\004type:\\005\\n\\0032\\001\\001\\\"\\027\\n\\007epsilon\\022\\005float\\032\\005%\\027\\267\\3218\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\"\\027\\n\\013is_training\\022\\004bool\\032\\002(\\001\\n\\256\\002\\n\\020FusedBatchNormV2\\022\\006\\n\\001x\\\"\\001T\\022\\n\\n\\005scale\\\"\\001U\\022\\013\\n\\006offset\\\"\\001U\\022\\t\\n\\004mean\\\"\\001U\\022\\r\\n\\010variance\\\"\\001U\\032\\006\\n\\001y\\\"\\001T\\032\\017\\n\\nbatch_mean\\\"\\001U\\032\\023\\n\\016batch_variance\\\"\\001U\\032\\024\\n\\017reserve_space_1\\\"\\001U\\032\\024\\n\\017reserve_space_2\\\"\\001U\\\"\\022\\n\\001T\\022\\004type:\\007\\n\\0052\\003\\023\\016\\001\\\"\\020\\n\\001U\\022\\004type:\\005\\n\\0032\\001\\001\\\"\\027\\n\\007epsilon\\022\\005float\\032\\005%\\027\\267\\3218\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\"\\027\\n\\013is_training\\022\\004bool\\032\\002(\\001\\n\\272\\001\\n\\016FusedPadConv2D\\022\\n\\n\\005input\\\"\\001T\\022\\014\\n\\010paddings\\030\\003\\022\\013\\n\\006filter\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\022\\n\\001T\\022\\004type:\\007\\n\\0052\\003\\023\\001\\002\\\"&\\n\\004mode\\022\\006string:\\026\\n\\024\\022\\007REFLECT\\022\\tSYMMETRIC\\\"\\024\\n\\007strides\\022\\tlist(int)\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\n\\357\\001\\n\\027FusedResizeAndPadConv2D\\022\\n\\n\\005input\\\"\\001T\\022\\010\\n\\004size\\030\\003\\022\\014\\n\\010paddings\\030\\003\\022\\013\\n\\006filter\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\022\\n\\001T\\022\\004type:\\007\\n\\0052\\003\\023\\001\\002\\\" \\n\\024resize_align_corners\\022\\004bool\\032\\002(\\000\\\"&\\n\\004mode\\022\\006string:\\026\\n\\024\\022\\007REFLECT\\022\\tSYMMETRIC\\\"\\024\\n\\007strides\\022\\tlist(int)\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\nW\\n\\006InTopK\\022\\017\\n\\013predictions\\030\\001\\022\\014\\n\\007targets\\\"\\001T\\032\\r\\n\\tprecision\\030\\n\\\"\\010\\n\\001k\\022\\003int\\\"\\025\\n\\001T\\022\\004type\\032\\0020\\003:\\006\\n\\0042\\002\\003\\t\\nW\\n\\010InTopKV2\\022\\017\\n\\013predictions\\030\\001\\022\\014\\n\\007targets\\\"\\001T\\022\\006\\n\\001k\\\"\\001T\\032\\r\\n\\tprecision\\030\\n\\\"\\025\\n\\001T\\022\\004type\\032\\0020\\003:\\006\\n\\0042\\002\\003\\t\\n2\\n\\006L2Loss\\022\\006\\n\\001t\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\n\\222\\001\\n\\003LRN\\022\\n\\n\\005input\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\027\\n\\014depth_radius\\022\\003int\\032\\002\\030\\005\\\"\\024\\n\\004bias\\022\\005float\\032\\005%\\000\\000\\200?\\\"\\025\\n\\005alpha\\022\\005float\\032\\005%\\000\\000\\200?\\\"\\024\\n\\004beta\\022\\005float\\032\\005%\\000\\000\\000?\\\"\\026\\n\\001T\\022\\004type\\032\\0020\\001:\\007\\n\\0052\\003\\023\\016\\001\\n\\301\\001\\n\\007LRNGrad\\022\\020\\n\\013input_grads\\\"\\001T\\022\\020\\n\\013input_image\\\"\\001T\\022\\021\\n\\014output_image\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\027\\n\\014depth_radius\\022\\003int\\032\\002\\030\\005\\\"\\024\\n\\004bias\\022\\005float\\032\\005%\\000\\000\\200?\\\"\\025\\n\\005alpha\\022\\005float\\032\\005%\\000\\000\\200?\\\"\\024\\n\\004beta\\022\\005float\\032\\005%\\000\\000\\000?\\\"\\026\\n\\001T\\022\\004type\\032\\0020\\001:\\007\\n\\0052\\003\\023\\016\\001\\n?\\n\\nLogSoftmax\\022\\013\\n\\006logits\\\"\\001T\\032\\017\\n\\nlogsoftmax\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\n\\324\\001\\n\\007MaxPool\\022\\n\\n\\005input\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\036\\n\\001T\\022\\004type\\032\\0020\\001:\\017\\n\\r2\\013\\023\\016\\001\\002\\003\\t\\004\\005\\006\\021\\013\\\"\\026\\n\\005ksize\\022\\tlist(int)(\\0010\\004\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\004\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\":\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\033\\n\\031\\022\\004NHWC\\022\\004NCHW\\022\\013NCHW_VECT_C\\n\\300\\001\\n\\tMaxPool3D\\022\\n\\n\\005input\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\026\\n\\005ksize\\022\\tlist(int)(\\0010\\005\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\005\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"0\\n\\013data_format\\022\\006string\\032\\007\\022\\005NDHWC:\\020\\n\\016\\022\\005NDHWC\\022\\005NCDHW\\\"\\022\\n\\001T\\022\\004type:\\007\\n\\0052\\003\\023\\016\\001\\n\\221\\002\\n\\rMaxPool3DGrad\\022\\024\\n\\norig_input\\\"\\006TInput\\022\\025\\n\\013orig_output\\\"\\006TInput\\022\\t\\n\\004grad\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\026\\n\\005ksize\\022\\tlist(int)(\\0010\\005\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\005\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"0\\n\\013data_format\\022\\006string\\032\\007\\022\\005NDHWC:\\020\\n\\016\\022\\005NDHWC\\022\\005NCDHW\\\"\\026\\n\\001T\\022\\004type\\032\\0020\\001:\\007\\n\\0052\\003\\023\\016\\001\\\"\\033\\n\\006TInput\\022\\004type\\032\\0020\\001:\\007\\n\\0052\\003\\023\\016\\001\\n\\363\\001\\n\\021MaxPool3DGradGrad\\022\\017\\n\\norig_input\\\"\\001T\\022\\020\\n\\013orig_output\\\"\\001T\\022\\t\\n\\004grad\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\026\\n\\005ksize\\022\\tlist(int)(\\0010\\005\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\005\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"0\\n\\013data_format\\022\\006string\\032\\007\\022\\005NDHWC:\\020\\n\\016\\022\\005NDHWC\\022\\005NCDHW\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\n\\356\\001\\n\\013MaxPoolGrad\\022\\017\\n\\norig_input\\\"\\001T\\022\\020\\n\\013orig_output\\\"\\001T\\022\\t\\n\\004grad\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\026\\n\\005ksize\\022\\tlist(int)(\\0010\\004\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\004\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\"\\037\\n\\001T\\022\\004type\\032\\0020\\001:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\n\\356\\001\\n\\017MaxPoolGradGrad\\022\\017\\n\\norig_input\\\"\\001T\\022\\020\\n\\013orig_output\\\"\\001T\\022\\t\\n\\004grad\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\\"\\026\\n\\005ksize\\022\\tlist(int)(\\0010\\004\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\004\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\n\\326\\001\\n\\021MaxPoolGradGradV2\\022\\017\\n\\norig_input\\\"\\001T\\022\\020\\n\\013orig_output\\\"\\001T\\022\\t\\n\\004grad\\\"\\001T\\022\\t\\n\\005ksize\\030\\003\\022\\013\\n\\007strides\\030\\003\\032\\013\\n\\006output\\\"\\001T\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\n\\336\\001\\n\\031MaxPoolGradGradWithArgmax\\022\\n\\n\\005input\\\"\\001T\\022\\t\\n\\004grad\\\"\\001T\\022\\021\\n\\006argmax\\\"\\007Targmax\\032\\013\\n\\006output\\\"\\001T\\\"\\026\\n\\005ksize\\022\\tlist(int)(\\0010\\004\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\004\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"\\027\\n\\007Targmax\\022\\004type:\\006\\n\\0042\\002\\003\\t\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\n\\326\\001\\n\\rMaxPoolGradV2\\022\\017\\n\\norig_input\\\"\\001T\\022\\020\\n\\013orig_output\\\"\\001T\\022\\t\\n\\004grad\\\"\\001T\\022\\t\\n\\005ksize\\030\\003\\022\\013\\n\\007strides\\030\\003\\032\\013\\n\\006output\\\"\\001T\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"-\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\016\\n\\014\\022\\004NHWC\\022\\004NCHW\\\"\\037\\n\\001T\\022\\004type\\032\\0020\\001:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\n\\332\\001\\n\\025MaxPoolGradWithArgmax\\022\\n\\n\\005input\\\"\\001T\\022\\t\\n\\004grad\\\"\\001T\\022\\021\\n\\006argmax\\\"\\007Targmax\\032\\013\\n\\006output\\\"\\001T\\\"\\026\\n\\005ksize\\022\\tlist(int)(\\0010\\004\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\004\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"\\027\\n\\007Targmax\\022\\004type:\\006\\n\\0042\\002\\003\\t\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\n\\274\\001\\n\\tMaxPoolV2\\022\\n\\n\\005input\\\"\\001T\\022\\t\\n\\005ksize\\030\\003\\022\\013\\n\\007strides\\030\\003\\032\\013\\n\\006output\\\"\\001T\\\"\\036\\n\\001T\\022\\004type\\032\\0020\\001:\\017\\n\\r2\\013\\023\\016\\001\\002\\003\\t\\004\\005\\006\\021\\013\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\":\\n\\013data_format\\022\\006string\\032\\006\\022\\004NHWC:\\033\\n\\031\\022\\004NHWC\\022\\004NCHW\\022\\013NCHW_VECT_C\\n\\317\\001\\n\\021MaxPoolWithArgmax\\022\\n\\n\\005input\\\"\\001T\\032\\013\\n\\006output\\\"\\001T\\032\\021\\n\\006argmax\\\"\\007Targmax\\\"\\026\\n\\005ksize\\022\\tlist(int)(\\0010\\004\\\"\\030\\n\\007strides\\022\\tlist(int)(\\0010\\004\\\"\\033\\n\\007Targmax\\022\\004type\\032\\0020\\t:\\006\\n\\0042\\002\\003\\t\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\n^\\n\\nNthElement\\022\\n\\n\\005input\\\"\\001T\\022\\005\\n\\001n\\030\\003\\032\\013\\n\\006values\\\"\\001T\\\"\\023\\n\\007reverse\\022\\004bool\\032\\002(\\000\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\n\\315\\001\\n\\020QuantizedAvgPool\\022\\n\\n\\005input\\\"\\001T\\022\\r\\n\\tmin_input\\030\\001\\022\\r\\n\\tmax_input\\030\\001\\032\\013\\n\\006output\\\"\\001T\\032\\016\\n\\nmin_output\\030\\001\\032\\016\\n\\nmax_output\\030\\001\\\"\\024\\n\\001T\\022\\004type:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\\"\\022\\n\\005ksize\\022\\tlist(int)\\\"\\024\\n\\007strides\\022\\tlist(int)\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\n\\231\\003\\n)QuantizedBatchNormWithGlobalNormalization\\022\\013\\n\\001t\\\"\\006Tinput\\022\\t\\n\\005t_min\\030\\001\\022\\t\\n\\005t_max\\030\\001\\022\\013\\n\\001m\\\"\\006Tinput\\022\\t\\n\\005m_min\\030\\001\\022\\t\\n\\005m_max\\030\\001\\022\\013\\n\\001v\\\"\\006Tinput\\022\\t\\n\\005v_min\\030\\001\\022\\t\\n\\005v_max\\030\\001\\022\\016\\n\\004beta\\\"\\006Tinput\\022\\014\\n\\010beta_min\\030\\001\\022\\014\\n\\010beta_max\\030\\001\\022\\017\\n\\005gamma\\\"\\006Tinput\\022\\r\\n\\tgamma_min\\030\\001\\022\\r\\n\\tgamma_max\\030\\001\\032\\022\\n\\006result\\\"\\010out_type\\032\\016\\n\\nresult_min\\030\\001\\032\\016\\n\\nresult_max\\030\\001\\\"\\031\\n\\006Tinput\\022\\004type:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\\"\\033\\n\\010out_type\\022\\004type:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\\"\\031\\n\\020variance_epsilon\\022\\005float\\\"!\\n\\031scale_after_normalization\\022\\004bool\\n\\336\\001\\n\\020QuantizedBiasAdd\\022\\013\\n\\005input\\\"\\002T1\\022\\n\\n\\004bias\\\"\\002T2\\022\\r\\n\\tmin_input\\030\\001\\022\\r\\n\\tmax_input\\030\\001\\022\\014\\n\\010min_bias\\030\\001\\022\\014\\n\\010max_bias\\030\\001\\032\\022\\n\\006output\\\"\\010out_type\\032\\013\\n\\007min_out\\030\\001\\032\\013\\n\\007max_out\\030\\001\\\"\\025\\n\\002T1\\022\\004type:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\\"\\025\\n\\002T2\\022\\004type:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\\"\\033\\n\\010out_type\\022\\004type:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\n\\333\\002\\n\\017QuantizedConv2D\\022\\017\\n\\005input\\\"\\006Tinput\\022\\021\\n\\006filter\\\"\\007Tfilter\\022\\r\\n\\tmin_input\\030\\001\\022\\r\\n\\tmax_input\\030\\001\\022\\016\\n\\nmin_filter\\030\\001\\022\\016\\n\\nmax_filter\\030\\001\\032\\022\\n\\006output\\\"\\010out_type\\032\\016\\n\\nmin_output\\030\\001\\032\\016\\n\\nmax_output\\030\\001\\\"\\031\\n\\006Tinput\\022\\004type:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\\"\\032\\n\\007Tfilter\\022\\004type:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\\"\\037\\n\\010out_type\\022\\004type\\032\\0020\\r:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\\"\\024\\n\\007strides\\022\\tlist(int)\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\\" \\n\\tdilations\\022\\tlist(int)\\032\\010\\n\\006\\032\\004\\001\\001\\001\\001\\n\\315\\001\\n\\020QuantizedMaxPool\\022\\n\\n\\005input\\\"\\001T\\022\\r\\n\\tmin_input\\030\\001\\022\\r\\n\\tmax_input\\030\\001\\032\\013\\n\\006output\\\"\\001T\\032\\016\\n\\nmin_output\\030\\001\\032\\016\\n\\nmax_output\\030\\001\\\"\\024\\n\\001T\\022\\004type:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\\"\\022\\n\\005ksize\\022\\tlist(int)\\\"\\024\\n\\007strides\\022\\tlist(int)\\\"\\\"\\n\\007padding\\022\\006string:\\017\\n\\r\\022\\004SAME\\022\\005VALID\\n\\306\\001\\n\\rQuantizedRelu\\022\\022\\n\\010features\\\"\\006Tinput\\022\\020\\n\\014min_features\\030\\001\\022\\020\\n\\014max_features\\030\\001\\032\\027\\n\\013activations\\\"\\010out_type\\032\\023\\n\\017min_activations\\030\\001\\032\\023\\n\\017max_activations\\030\\001\\\"\\031\\n\\006Tinput\\022\\004type:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\\"\\037\\n\\010out_type\\022\\004type\\032\\0020\\014:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\n\\307\\001\\n\\016QuantizedRelu6\\022\\022\\n\\010features\\\"\\006Tinput\\022\\020\\n\\014min_features\\030\\001\\022\\020\\n\\014max_features\\030\\001\\032\\027\\n\\013activations\\\"\\010out_type\\032\\023\\n\\017min_activations\\030\\001\\032\\023\\n\\017max_activations\\030\\001\\\"\\031\\n\\006Tinput\\022\\004type:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\\"\\037\\n\\010out_type\\022\\004type\\032\\0020\\014:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\n\\326\\001\\n\\016QuantizedReluX\\022\\022\\n\\010features\\\"\\006Tinput\\022\\r\\n\\tmax_value\\030\\001\\022\\020\\n\\014min_features\\030\\001\\022\\020\\n\\014max_features\\030\\001\\032\\027\\n\\013activations\\\"\\010out_type\\032\\023\\n\\017min_activations\\030\\001\\032\\023\\n\\017max_activations\\030\\001\\\"\\031\\n\\006Tinput\\022\\004type:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\\"\\037\\n\\010out_type\\022\\004type\\032\\0020\\014:\\t\\n\\0072\\005\\013\\014\\r\\017\\020\\nE\\n\\004Relu\\022\\r\\n\\010features\\\"\\001T\\032\\020\\n\\013activations\\\"\\001T\\\"\\034\\n\\001T\\022\\004type:\\021\\n\\0172\\r\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\013\\nE\\n\\005Relu6\\022\\r\\n\\010features\\\"\\001T\\032\\020\\n\\013activations\\\"\\001T\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\nW\\n\\tRelu6Grad\\022\\016\\n\\tgradients\\\"\\001T\\022\\r\\n\\010features\\\"\\001T\\032\\016\\n\\tbackprops\\\"\\001T\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\nV\\n\\010ReluGrad\\022\\016\\n\\tgradients\\\"\\001T\\022\\r\\n\\010features\\\"\\001T\\032\\016\\n\\tbackprops\\\"\\001T\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\\n<\\n\\004Selu\\022\\r\\n\\010features\\\"\\001T\\032\\020\\n\\013activations\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\nM\\n\\010SeluGrad\\022\\016\\n\\tgradients\\\"\\001T\\022\\014\\n\\007outputs\\\"\\001T\\032\\016\\n\\tbackprops\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\n9\\n\\007Softmax\\022\\013\\n\\006logits\\\"\\001T\\032\\014\\n\\007softmax\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\nj\\n\\035SoftmaxCrossEntropyWithLogits\\022\\r\\n\\010features\\\"\\001T\\022\\013\\n\\006labels\\\"\\001T\\032\\t\\n\\004loss\\\"\\001T\\032\\r\\n\\010backprop\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\n@\\n\\010Softplus\\022\\r\\n\\010features\\\"\\001T\\032\\020\\n\\013activations\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\nR\\n\\014SoftplusGrad\\022\\016\\n\\tgradients\\\"\\001T\\022\\r\\n\\010features\\\"\\001T\\032\\016\\n\\tbackprops\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\n@\\n\\010Softsign\\022\\r\\n\\010features\\\"\\001T\\032\\020\\n\\013activations\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\nR\\n\\014SoftsignGrad\\022\\016\\n\\tgradients\\\"\\001T\\022\\r\\n\\010features\\\"\\001T\\032\\016\\n\\tbackprops\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\n\\223\\001\\n#SparseSoftmaxCrossEntropyWithLogits\\022\\r\\n\\010features\\\"\\001T\\022\\021\\n\\006labels\\\"\\007Tlabels\\032\\t\\n\\004loss\\\"\\001T\\032\\r\\n\\010backprop\\\"\\001T\\\"\\023\\n\\001T\\022\\004type:\\010\\n\\0062\\004\\023\\016\\001\\002\\\"\\033\\n\\007Tlabels\\022\\004type\\032\\0020\\t:\\006\\n\\0042\\002\\003\\t\\n\\201\\001\\n\\004TopK\\022\\n\\n\\005input\\\"\\001T\\032\\013\\n\\006values\\\"\\001T\\032\\013\\n\\007indices\\030\\003\\\"\\n\\n\\001k\\022\\003int(\\001\\\"\\022\\n\\006sorted\\022\\004bool\\032\\002(\\001\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027B\\026\\010\\007\\022\\022Use TopKV2 instead\\nf\\n\\006TopKV2\\022\\n\\n\\005input\\\"\\001T\\022\\005\\n\\001k\\030\\003\\032\\013\\n\\006values\\\"\\001T\\032\\013\\n\\007indices\\030\\003\\\"\\022\\n\\006sorted\\022\\004bool\\032\\002(\\001\\\"\\033\\n\\001T\\022\\004type:\\020\\n\\0162\\014\\001\\002\\003\\004\\005\\006\\t\\016\\021\\023\\026\\027\")\n"
     ]
    }
   ],
   "source": [
    "!cat /home/default/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
